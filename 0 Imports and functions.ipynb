{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922609c5-86a7-4cbf-8068-8dd90d034a73",
   "metadata": {},
   "source": [
    "# Updates/Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48de9dc9-474e-4fb7-8404-90eb4065006f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "#Need to update? Here ya go!!\n",
    "\n",
    "\n",
    "#!py -m pip uninstall astropy\n",
    "#!py -m pip install git+https://github.com/astropy/astropy\n",
    "#!pip install emcee\n",
    "!pip install corner\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "!py -m pip install git+https://github.com/radio-astro-tools/spectral-cube.git\n",
    "!py -m pip install reproject\n",
    "!py -m pip install git+https://github.com/radio-astro-tools/spectral-cube.git \n",
    "!py -m pip install pyspeckit\n",
    "!py -m pip install regions\n",
    "!py -m pip install astrodendro\n",
    "!py -m pip  install wcsaxes \n",
    "!py -m pip  install ipympl\n",
    "!py -m pip install dask\n",
    "!py -m pip install radio_beam\n",
    "!py -m pip install casa_formats_io\n",
    "#try:\n",
    "#    !pip install casa_formats_io --no-binary :all:\n",
    "#except:\n",
    "#    !pip install casa_formats_io --no-cache --no-binary :all:\n",
    "\n",
    "!py -m pip  install spectral_cube \n",
    "!py -m pip  install typing \n",
    "!py -m pip install mypy\n",
    "!py -m pip  install typing_extensions \n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9fb49-ce1d-425e-a426-1a0cfa419903",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc0ed5b-634f-455a-80e0-6f7bfa128b70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/bin/python\n",
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\n",
      "astropy 5.1.dev153+gb740594dc\n",
      "spectral_cube 0.6.1.dev22+g003ef16\n",
      "spectral_cube file path /home/ben/.local/lib/python3.8/site-packages/spectral_cube/__init__.py\n",
      "reproject 0.8\n",
      "1.23.1 Numpy\n",
      "astrodendro_file: /home/ben/.local/lib/python3.8/site-packages/astrodendro/__init__.py\n",
      "Results will be saved to Directory ./Result Files\n",
      "Cubes will be saved and loaded with Directory ./Spectral Cubes\n",
      "Created directory ./Plots\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "\n",
    "#Every data reduction and analysis file will use these imports and functions.\n",
    "#So i run this file at the beggining of every other file to import stuff.\n",
    "\n",
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "\n",
    "#These will show you what version of Python you are working with. Important because astropy works best with certain versions like 3.8.5\n",
    "\n",
    "import sys, traceback\n",
    "\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# The most important package for astronomy\n",
    "\n",
    "import astropy\n",
    "from astropy.coordinates import SkyCoord\n",
    "print('astropy',astropy.__version__ )\n",
    "import astropy.io.fits as fits              \n",
    "from astropy.wcs import WCS # World coordinate system\n",
    "from astropy import units as u  \n",
    "from astropy.table import Table\n",
    "from astropy.convolution import Gaussian1DKernel\n",
    "from astropy.utils import NumpyRNGContext\n",
    "from astropy.coordinates import Angle\n",
    "\n",
    "\n",
    "# Spectral cubes are amazing at taking fits images and turning them into workable position-position-velocity cubes \n",
    "# (The cubes are 3D, but the moment maps/continuum images will be 2D and SC will also work for them)\n",
    "\n",
    "from spectral_cube import SpectralCube    \n",
    "from spectral_cube import LazyMask\n",
    "import spectral_cube\n",
    "print('spectral_cube',spectral_cube.__version__)\n",
    "print('spectral_cube file path',spectral_cube.__file__)\n",
    "\n",
    "# Need this for projection of the cubes\n",
    "\n",
    "from reproject import reproject_interp      \n",
    "from reproject.mosaicking import find_optimal_celestial_wcs \n",
    "import regions\n",
    "import reproject\n",
    "print('reproject',reproject.__version__)\n",
    "\n",
    "# Useful for doing analysis\n",
    "\n",
    "import pylab                                \n",
    "import matplotlib \n",
    "import matplotlib.gridspec as gridspec                                                                                             \n",
    "import matplotlib.colors as colors\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "import numpy as np                          \n",
    "from matplotlib.patches import Ellipse\n",
    "print(matplotlib.__version__,\"Matplotlib\")\n",
    "print(matplotlib.__file__)\n",
    "print(np.__version__,\"Numpy\")\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import leastsq\n",
    "from scipy.spatial.transform import Rotation as R # For doing 3d rotations\n",
    "\n",
    "# astrodendro is a key package, it allows a quick way to identify structures\n",
    "\n",
    "import astrodendro \n",
    "from astrodendro.analysis import PPVStatistic # Takes statistics of PPV structures\n",
    "\n",
    "print(\"astrodendro_file:\", astrodendro.__file__)\n",
    "\n",
    "# The radio_beam library allows you to check/change teh interferometric properties of the beam.\n",
    "\n",
    "import radio_beam\n",
    "\n",
    "# Garbage collection\n",
    "\n",
    "import gc\n",
    "\n",
    "# Suppress warnings we don't care about:\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    \n",
    "#     \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "# Create the save directories\n",
    "#\n",
    "\n",
    "if(os.path.exists(\"./Result Files\")):\n",
    "    print(\"Results will be saved to Directory ./Result Files\")\n",
    "else:\n",
    "    %mkdir \"./Result Files\"\n",
    "    print(\"Created directory ./Result Files where results will be saved\")\n",
    "    \n",
    "if(os.path.exists(\"./Spectral Cubes\")):\n",
    "    print(\"Cubes will be saved and loaded with Directory ./Spectral Cubes\")\n",
    "else:\n",
    "    %mkdir \"./Spectral Cubes\"\n",
    "    print(\"Created directory ./Spectral Cubes where cubes will be saved/loaded\")\n",
    "    \n",
    "if(os.path.exists(\"./Spectral Cubes/Cube Information\")):\n",
    "    pass\n",
    "else:\n",
    "    %mkdir \"./Spectral Cubes/Cube Information\"\n",
    "    print(\"Created directory ./Spectral Cubes/Cube Information where cube information will be saved\")\n",
    "    \n",
    "if(os.path.exists(\"./Plots\")):\n",
    "    print(\"Plots will be saved to Directory ./Plots\")\n",
    "else:\n",
    "    %mkdir \"./Plots\"\n",
    "    print(\"Created directory ./Plots\")\n",
    "    \n",
    "if(os.path.exists(\"./Dendrograms\")):\n",
    "    print(\"Dendrograms will be saved to Directory ./Dendrograms\")\n",
    "else:\n",
    "    %mkdir \"./Dendrograms\"\n",
    "    print(\"Created directory ./Dendrograms\")\n",
    "# If you need to interact with teh plots, use widget, otherwise this runs better\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib widget \n",
    "sys.setrecursionlimit(9999999)  # for very large functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afe913a-d01f-43ff-aa4b-ddb23765f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants (unused)\n",
    "\n",
    "Num_per_kg= 6.0221409*10**23/(2.8*10**-3) /u.kg # A number that Krieger used for number/kg of H2\n",
    "\n",
    "a_850 = 6.7*10**19*u.erg/u.s/u.Hz/u.M_sun #6.7+-1.7, from Bolatto 2013a, for converting continuum flux to mass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e724d-9141-4353-b6b9-da1591dcf8d9",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Make general functions for each process so I can call them simply.\n",
    "The only things I need to change are the input files and their properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a18e1-3372-4c94-be24-f4c3acbe8e6d",
   "metadata": {},
   "source": [
    "# Pointing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9e65a-7352-4c62-b1bd-d666e7fa8ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pointing_Information\n",
    "\n",
    "\n",
    "# Pointing_Information an important variable that will need to be defined for each source\n",
    "# It is a dictionary containing all the metadata and pointing information. This is important because the PPVstatistic needs it to be in a specific format\n",
    "#\n",
    "# eg Pointing_Information:  \n",
    "'''\n",
    "\n",
    "Pointing_Information = {}\n",
    "\n",
    "#This is the stuff that needs changing between cubes\n",
    "\n",
    "Pointing_Information[\"Original_File_Name\"] = \"HCN_J1-0.cube.fits\" #the name of the initial SC file.\n",
    "Pointing_Information[\"File_Descriptor\"] = \"NGC_253_HCN_J1-0_\"\n",
    "Pointing_Information[\"target\"] = \"NGC253\"#or the cmz\n",
    "Pointing_Information[\"center\"] = SkyCoord('00h47m33.14s' ,'-25d17m17.52s',frame='icrs') #the center of NGC253\n",
    "#I use center = SkyCoord('-00d03m20.76s  ', '-00d02m46.176s', frame='galactic') for the cmz center\n",
    "desired_beam_size = 4.3*u.pc #I add this to the PI later. choose this based on the largest common beam size between the compared observations\n",
    "distance = 3.5*u.Mpc\n",
    "Pointing_Information[\"distance\"] = distance.to(u.Mpc) #in Mpc\n",
    "Pointing_Information[\"target_image_rotation\"]=33*u.deg #this is the rotation of the specific image, not the target. (use clockwise rotation angle)\n",
    "Pointing_Information[\"target_inclination\"]=78*u.deg\n",
    "Pointing_Information[\"target_velocity\"]=250*u.km/u.s #the speed NGC253 is moving away from us\n",
    "Pointing_Information[\"vaxis\"]=0 #which axis is the velocity\n",
    "Pointing_Information[\"desired_velocity_resolution\"]= 3.3*u.km/u.s\n",
    "ovs = 3 #how much do you desire to oversample the beam by\n",
    "desired_fov = [360*u.pc,70*u.pc]\n",
    "Pointing_Information[\"Corresponding_Continuum\"] = 'Continuum_Reproject.fits' # this is for the band 7, not for HCN J1-0 #The continuum image for this band\n",
    "#                                                 '4.3pc_beam_CMZ_850um_Cont_140x800pc.fits' # this is the JCMT continuum for 850microns (the CO 3-2)\n",
    "#                                                 \"gc_850micron_dust.fits\" # this is the CMZoom dust continuum from SCUBA Galactic Centre Survey (850um)\n",
    "\n",
    "#This stuff will input automagically if the rest is correct\n",
    "\n",
    "sc = SpectralCube.read(\"Spectral Cubes/\"+Pointing_Information[\"Original_File_Name\"])\n",
    "header = sc.header\n",
    "\n",
    "try:\n",
    "    freq = header[\"RESTFREQ\"]*u.Hz# assuming the header is in Hz\n",
    "    Pointing_Information['wavelength']=299792458*u.m/header[\"RESTFREQ\"]#\n",
    "    Pointing_Information['restfreq']=header[\"RESTFREQ\"]#            \n",
    "except:\n",
    "    freq = header[\"RESTFRQ\"]*u.Hz#\n",
    "    Pointing_Information['wavelength']=299792458*u.m/header[\"RESTFRQ\"]#            \n",
    "    Pointing_Information['restfreq']=header[\"RESTFRQ\"]#    \n",
    "\n",
    "######calculate teh beam size of the original image:\n",
    "\n",
    "if (header['CUNIT1'].find(\"deg\")!=-1):\n",
    "    CUNIT = 1*u.degree\n",
    "    Pointing_Information[\"CUNIT\"]=CUNIT\n",
    "else:\n",
    "    print(\"The header should show CUNIT in degrees. If not, just fix this or write CUNIT = the unit it says in the header\")\n",
    "    del jhgasdhgjkahsdkgdfjhsgjgjsdhkfjghjd #this causes an error and stops execution \n",
    "    \n",
    "beam_major =  (header[\"BMAJ\"]*CUNIT).to(u.arcsec) #degrees beam size -> arcsec\n",
    "beam_minor =  (header[\"BMIN\"]*CUNIT).to(u.arcsec)\n",
    "\n",
    "\n",
    "Pointing_Information[\"original_BMAJ\"]=beam_major\n",
    "Pointing_Information[\"original_BMIN\"]=beam_minor\n",
    "Pointing_Information[\"original_BMAJ_pc\"]=beam_major.to(u.rad)*Pointing_Information['distance']\n",
    "Pointing_Information[\"original_BMIN_pc\"]=beam_minor.to(u.rad)*Pointing_Information['distance']\n",
    "Pointing_Information[\"desired_beam_size\"] = desired_beam_size #ill put it in a circular beam at this size\n",
    "\n",
    "\n",
    "#this accounts for elliptical beams:            \n",
    "Pointing_Information['original_pixel_scale_x'] = (abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix\n",
    "Pointing_Information['original_pixel_scale_y'] = (abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix\n",
    "Pointing_Information['original_spatial_scale_x'] = (abs(header[\"CDELT1\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance']\n",
    "Pointing_Information['original_spatial_scale_y'] = (abs(header[\"CDELT2\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance']#convert to pc using the distance\n",
    "\n",
    "average_pixel=np.sqrt((abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix*(abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix)\n",
    "\n",
    "Pointing_Information['original_beam_oversampling_MAJ'] = beam_major/average_pixel\n",
    "Pointing_Information['original_beam_oversampling_MIN'] = beam_minor/average_pixel\n",
    "Pointing_Information['desired_beam_oversampling'] = ovs\n",
    "#Pointing_Information['orig_FOV']= Crop_Around_Center(sc,Pointing_Information['target'],Pointing_Information['target_image_rotation'],Pointing_Information['center'],desired_fov,Pointing_Information['distance'])[1] #returns the current fov\n",
    "Pointing_Information['desired_FOV']=desired_FOV\n",
    "\n",
    "######\n",
    "\n",
    "\n",
    "#Cube_Information. This needs to be updated every time a reduction occurs on the cube. By the end of file 1 which does all the reprojection, this will not change.\n",
    "#The pointing information will not change, so I'll start with a copy of that:\n",
    "\n",
    "Cube_Information = copy.deepcopy(Pointing_Information)\n",
    "\n",
    "#For example, after doing the reprojection, I need to load: \n",
    "\n",
    "\n",
    "Current_Cube_Name = Pointing_Information[\"File_Descriptor\"] + str(Pointing_Information[\"desired_beam_size\"].value)+\"pc_beam_\"+str(FOVp[0])+\"x\"+str(FOVp[1])+'pc_'++str(i)+'.fits'\n",
    "Cube_Information[\"File_Name\"]=Current_Cube_Name\n",
    "\n",
    "sc = SpectralCube.read(Current_Cube_Name)\n",
    "header = sc.header\n",
    "\n",
    "pc_per_pixelx = abs(header[\"CDELT1\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "pc_per_pixely = abs(header[\"CDELT2\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "\n",
    "Cube_Information[\"pixel_scale_x\"]=pc_per_pixelx\n",
    "Cube_Information[\"pixel_scale_y\"]=pc_per_pixely\n",
    "\n",
    "Cube_Information['data_unit'] =sc[0][0][0].unit# or just use sc.header['BUNIT']\n",
    "\n",
    "Cube_Information['arc_per_pix_y'] =  abs(header[\"CDELT1\"]*CUNIT).to(u.arcsec)/u.pix \n",
    "Cube_Information['arc_per_pix_x'] =  abs(header[\"CDELT2\"]*CUNIT).to(u.arcsec)/u.pix\n",
    "\n",
    "beam_major =  (header[\"BMAJ\"]*CUNIT).to(u.arcsec) #degrees beam size -> arcsec\n",
    "beam_minor =  (header[\"BMIN\"]*CUNIT).to(u.arcsec) #if these two are different, something went wrong in the reprojection\n",
    "Cube_Information['beam_major'] =  beam_major\n",
    "Cube_Information['beam_minor'] =  beam_minor\n",
    "\n",
    "beam_area_ratio = beam_minor*beam_major/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']*1.13309#beam_area_ratio = Cube_Information['beam_minor']*Cube_Information['beam_major']/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']#This is for FWHM, use *(2*np.sqrt(2*np.log(2)))**2#For gaussian beam\n",
    "Cube_Information['beam_area_ratio']=beam_area_ratio #This is important for finding the minimum number of pixels a structure can have, or for calculating column densities\n",
    "\n",
    "#this accounts for elliptical beams:            \n",
    "Cube_Information['spatial_scale_x'] = (abs(header[\"CDELT1\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance']\n",
    "Cube_Information['spatial_scale_y'] = (abs(header[\"CDELT2\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance']\n",
    "                                     \n",
    "                                     \n",
    "\n",
    "\n",
    "\n",
    "Cube_Information[\"velocity_scale\"] = abs(header[\"CDELT3\"])*u.km/u.s#the cube should be in u.km/u.s instead of frequency on the z axis\n",
    "\n",
    "average_pixel=np.sqrt((abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix*(abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix)\n",
    "\n",
    "Cube_Information['beam_oversampling'] = beam_minor/average_pixel\n",
    "Cube_Information['desired_beam_oversampling'] = Pointing_Information['desired_beam_oversampling']\n",
    "\n",
    "Cube_Information[\"wcsu\"]=sc.wcs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#make a function to do this easily\n",
    "\n",
    "def Update_Cube_Information(Pointing_Information,Current_Cube_Name):\n",
    "\n",
    "    Cube_Information = copy.deepcopy(Pointing_Information)\n",
    "\n",
    "    Cube_Information[\"File_Name\"]=Current_Cube_Name\n",
    "    try:\n",
    "        del sc\n",
    "        print(\"Deleted sc\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try: \n",
    "        try:\n",
    "            sc = SpectralCube.read(\"Spectral Cubes/\"+Current_Cube_Name) #For a 3d cube\n",
    "        except:\n",
    "            sc = SpectralCube.read(\"Spectral Cubes/\"+Current_Cube_Name,format='fits') #For a fits cube that is not labelled as such and is 3d\n",
    "    except: \n",
    "        sc = spectral_cube.Projection.from_hdu(fits.open(\"Spectral Cubes/\"+Current_Cube_Name)) # For a 2d cube\n",
    "    header = sc.header\n",
    "\n",
    "    if (header['CUNIT1'].find(\"deg\")!=-1):\n",
    "        CUNIT = 1*u.degree\n",
    "        Pointing_Information[\"CUNIT\"]=CUNIT\n",
    "    else:\n",
    "        print(\"The header should show CUNIT in degrees. If not, just fix this or write CUNIT = the unit it says in the header\")\n",
    "        del jhgasdhgjkahsdkgdfjhsgjgjsdhkfjghjd #this causes an error and stops execution \n",
    "    \n",
    "    #pc_per_pixelx = abs(header[\"CDELT1\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "    #pc_per_pixely = abs(header[\"CDELT2\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "\n",
    "    #Cube_Information[\"pixel_scale_x\"]=pc_per_pixelx\n",
    "    #Cube_Information[\"pixel_scale_y\"]=pc_per_pixely\n",
    "    \n",
    "    try:\n",
    "        Cube_Information['data_unit'] =sc[0][0][0].unit# or just use sc.header['BUNIT']\n",
    "    except:\n",
    "        Cube_Information['data_unit'] =sc[0][0].unit# or just use sc.header['BUNIT']\n",
    "\n",
    "    Cube_Information['arc_per_pix_y'] =  abs(header[\"CDELT1\"]*CUNIT).to(u.arcsec)/u.pix \n",
    "    Cube_Information['arc_per_pix_x'] =  abs(header[\"CDELT2\"]*CUNIT).to(u.arcsec)/u.pix\n",
    "\n",
    "    try:\n",
    "        \n",
    "        # Get the beam size. If there is none, use the pixel size\n",
    "        beam_major =  (header[\"BMAJ\"]*CUNIT).to(u.arcsec) #degrees beam size -> arcsec\n",
    "        beam_minor =  (header[\"BMIN\"]*CUNIT).to(u.arcsec) #if these two are different, something went wrong in the reprojection \n",
    "        Cube_Information['Beam_Position_angle'] = header['bpa']*u.deg\n",
    "\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        cdelt_x = u.Quantity(str(np.abs(header['cdelt1']))+header['cunit1'])\n",
    "        cdelt_y = u.Quantity(str(np.abs(header['cdelt2']))+header['cunit2'])\n",
    "        if(cdelt_x>cdelt_y):\n",
    "            beam_major=cdelt_x\n",
    "            beam_minor=cdelt_y\n",
    "        elif(cdelt_x<cdelt_y):\n",
    "            beam_major=cdelt_y\n",
    "            beam_minor=cdelt_x\n",
    "        elif(cdelt_x==cdelt_y):\n",
    "            beam_major=cdelt_x\n",
    "            beam_minor=cdelt_x\n",
    "        \n",
    "        Cube_Information['Beam_Position_angle'] = 0*u.deg\n",
    "\n",
    "    \n",
    "    Cube_Information['beam_major'] =  beam_major\n",
    "    Cube_Information['beam_minor'] =  beam_minor\n",
    "\n",
    "    beam_area_ratio = beam_minor*beam_major/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']*1.13309#beam_area_ratio = Cube_Information['beam_minor']*Cube_Information['beam_major']/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']#This is for FWHM, use *(2*np.sqrt(2*np.log(2)))**2#For gaussian beam\n",
    "    Cube_Information['beam_area_ratio']=beam_area_ratio.value*u.pix**2/u.beam #This is important for finding the minimum number of pixels a structure can have, or for calculating column densities\n",
    "    \n",
    "    \n",
    "    \n",
    "    Cube_Information['spatial_scale_x'] = (abs(header[\"CDELT1\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance'] / u.rad\n",
    "    Cube_Information['spatial_scale_y'] = (abs(header[\"CDELT2\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance'] / u.rad #pc/pixel\n",
    "    \n",
    "    #this is a specific value used by the dendrogram, and they want it in (angle)\n",
    "    Cube_Information['spatial_scale'] = np.sqrt(Cube_Information['arc_per_pix_x']*Cube_Information['arc_per_pix_y'])*u.pix\n",
    "\n",
    "    try:\n",
    "        Cube_Information[\"velocity_scale\"] = abs(header[\"CDELT3\"])*u.km/u.s#the cube should be in u.km/u.s instead of frequency on the z axis\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    average_pixel=np.sqrt((abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix*(abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix)\n",
    "\n",
    "    Cube_Information['beam_oversampling'] = beam_minor/average_pixel\n",
    "    Cube_Information['desired_beam_oversampling'] = Pointing_Information['desired_beam_oversampling']\n",
    "\n",
    "    Cube_Information[\"wcsu\"]=sc.wcs\n",
    "    \n",
    "    #Cube_Information['FOV']= Crop_Around_Center(sc,Cube_Information['target_image_rotation'],Cube_Information['center'],Cube_Information['desired_FOV'],Cube_Information['distance'])[1] #returns [the cropped sc, the current fov, the desired fov]\n",
    "    #Cube_Information['desired_FOV']=Pointing_Information['desired_FOV']\n",
    "    \n",
    "    return Cube_Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6adba-7634-47ba-a3df-41fef9d61862",
   "metadata": {},
   "source": [
    "# Reprojection function\n",
    "\n",
    "To align the files across and get the same beam size and FOVs along different observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149233c-fc12-405c-b28b-c205e426cd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Spatial reprojection \n",
    "\n",
    "# File = the file name of the SC you want to reproject\n",
    "# Prime_Beam = the pc size of the beam. When comparing a line in two different sources, i match the beam sizes across the sources\n",
    "\n",
    "# Pointing_Information= see earlier\n",
    "# Cube_Information= see earlier\n",
    "# i_step = the amount of velocity channels for each step in the reprojection. lower this if your computer lags a lot.\n",
    "\n",
    "def Reproject_To_Region(Pointing_Information,Cube_Information,i_step=30,Cube_Name_Save='',Force_Origin=[False,[0,0]*u.deg,[0,0]*u.deg],partway = False,crop_nans=True,change_origins=True,force_pixels=False,fy=200,fx=200,GLON_BASE=False):\n",
    "    \n",
    "    File = Cube_Information[\"File_Name\"]\n",
    "    Prime_Beam = Pointing_Information[\"desired_beam_size\"]\n",
    "    Gal = Pointing_Information[\"target\"]\n",
    "    ovs = Pointing_Information[\"desired_beam_oversampling\"]\n",
    "    FOV = Pointing_Information[\"desired_FOV\"]\n",
    "    distance=Pointing_Information[\"distance\"]\n",
    "    target_velocity= Pointing_Information[\"target_velocity\"]\n",
    "    try:\n",
    "        center = Pointing_Information[\"crop_center\"] # If there is a specified cropping center, use it.\n",
    "    except:\n",
    "        center = Pointing_Information[\"center\"]\n",
    "    rotation_angleP = Pointing_Information[\"target_image_rotation\"]\n",
    "    Line_Name = Pointing_Information[\"File_Descriptor\"]\n",
    "\n",
    "    run_completed=False\n",
    "    # These are related to error correction the original CMZ cubes because they have an error with their longitude values going from 360->0 through the center\n",
    "    \n",
    "    Force_Origins=Force_Origin[0]\n",
    "    Force_Value_x=Force_Origin[1]\n",
    "    Force_Value_y=Force_Origin[2]\n",
    "    \n",
    "    #Load observation cube\n",
    "    \n",
    "    try:\n",
    "        scB = SpectralCube.read(\"Spectral Cubes/\"+File) \n",
    "    except:\n",
    "        scB = SpectralCube.read(\"Spectral Cubes/\"+File,format='fits') \n",
    "    scB.allow_huge_operations=True\n",
    "    \n",
    "    scB = scB.with_spectral_unit(u.km/u.s,velocity_convention=\"radio\") # Change units from Hz to km/s in case they are in Hz\n",
    "    \n",
    "    #Determine the resolution based on the oversample factor\n",
    "    \n",
    "    beam_deg =  ((Prime_Beam/(distance))*u.rad).to(u.deg)#deg corresponding to the desired beam size\n",
    "    \n",
    "    # Check the coordinate systems\n",
    "    \n",
    "    vel,RA,Dec = scB.world[:,0,0]\n",
    "    \n",
    "    # Need to break it up into 30-wide vel slices to do the reprojection (ram-draw too high otherwise)  \n",
    "    \n",
    "    for i in range(int(len(scB)/i_step) +1):\n",
    "        \n",
    "        try:\n",
    "            print('begin step:',i,\"of\",int(len(scB)/i_step) +1)\n",
    "            \n",
    "            Cube_Name_Save = str(Prime_Beam.value)+\"pc_beam_\"+Line_Name+str(FOV[0].value)+\"x\"+str(FOV[1].value)+'pc_'+'reprojected.fits'\n",
    "            \n",
    "            if os.path.exists((\"Spectral Cubes/\"+str(str(i)+\"_\"+Cube_Name_Save))):\n",
    "                if partway:\n",
    "                    continue #If there are steps that have already been completed, skip them\n",
    "            \n",
    "            n = scB[i*i_step:i*i_step+i_step]\n",
    "            n.write(\"h_interim.fits\",overwrite=True)\n",
    "            #sc = copy.deepcopy(scB[i*i_step:i*i_step+i_step])\n",
    "            sc = SpectralCube.read(\"h_interim.fits\") #need to do this because python has glitches with pointers and copying\n",
    "            \n",
    "            vel_P,xx,xxxx = sc.world[:,0,0]\n",
    "            if(min(vel_P.to(u.km/u.s).value) > (target_velocity + 251*u.km/u.s).value or max(vel_P.to(u.km/u.s).value) < (target_velocity - 251*u.km/u.s).value):\n",
    "                if run_completed:\n",
    "                    break #If the data is out of the velocity bounds, stop the loop\n",
    "                else:\n",
    "                    del sc\n",
    "                    del vel_P\n",
    "                    continue\n",
    "            sc.write(\"test1.fits\",overwrite=True)\n",
    "\n",
    "            sc = sc.spectral_slab(target_velocity- 251*u.km/u.s, target_velocity+ 251*u.km/u.s)  # Crop out velocities we don't care about  \n",
    "            \n",
    "            sc.allow_huge_operations=True\n",
    "            sc.write(\"test2.fits\",overwrite=True)\n",
    "            \n",
    "            '''\n",
    "            #\n",
    "            #\n",
    "            # Rotate cube to flat\n",
    "            #\n",
    "            #\n",
    "            '''\n",
    "            # First determine the end bounds of the cube\n",
    "            long_extrema = sc.longitude_extrema\n",
    "            lat_extrema = sc.latitude_extrema\n",
    "            \n",
    "            rotation_angle = rotation_angleP # The original rotation\n",
    "            if(rotation_angle!=0*u.deg):\n",
    "                \n",
    "                print('starting cube rotation')\n",
    "                sc_R =  Rotate_Cube(sc, rotation_angle) # Rotates the cube to flatten it. This might take a while\n",
    "                del sc\n",
    "                sc = SpectralCube.read(sc_R.hdu)\n",
    "                del sc_R\n",
    "                rotation_angle = 0*u.deg # New rotation angle (The cube is flat now)\n",
    "                Pointing_Information[\"target_image_rotation\"] = rotation_angle # This is not really pointing information but I need it here\n",
    "                print('finished cube rotation')\n",
    "\n",
    "\n",
    "            \n",
    "            print('start beam convolution')\n",
    "\n",
    "            try:\n",
    "                #Make a circular beam to convolve the image to.\n",
    "    \n",
    "                beam = radio_beam.Beam(major=beam_deg, minor=beam_deg, pa=0*u.deg)\n",
    "                sc = sc.convolve_to(beam)\n",
    "            \n",
    "            except:\n",
    "                print(\"The initial image has no beam because it's not interferometric, so one will be created using the pixel size as the initial beam size\")\n",
    "                \n",
    "                cdelt_x = u.Quantity(str(np.abs(sc.header['cdelt1']))+sc.header['cunit1'])\n",
    "                cdelt_y = u.Quantity(str(np.abs(sc.header['cdelt2']))+sc.header['cunit2'])\n",
    "                if(cdelt_x>cdelt_y):\n",
    "                    majorBase=cdelt_x\n",
    "                    minorBase=cdelt_y\n",
    "                elif(cdelt_x<cdelt_y):\n",
    "                    majorBase=cdelt_y\n",
    "                    minorBase=cdelt_x\n",
    "                elif(cdelt_x==cdelt_y):\n",
    "                    majorBase=cdelt_x\n",
    "                    minorBase=cdelt_x\n",
    "                    \n",
    "                BaseBeam = radio_beam.Beam(major=majorBase, minor=minorBase, pa=0*u.deg)\n",
    "\n",
    "                sc = sc.with_beam(BaseBeam)\n",
    "                beam = radio_beam.Beam(major=beam_deg, minor=beam_deg, pa=0*u.deg)\n",
    "\n",
    "                sc.allow_huge_operations=True\n",
    "                #Requires me to edit convolve.py and set allow_huge =True\n",
    "                #If this fails, go edit that file in your repository.\n",
    "                sc = sc.convolve_to(beam)\n",
    "                \n",
    "            print('convolve end\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Mask the pixels outside the fov\n",
    "            \n",
    "            #returns: cropped_sc,orig_fov,FOV\n",
    "            print('fov crop start \\n')\n",
    "            \n",
    "            #sc.write(\"test3.fits\",overwrite=True)\n",
    "            \n",
    "\n",
    "            cropped_sc = Crop_Around_Center(sc,rotation_angle,center,FOV,distance,crop_nans=crop_nans)\n",
    "            del sc\n",
    "            sc = cropped_sc\n",
    "            sc.write(\"test4.fits\",overwrite=True)\n",
    "            del cropped_sc\n",
    "            print('check max SC value after crop:',np.nanmax(sc),\"SC shape:\", np.shape(sc))#These should be a non zero float and the shape of the cube (30,~1000,~1000)\n",
    "            print('fov cropped\\n')\n",
    "            \n",
    "            \n",
    "            #\n",
    "            #prepare a header for the reprojection\n",
    "            #\n",
    "            \n",
    "            reheader = copy.deepcopy(sc.hdu.header)\n",
    "            \n",
    "\n",
    "            ## Find the number of expected pixels for the new resolution and the location of the left/right, up/down sides \n",
    "\n",
    "            #find out which direction the cube is read, left to right or right/to left. (in terms of RA/DEC). Then, do the same for up and down\n",
    "            \n",
    "            if sc.header['cdelt1']>0:\n",
    "                pix_x    = (beam_deg/ovs).to(u.degree).value\n",
    "                #origin_x = sc.longitude_extrema[0].to(u.degree).value  #, this is the current value, but it doesnt work with the rotation so I use the original values\n",
    "                origin_x = long_extrema[0].value\n",
    "                \n",
    "                if(Force_Origins):\n",
    "                    origin_x = Force_Value_x[0]#358.6\n",
    "\n",
    "            else:\n",
    "                pix_x    = -1.*(beam_deg/ovs).to(u.degree).value\n",
    "                #origin_x = (sc.longitude_extrema[1]).to(u.degree).value #, this is the current value, but it doesnt work with the rotation so I use the original values:\n",
    "                origin_x = long_extrema[1].value\n",
    "\n",
    "                if(Force_Origins):\n",
    "                    origin_x = Force_Value_x[1]#.9\n",
    "\n",
    "            if sc.header['cdelt2']>0:\n",
    "                pix_y    = (beam_deg/ovs).to(u.degree).value\n",
    "                #origin_y = sc.latitude_extrema[0].to(u.degree).value  #, this is the current value, but it doesnt work with the rotation so I use the original values\n",
    "                origin_y = lat_extrema[0].value\n",
    "                if(Force_Origins):\n",
    "                    origin_y = Force_Value_y[0]#-.6\n",
    "                    \n",
    "            else:\n",
    "                pix_y    = -1.*(beam_deg/ovs).to(u.degree).value\n",
    "                #origin_y = sc.latitude_extrema[1].to(u.degree).value  #, this is the current value, but it doesnt work with the rotation so I use the original values\n",
    "                origin_y = lat_extrema[1].value\n",
    "\n",
    "                if(Force_Origins):\n",
    "                    origin_y = Force_Value_y[1]#.6\n",
    "                    \n",
    "        \n",
    "            #npix_x   = int(np.ceil(np.diff(sc.longitude_extrema, n=1)[0]/np.abs(pix_x)).value)\n",
    "            #npix_y   = int(np.ceil(np.diff(sc.latitude_extrema, n=1)[0]/np.abs(pix_y)).value)\n",
    "            #npix_x   =int(np.ceil(np.diff([origin_x,origin_x_max])/np.abs(pix_x)))\n",
    "            #npix_y   =int(np.ceil(np.diff([origin_y,origin_y_max])/np.abs(pix_y)))\n",
    "            #manually make the cubes overproject because it cuts off data otherwise, because it cannot properly convert the longitude/latitude extrema to ra/dec information:\n",
    "            try:\n",
    "                npix_x   =int(np.ceil(np.diff([long_extrema.value])/np.abs(pix_x)))\n",
    "                npix_y   =int(np.ceil(np.diff([lat_extrema.value])/np.abs(pix_y)))\n",
    "            except:\n",
    "                print(\"Longitude and lattitude values not found.\")\n",
    "                npix_x = 200\n",
    "                npix_y = 200 #default size\n",
    "                \n",
    "            crpix1 = 0\n",
    "            crpix2 = 0\n",
    "            \n",
    "            if(Force_Origins):\n",
    "                \n",
    "                npix_x   =abs(int(np.diff([Force_Value_x[0]-360,Force_Value_x[1]])/np.abs(pix_x)) )\n",
    "                npix_y   =int(np.diff(Force_Value_y)/np.abs(pix_y)) \n",
    "                print(\"Force values of:\", Force_Value_x,Force_Value_y,\"Giving pixel range of \",npix_x,npix_y)\n",
    "            \n",
    "            if(rotation_angleP != 0*u.deg):\n",
    "                #if there is a rotation, i need additional fov to account for it\n",
    "                npix_x +=400 #increase the axis size\n",
    "                npix_y +=200\n",
    "                crpix1 +=400 # the anchor pixel for the minimum longitude\n",
    "                crpix2 +=200 # for latitude\n",
    "            if(force_pixels):\n",
    "                npix_y=fy\n",
    "                npix_x=fx\n",
    "            #Correct the header to the expected pixels for the new res\n",
    "\n",
    "            reheader['cdelt1'] = pix_x\n",
    "            reheader['cdelt2'] = pix_y\n",
    "\n",
    "            reheader['naxis1'] = npix_x\n",
    "            reheader['naxis2'] = npix_y\n",
    "            \n",
    "            \n",
    "            if(change_origins):\n",
    "                \n",
    "                reheader['crval1'] = origin_x\n",
    "                reheader['crval2'] = origin_y\n",
    "\n",
    "                reheader['crpix1'] = crpix1\n",
    "                reheader['crpix2'] = crpix2\n",
    "            \n",
    "            if(str(sc.wcs).find(\"GLON-SIN\")!=-1):\n",
    "                reheader['CTYPE1'] = \"GLON-SIN\"\n",
    "                reheader['CTYPE2'] = \"GLAT-SIN\"\n",
    "            elif(str(sc.wcs).find(\"GLON-TAN\")!=-1):\n",
    "                reheader['CTYPE1'] = \"GLON-TAN\"\n",
    "                reheader['CTYPE2'] = \"GLAT-TAN\"\n",
    "            elif(str(sc.wcs).find(\"GLON-CAR\")!=-1):\n",
    "                reheader['CTYPE1'] = \"GLON-CAR\"\n",
    "                reheader['CTYPE2'] = \"GLAT-CAR\"\n",
    "            elif(str(sc.wcs).find(\"GLON\")!=-1 and GLON_BASE==False):\n",
    "                reheader['CTYPE1'] = \"GLON-SIN\" #SIN is the default for GLON\n",
    "                reheader['CTYPE2'] = \"GLAT-SIN\"                \n",
    "            elif(str(sc.wcs).find(\"GLON\")!=-1 and GLON_BASE):\n",
    "                reheader['CTYPE1'] = \"GLON\" #SIN is the default for GLON\n",
    "                reheader['CTYPE2'] = \"GLAT\"\n",
    "                \n",
    "            # remove these things that confuse the reprojection since we won't be using them\n",
    "            try:\n",
    "                del reheader['lonpole']\n",
    "                del reheader['latpole']\n",
    "                del reheader['wcsaxes']# Dont need these anymore\n",
    "                if(str(sc.wcs).find(\"GLON\")!=-1):\n",
    "\n",
    "                    del reheader['LBOUND1']\n",
    "                    del reheader['LBOUND2']\n",
    "                    del reheader['LBOUND3']\n",
    "                    del reheader.cards['LBOUND1']\n",
    "                    del reheader.cards['LBOUND2']\n",
    "                    del reheader.cards['LBOUND3']\n",
    "\n",
    "                    reheader['LBOUND1']=0\n",
    "                    reheader['LBOUND2']=0\n",
    "                    reheader['LBOUND3']=0\n",
    "            except Exception as e:\n",
    "                print(\"Delete LBOUND if there is one in the header, if not this will pass the error:\")\n",
    "                print(e)\n",
    "                print(\"-\"*60)\n",
    "                traceback.print_exc(file=sys.stdout)\n",
    "                print()\n",
    "\n",
    "            # regrid cube to target pixel size\n",
    "\n",
    "            print('start reprojection\\n')\n",
    "            print('check max SC value:',np.nanmax(sc),\"SC shape:\", np.shape(sc))#These should be a non zero float and the shape of the cube (30,~1000,~1000)\n",
    "            sc.write(\"test5.fits\",overwrite=True)\n",
    "            sc.allow_huge_operations=True\n",
    "            sc2 = sc.reproject(reheader, order='bilinear', use_memmap=True, filled=True) # Had to change reproject.py so it deletes output.np before making a new one\n",
    "            sc2.write(\"test6.fits\",overwrite=True)\n",
    "\n",
    "            del sc # save space\n",
    "\n",
    "            # make a new cube with the reprojcted data (remove all the logs from the old cube)\n",
    "            \n",
    "            new = SpectralCube(data=sc2.hdu.data,wcs =WCS(sc2.header),header=sc2.header,mask=sc2.mask)\n",
    "            new.allow_huge_operations=True\n",
    "            new = new*sc2[0][0][0].unit\n",
    "            \n",
    "            #do this because scs dont like being modified\n",
    "            del sc2\n",
    "            sc2 = new\n",
    "            del new\n",
    "            \n",
    "            print('\\nend reprojection\\n')\n",
    "            print('check max SC value:',np.nanmax(sc2),\"SC shape:\", np.shape(sc2))#These should ALSO be a non zero float and the shape of the cube (30,~1000,~1000)\n",
    "\n",
    "            sc = sc2\n",
    "            del sc2\n",
    "            sc.allow_huge_operations=True\n",
    "            \n",
    "            if(crop_nans):\n",
    "                #Mask the pixels outside the fov again after the reprojection to get rid of nan created pixels\n",
    "\n",
    "                print('fov crop start 2 \\n')\n",
    "\n",
    "                cropped_sc = Crop_Around_Center(sc,rotation_angle,center,FOV,distance)\n",
    "                del sc\n",
    "                sc = cropped_sc\n",
    "                del cropped_sc\n",
    "                print('fov cropped 2\\n')\n",
    "                # Write the intermediary cubes that will be spliced together\n",
    "            \n",
    "            sc.write(\"Spectral Cubes/\"+str(str(i)+\"_\"+Cube_Name_Save),overwrite=True)\n",
    "            \n",
    "            run_completed=True\n",
    "            del sc\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Failed (unless this says attempt to get argmin of empty sequence)\")\n",
    "            print(\"-\"*60)\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04e97c-3232-4c4a-9a3a-273aec12e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#############\n",
    "# Crop_Around_Center\n",
    "#############\n",
    "\n",
    "\n",
    "#This function crops out things outside the rectangle where the actual disk lies.\n",
    "#The disk may be rotated, so this will be a rotated rectangle.\n",
    "#Without a circular beam, this will be off, but I only use it after I circularize the beam.\n",
    "\n",
    "\n",
    "#returns: cropped_sc,orig_fov,FOV\n",
    "\n",
    "def Crop_Around_Center(sc_for_cropping,rotation_angle,center,desired_fov,distance,crop_nans=True):\n",
    "\n",
    "    \n",
    "    \n",
    "    r,c,FOV,d = rotation_angle,center,desired_fov,distance\n",
    "    \n",
    "    \n",
    "    cdelt_x = u.Quantity(str(np.abs(sc_for_cropping.header['cdelt1']))+sc_for_cropping.header['cunit1'])\n",
    "    cdelt_y = u.Quantity(str(np.abs(sc_for_cropping.header['cdelt2']))+sc_for_cropping.header['cunit2'])\n",
    "    center_ra_pix,center_dec_pix = [int(sc_for_cropping.wcs[:][:][0].world_to_pixel(c)[0]),int(sc_for_cropping.wcs[:][:][0].world_to_pixel(c)[1])]\n",
    "    PixFov = [int((FOV[0].to(u.pc)/(cdelt_x.to(u.rad)*d.to(u.pc))).value/2),int((FOV[1].to(u.pc)/(cdelt_x.to(u.rad)*d.to(u.pc))).value/2)] #they'll be in pixels, but I only need the int\n",
    "\n",
    "    print(\"Center:\",c,\"Pixel center:\",center_ra_pix,center_dec_pix,\"Pixel FOV:\",PixFov)\n",
    "    \n",
    "    pixels = np.zeros(np.shape(sc_for_cropping))           \n",
    "    \n",
    "    print(\"cropping cube. rotation:\",r,\"center:\",center,\"crop to:\",desired_fov)\n",
    "    \n",
    "    if(r!=0*u.deg):\n",
    "        #to save time\n",
    "        r_rad=r.to(u.rad).value\n",
    "        #Find the pixels that are outside the rectangular FOV\n",
    "        for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "            for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "\n",
    "                #The hypotenuse\n",
    "                hypo = np.sqrt(((lmj-center_dec_pix)**2) + (lmk-center_ra_pix)**2)\n",
    "\n",
    "                if (lmj-center_dec_pix!=0):\n",
    "                    ang = np.arctan(abs(lmk-center_ra_pix)/abs(lmj-center_dec_pix))#*u.rad#Find angle to the center\n",
    "                else:\n",
    "                    ang = np.pi/2#*u.rad\n",
    "                if(lmk>center_ra_pix and lmj>center_dec_pix):\n",
    "                    ang*=-1\n",
    "                elif(lmk<center_ra_pix and lmj<center_dec_pix):\n",
    "                    ang*=-1\n",
    "                elif(lmk>center_ra_pix and lmj<center_dec_pix):\n",
    "                    if ang > (np.pi/2-r_rad):\n",
    "\n",
    "                        ang= -r_rad+(np.pi-r_rad)-ang#coming from the opposite end of the ra axis now, but projecting still to 33 deg from north.\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                up_pixels = abs(hypo*np.cos(abs(r_rad+ang)))\n",
    "                side_pixels = abs(hypo*np.sin(abs(r_rad)+ang))\n",
    "\n",
    "                #Check if the pixels are inside the FOV\n",
    "                if(up_pixels<PixFov[0] and side_pixels<PixFov[1]):\n",
    "                    for lmi in range(np.shape(sc_for_cropping)[0]):\n",
    "                        pixels[lmi][lmj][lmk] = 1  # keep this pixel\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        # If the image is not rotated, the FOV is just a rectangle\n",
    "        \n",
    "        # Find all pixels in the fov\n",
    "        \n",
    "        for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "            for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "\n",
    "                up_pixels = abs(lmj-center_dec_pix)#Should not be over the fov in the upwards direction (relative to 0 degrees)\n",
    "                side_pixels = abs(lmk-center_ra_pix)#Should not be over the fov in the side-side direction (relative to 0 degrees)\n",
    "\n",
    "                if(up_pixels<PixFov[0] and side_pixels<PixFov[1]):\n",
    "                    for lmi in range(np.shape(sc_for_cropping)[0]):\n",
    "                        pixels[lmi][lmj][lmk] = 1  # keep this pixel\n",
    "                        \n",
    "    # Mask teh pixels outside the fov\n",
    "\n",
    "    bp = np.where(pixels!=1)\n",
    "    scCopy = sc_for_cropping.hdu\n",
    "    scCopy.data[bp]=np.nan\n",
    "    scP = SpectralCube.read(scCopy)\n",
    "    del scCopy\n",
    "    del bp\n",
    "    #Get right size by removing the nan pixels\n",
    " \n",
    "    scP.allow_huge_operations=True\n",
    "    datn = scP.hdu.data\n",
    "    dat_sum = np.nansum(datn,axis=0)\n",
    "    sx,sy,ex,ey=0,0,0,0\n",
    "    for lmi in range(np.shape(dat_sum[:,:])[0]):\n",
    "\n",
    "        # Go through a slice of the cube and find the first pixels with rael data\n",
    "        \n",
    "        # After this is done these will be non zero so break the loop\n",
    "        if(ey!=0 and sx!=0 and ex!=0 and sy!=0):\n",
    "            \n",
    "            break\n",
    "            \n",
    "        for lmj in range(np.shape(dat_sum[:,:])[1]):\n",
    "\n",
    "            if(sx==0):            \n",
    "                if(np.nanmean(dat_sum[lmi,:])>0 or np.nanmean(dat_sum[lmi,:])<0):\n",
    "                    sx=lmi\n",
    "\n",
    "            if(sy==0):\n",
    "                if(np.nanmean(dat_sum[:,lmj])>0 or np.nanmean(dat_sum[:,lmj])<0):\n",
    "                    sy=lmj\n",
    "\n",
    "            if(ex==0):\n",
    "                if(np.nanmean(dat_sum[np.shape(dat_sum[:,:])[0]-lmi-1,:])>0 or np.nanmean(dat_sum[np.shape(dat_sum[:,:])[0]-lmi-1,:])<0):\n",
    "                    ex=np.shape(dat_sum[:,:])[0]-lmi-1\n",
    "\n",
    "            if(ey==0):\n",
    "                if(np.nanmean(dat_sum[:,np.shape(dat_sum[:,:])[1]-lmj-1])>0 or np.nanmean(dat_sum[:,np.shape(dat_sum[:,:])[1]-lmj-1])<0):\n",
    "                    ey=np.shape(dat_sum[:,:])[1]-lmj-1\n",
    "\n",
    "            if(ey!=0 and ex!=0 and sx!=0 and sy!=0):\n",
    "                break\n",
    "    if crop_nans:\n",
    "        sc1 = scP[:,sx:ex,sy:ey] #get rid of the nan pixels\n",
    "    else:\n",
    "        sc1 = scP\n",
    "    scP_Hdu=sc1.hdu\n",
    "    \n",
    "    \n",
    "    # Also check if there are zeroes in place of nan values, which is done on some cubes\n",
    "    \n",
    "    zeros=((scP_Hdu.data[:,:,:]==0))\n",
    "    bp = np.where(zeros)\n",
    "    scP_Hdu.data[bp]=np.nan\n",
    "    scCopy = SpectralCube.read(scP_Hdu)\n",
    "    del scP_Hdu\n",
    "    \n",
    "    # Make the final cropped cube\n",
    "\n",
    "    cropped_sc = SpectralCube.read(scCopy.hdu)\n",
    "    \n",
    "    del scCopy\n",
    "    \n",
    "    data=sc_for_cropping.hdu.data\n",
    "    \n",
    "    print(\"Cropped to \",FOV, \"pixels\",sx,ex,sy,ey)\n",
    "    \n",
    "    return cropped_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f9a16-f527-4178-8281-e811e8b6ebae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Splice the partwise cubes back together:\n",
    "    \n",
    "def Splice_vels(Cube_Name_Load,istart=0,iend=2000,Save='',crop=list([[0,99999999999],[0,99999999999]])):\n",
    "    missed=False\n",
    "    hit=False\n",
    "    for i in range(0,iend):\n",
    "        gc.collect()\n",
    "        try:\n",
    "            Cube_Name_Load_p = str(i)+\"_\"+Cube_Name_Load\n",
    "            Cube_Name_Save = Cube_Name_Load\n",
    "\n",
    "            sc=SpectralCube.read((\"Spectral Cubes/\"+Cube_Name_Load_p))[:,crop[1][0]:crop[1][1],crop[0][0]:crop[0][1]]\n",
    "            hit=True\n",
    "            if istart ==0 and missed:\n",
    "                istart=i\n",
    "            print(\"Loaded\",Cube_Name_Load_p)\n",
    "            print\n",
    "            \n",
    "            # Define a header that we will form into the new header for the spliced cube\n",
    "            if i == istart:\n",
    "                reheader = sc.header\n",
    "                rewcs = sc.wcs\n",
    "\n",
    "            if i == istart:\n",
    "                scW=SpectralCube.read((\"Spectral Cubes/\"+Cube_Name_Load_p))[:,crop[1][0]:crop[1][1],crop[0][0]:crop[0][1]]\n",
    "                mask = scW.mask.include() # Need to create a mask because it doesn't get splcied\n",
    "                \n",
    "            else:\n",
    "                if i == istart+1:\n",
    "                    scW = np.concatenate((scW[:].hdu.data,sc[:].hdu.data),dtype = type(sc))\n",
    "                    mask = np.concatenate((mask[:],sc[:].mask.include()),dtype = type(sc[:].mask.include()))\n",
    "                else:\n",
    "                    scW = np.concatenate((scW[:],sc.hdu.data[:]),dtype = type(sc))\n",
    "                    mask = np.concatenate((mask[:],sc[:].mask.include()),dtype = type(sc[:].mask.include()))\n",
    "            scUNIT=copy.deepcopy(sc[0][0][0].unit)\n",
    "            del sc\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"-\"*60)\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "            missed=True\n",
    "            if hit:\n",
    "                break\n",
    "            \n",
    "    # This only matters for formatting reasons:\n",
    "    def duh(lol):\n",
    "        gp = np.where(lol!=np.nan)\n",
    "        lol[gp]=True\n",
    "        return lol # Anywhere that has data will be unmasked\n",
    "    \n",
    "    reheader[\"NAXIS3\"] = len(scW)\n",
    "    Full_Mask = LazyMask(function = duh,data = mask, wcs = rewcs)\n",
    "    \n",
    "    \n",
    "    scWsc = SpectralCube(data = scW,wcs = rewcs, header = reheader, mask = Full_Mask)# The spliced cube\n",
    "\n",
    "    scWsc.allow_huge_operations=True\n",
    "    scWsc = scWsc*scUNIT#Add unit back in\n",
    "\n",
    "    if Save !='':\n",
    "        scWsc.write(\"Spectral Cubes/\"+Save,overwrite=True)\n",
    "    else:\n",
    "        scWsc.write(\"Spectral Cubes/\"+Cube_Name_Save,overwrite=True)\n",
    "    del scWsc\n",
    "    del Full_Mask\n",
    "    del reheader\n",
    "    del scW\n",
    "    del rewcs\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f716a-64b9-429a-a2d6-a52e76813bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Velocity reprojection\n",
    "###\n",
    "        \n",
    "# Smooth the data (gaussian) to match the velocity resolution of other cubes\n",
    "\n",
    "def Repo_Velocity(scp,Cube_Name_Save,Cube_Information,vel_range=251*u.km/u.s):\n",
    "    \n",
    "    print(\"Start velocity reprojection of\" ,Cube_Name_Save)\n",
    "    \n",
    "    vel_prime = Cube_Information[\"desired_velocity_resolution\"] \n",
    "    target_velocity = Cube_Information[\"target_velocity\"]\n",
    "    Initial_vel = u.Quantity(str(np.abs(scp.header['cdelt3']))+scp.header['cunit3']) # The current velocity resolution\n",
    "    restfreq = Pointing_Information['restfreq']\n",
    "    \n",
    "    # Gaussian width = sqrt( new_vel^2 -  old_vel^2)\n",
    "    \n",
    "    G_width = np.sqrt((vel_prime.to(u.km/u.s)).value**2-(Initial_vel.to(u.km/u.s)).value**2) \n",
    "    \n",
    "    vel = np.arange((target_velocity - vel_range).value, (target_velocity + vel_range).value,vel_prime.to(u.km/u.s).value)*u.km/u.s #make the new velocity axis\n",
    "    scWsc_copy = scp\n",
    "    \n",
    "    # the factor converting the gaussian width to its FWHM\n",
    "    fwhm_factor = np.sqrt(8*np.log(2))\n",
    "\n",
    "    scWsc_copy = scWsc_copy.with_spectral_unit(u.km / u.s, velocity_convention='optical', rest_value=restfreq) # Make sure it has the right rest frequency\n",
    "    \n",
    "    # The reprojected velocity must be larger than the initial velocity or else this gives an error\n",
    "    if (G_width>0):\n",
    "\n",
    "        scWsc_copy = scWsc_copy.spectral_smooth(Gaussian1DKernel(G_width/fwhm_factor))#Preserves information from the pixels lost in downsampling\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"Smoothed to Gaussian Kernel of width\",G_width/fwhm_factor)\n",
    "\n",
    "    scWsc_copy = scWsc_copy.spectral_interpolate(spectral_grid=vel) # Match velocities to -250 251 range  \n",
    "\n",
    "    \n",
    "    Cube_Name_Save_new = Cube_Name_Save[0:len(Cube_Name_Save)-5]+\"_\"+str(vel_prime.value)+'_vel_res_'+'.fits'\n",
    "    \n",
    "    \n",
    "    scWsc_copy.write(\"Spectral Cubes/\"+Cube_Name_Save_new,overwrite=True)\n",
    "    \n",
    "    \n",
    "    print(\"Wrote reprojected cube to\",\"Spectral Cubes/\"+Cube_Name_Save_new)\n",
    "    \n",
    "    gc.collect()\n",
    "    del scWsc_copy\n",
    "\n",
    "    gc.collect()######################################################################\n",
    "    return Cube_Name_Save_new\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcb87f-fb6e-4179-b210-827905050697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e25a7b-87f6-44af-856d-b69ed52de240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after the reprojection, there may be duplicated data from the fact that it just fills in stuff \n",
    "# expecting new data to be there. This fixes it by checking to see if it matches other data, then removes\n",
    "# the repeated slices. in other words, it may be set to reproject to 0-500 km/s through the velocity channels\n",
    "# but there may only be data between 100-400, so it will fill it in with copied data channels. This will check for those.\n",
    "\n",
    "# Fix reprojected repeated pixels:\n",
    "\n",
    "# All these parameters only matter here because they are in the cube name\n",
    "# Line_Name = Line Name, Beam_Size=beam size, FOV=field of view, min_vel =  velocity resolution\n",
    "\n",
    "def Remove_Repeated_Pixels(Cube_Name_Load,crop_nans=False):\n",
    "    \n",
    "    Cube_Name_Save = \"Cropped_\"+Cube_Name_Load\n",
    "\n",
    "    scRRP = SpectralCube.read(\"Spectral Cubes/\"+Cube_Name_Load).with_spectral_unit(u.km/u.s,velocity_convention=\"radio\")\n",
    "    scRRP.allow_huge_operations=True\n",
    "    \n",
    "    sp=0 #starting pixel with real data (tbd)\n",
    "    \n",
    "    for lmi in range(len(scRRP)):\n",
    "        #Check to see if the slice has been repeated by the interpolation function\n",
    "        if(np.round(np.nanmean(scRRP[lmi].hdu.data),9)==np.round(np.nanmean(scRRP[lmi+1].hdu.data),9)):\n",
    "\n",
    "            sp = lmi+1\n",
    "        else:\n",
    "            print(\"Good data starting from channel\",sp,\"; start has been cropped to that channel\")\n",
    "            break\n",
    "            \n",
    "    l = len(scRRP)-1\n",
    "    ep=l\n",
    "    for lmi in range(l):\n",
    "        #Cehck again, starting from the end this time\n",
    "        if(np.round(np.nanmean(scRRP[l-lmi].hdu.data),9)==np.round(np.nanmean(scRRP[l-lmi-1].hdu.data),9)):\n",
    "            ep = l-lmi-1\n",
    "\n",
    "        else:\n",
    "            print(\"Good data ending at channel\",ep,\"; end has been cropped to that channel\")\n",
    "            break\n",
    "\n",
    "    #sp,ep, These are the start and stop slices where the actual unique data resides\n",
    "\n",
    "    scRRP.allow_huge_operations=True\n",
    "    scRRP = scRRP[sp:ep]\n",
    "\n",
    "    if (crop_nans):\n",
    "        center = scRRP.wcs[:][:][0].pixel_to_world(0,0)\n",
    "        print(\"Cropping nans...\")\n",
    "        scRRP = Crop_Around_Center(scRRP,0*u.deg,center=center,desired_fov=[99999999999,99999999999999]*u.pc,distance=1000*u.pc)\n",
    "\n",
    "\n",
    "    scP_Hdu=scRRP.hdu\n",
    "    zeros=((scP_Hdu.data[:,:,:]==0))\n",
    "    bp = np.where(zeros)\n",
    "    scP_Hdu.data[bp]=np.nan\n",
    "    scRRP = SpectralCube.read(scP_Hdu)\n",
    "\n",
    "\n",
    "    scRRP.allow_huge_operations=True\n",
    "    #print(scRRP.beam)\n",
    "    #print(scRRP.spectral_axis)\n",
    "    #print(\"Beam:\", scRRP.beam)\n",
    "    #print(\"Unit:\", scRRP.unit)\n",
    "\n",
    "    scRRP = scRRP.to(u.K)\n",
    "\n",
    "    scRRP.write((\"Spectral Cubes/\"+Cube_Name_Save),overwrite=True)\n",
    "    del scRRP\n",
    "    del scP_Hdu\n",
    "\n",
    "    print(\"Cropped cube saved as \",\"Spectral Cubes/\"+Cube_Name_Save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f53b6c-17d5-49e5-912f-ea93578a32c5",
   "metadata": {},
   "source": [
    "# Dendrogram Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f03c1b-9950-4f4c-8583-846e3626c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# d = astrodendro.Dendrogram.compute(datn,min_delta=m*delt_factor,min_value=m*5*(noise_factor),min_npix=beam_area_ratio.value*pix_thresh_factor)\n",
    "\n",
    "# min_delta = how many times the pixel brightness must be (multiplied by the noise) before a pixel is considered outside the current structure.\n",
    "# noise_requirement = how many times the noise we consider before allowing pixels\n",
    "# Minimum_Pixel_Requirement = how many pixels must there be before we call it a structure. I use the beam_area_ratio for this\n",
    "\n",
    "def Dendrogram_Calculation(Cube_Information,Minimum_Pixel_Requirement=1,min_delta=5,noise_requirement=5,Dendrogram_Save_Name='',manual_noise=0*u.K,image_args={},parts=[0]):\n",
    "\n",
    "    Line_Name = Cube_Information['File_Descriptor']\n",
    "    File_Name = Cube_Information['File_Name']\n",
    "    distance = Cube_Information['distance']\n",
    "    vel_prime = Cube_Information['velocity_scale']\n",
    "    center = Cube_Information['center']\n",
    "    Prime_Beam = (Cube_Information['beam_minor'].to(u.rad)*distance / u.rad).to(u.pc) #The current beam size in pc\n",
    "    pathCont = Cube_Information[\"Corresponding_Continuum\"]\n",
    "    beam_area_ratio = Cube_Information['beam_area_ratio']\n",
    "    \n",
    "    Cube_Information[\"Dendrogram_Noise_Threshold\"] =noise_requirement\n",
    "    Cube_Information[\"Minimum_Pixel_Requirement\"] =Minimum_Pixel_Requirement\n",
    "    Cube_Information[\"min_delta\"] =min_delta\n",
    "\n",
    "    ###### \n",
    "    ### prepare some data along with the dendrogram so we can better understand what we're looking at\n",
    "    ######\n",
    "    \n",
    "    # Load The SC\n",
    "    if parts ==[0]:\n",
    "        Qp = SpectralCube.read(\"Spectral Cubes/\"+File_Name).with_spectral_unit(u.km/u.s,velocity_convention=\"radio\") \n",
    "    else:\n",
    "        Qp = SpectralCube.read(\"Spectral Cubes/\"+File_Name).with_spectral_unit(u.km/u.s,velocity_convention=\"radio\")[parts[0]:parts[1],parts[2]:parts[3],parts[4]:parts[5]]\n",
    "    Qp.allow_huge_operations=True\n",
    "\n",
    "    Q = Qp.to(u.K)# Jy to Kelvin in case its not already in\n",
    "    sc = Q.unmasked_copy() # remove a mask if there is one \n",
    "    del Q \n",
    "    del Qp\n",
    "\n",
    "    # The cube data\n",
    "\n",
    "    scW = sc.wcs[:][:][0]\n",
    "    datn = sc.hdu.data\n",
    "    moment_0_sc  = sc.moment(order=0,how='slice')            # Calculate the Moment 0 map to use for plotting\n",
    "    moment_0_sc.write((\"Spectral Cubes/\"+Cube_Information[\"File_Name\"][0:len(Cube_Information[\"File_Name\"])-5]+\"_Moment_0\"+\".fits\"),overwrite=True)\n",
    "    header = sc.hdu.header\n",
    "    \n",
    "    #####\n",
    "    #####\n",
    "    # Reproject Continuum image to match the current image (this doesnt require any other data reduction steps since we already did those to the current SC)\n",
    "    #####\n",
    "    #####\n",
    "    \n",
    "    # Continuum image, for use in the column density maps\n",
    "    \n",
    "    scCont = spectral_cube.Projection.from_hdu(fits.open(\"Spectral Cubes/\"+pathCont)) # {Why was there a  [0] here?}\n",
    "    \n",
    "    try:\n",
    "        BUNIT = scCont.unit\n",
    "        print(\"The continuum has unit\",BUNIT)\n",
    "        scCont = scCont.to(u.Jy/u.beam) #Fails if unitless\n",
    "        BUNIT = scCont.unit\n",
    "        print(\"Change to:\",BUNIT)\n",
    "        print(\"verify unit length is non zero, length = \",len(str(BUNIT)),str(BUNIT)) #Fails if there is no unit\n",
    "\n",
    "    except:\n",
    "        print(\"The continuum is unitless; unit defaults to Jy/beam\")\n",
    "        scCont = scCont*u.Jy/u.beam #Some data was taken before BUNIT were implemented, but they are in Jy/beam\n",
    "        \n",
    "    Use_Dict = {'desired_beam_oversampling': \"NA\"} # this is irrelevant since the continuum is higher resolution than the data cubes\n",
    "    Use_Dict[\"distance\"] = distance # same as for the data cube\n",
    "    \n",
    "    Continuum_Information = Update_Cube_Information(Use_Dict,pathCont)\n",
    "    \n",
    "    \n",
    "    ## Reproject continuum \n",
    "    scCont.allow_huge_operations=True\n",
    "    Continuum = scCont.reproject(moment_0_sc.header)\n",
    "    del scCont\n",
    "    Continuum_Data  = Continuum.hdu.data\n",
    "    Cont_Save_File=(Continuum_Information[\"File_Name\"][0:len(Continuum_Information[\"File_Name\"])-5]+\"_Reproject_to_\"+Cube_Information['File_Descriptor']+\".fits\")\n",
    "    \n",
    "    # save the reprojected continuum     \n",
    "    Continuum.write((\"Spectral Cubes/\"+Cont_Save_File),overwrite=True)\n",
    "    Continuum_Information = Update_Cube_Information(Continuum_Information,Cont_Save_File)\n",
    "    Cube_Information[\"Corresponding_Continuum_Reprojected\"] = Cont_Save_File\n",
    "    \n",
    "    # calculate the flux based on the beam to find te column density\n",
    "    # flux = Jansky/beam/(pixels/beam)/(beam_physical_size in pc^2)*particle number density (using a factor to account for the other, non H2 particles in the 850um continuum)\n",
    "    pc_per_pixel_continuum = Cube_Information['spatial_scale_y']\n",
    "\n",
    "    # The flux, converting from Jy/beam, accounting for the beams physical size\n",
    "    flux = (Continuum_Data*Continuum[0][0].unit)/((Continuum_Information[\"beam_area_ratio\"]*(pc_per_pixel_continuum**2)).to(u.cm**2/u.beam))\n",
    "    \n",
    "    Column_Densities = Flux_to_Mass(flux,distance)*Num_per_kg  # Calculates the number-luminosity from the particle flux and converts it to mass using the conversion factor from Bolatto 2013a, in number/cm^2\n",
    "    \n",
    "    # Ratio of flux / column density (compare the continuum to teh line emission)\n",
    "    rm=moment_0_sc.hdu.data/Column_Densities\n",
    "    rmU = rm*moment_0_sc[0][0].unit # Just put the units back in\n",
    "    del rm\n",
    "    \n",
    "    #rmU = np.array(rmU/10**22,dtype='float64')# reduce to floating point scale because they cant handle nubers this big\n",
    "\n",
    "    ###### there are errors at low points, giving negative mass, so i exclude them if theyre below the noise\n",
    "\n",
    "    bp = np.where(Column_Densities<=7*10**22/u.cm**2)\n",
    "    bp2 = np.where(moment_0_sc.hdu.data < (np.nanstd(moment_0_sc.hdu.data,where= ((moment_0_sc.hdu.data>0)  | (moment_0_sc.hdu.data<0) )))) # below the Noise (K km/s)\n",
    "    Continuum_Data_Cropped = copy.deepcopy(Continuum_Data)\n",
    "    Continuum_Data_Cropped[bp] = np.nan # A version of the continuum data withuot the negative pixels\n",
    "    \n",
    "    Column_Densities_Cropped = copy.deepcopy(Column_Densities)\n",
    "    Column_Densities_Cropped[bp]=np.nan # Remove negative pixels from the column densities\n",
    "    \n",
    "    rmU[bp]=np.nan # Remove the negative pixels from either the moment 0 or the continuum\n",
    "    rmU[bp2]=np.nan\n",
    "\n",
    "    #######\n",
    "    #######\n",
    "    # Calculate the RMS noise of the cube (for the line)\n",
    "    #######\n",
    "    #######\n",
    "\n",
    "    Non_nan=((datn[0]>0)  | (datn[0]<0 )) # All the data that is not a nan value in the first (emissionless) channel of the cube\n",
    "\n",
    "    m = np.nanstd(datn[0],where= Non_nan)*sc[0][0][0].unit #Noise (K)\n",
    "    if np.sum(((datn[0]>0)  | (datn[0]<0 )))<100:\n",
    "        #check to see if there is actually data at this slice. if not, jump ahead\n",
    "        Non_nan=((datn[32]>0)  | (datn[32]<0 )) # All the data that is not a nan value in the first (emissionless) channel of the cube\n",
    "        m = np.nanstd(datn[32],where= Non_nan)*sc[0][0][0].unit #Noise (K)\n",
    "        print(\"Noise slice=\",32)\n",
    "    elif np.sum(((datn[0]>0)  | (datn[0]<0 )))<1000:\n",
    "        Non_nan=((datn[20]>0)  | (datn[20]<0 )) # All the data that is not a nan value in the first (emissionless) channel of the cube\n",
    "        m = np.nanstd(datn[20],where= Non_nan)*sc[0][0][0].unit #Noise (K)\n",
    "        print(\"Noise slice=\",20)\n",
    "        \n",
    "        \n",
    "    if manual_noise != 0*u.K:\n",
    "        m = manual_noise\n",
    "    Cube_Information[\"RMS_Noise\"]=m\n",
    "    \n",
    "    print(m,\"  RMS Noise for\",Cube_Information[\"File_Descriptor\"])\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######\n",
    "    ######\n",
    "    ######\n",
    "    ### Generate relevant plots before the dendrogram so we know what we're looking at\n",
    "    ######\n",
    "    ######\n",
    "    ######\n",
    "        \n",
    "    #def Make_Image_Plot(Title,Cube,vmin,vmax,rows=1,columns=1,index=1,show=False):\n",
    "\n",
    "    ###### Moment 0 and continuum plots\n",
    "    show=True\n",
    "    for key in image_args.keys():\n",
    "        if key ==\"SHOW\":\n",
    "            show=image_args[key]\n",
    "\n",
    "    Make_Image_Plot((Line_Name+\" Moment 0\"),\"Moment 0 Intensity (K km s^-1)\",moment_0_sc.hdu.data,wcs=moment_0_sc.wcs,vmin=0,vmax=np.nanmax(moment_0_sc.hdu.data),rows=1,columns=1,index=1,show=False,Cube_Info=Cube_Information,image_args=image_args)\n",
    "    \n",
    "    if show:\n",
    "        pylab.show()\n",
    "\n",
    "    Make_Image_Plot((Line_Name+\" Moment 0 PC\"),\"Moment 0 Intensity (K km s^-1)\",moment_0_sc.hdu.data,wcs=moment_0_sc.wcs,vmin=0,vmax=np.nanmax(moment_0_sc.hdu.data),rows=1,columns=1,index=1,show=False,Axis=\"PC\",Cube_Info=Cube_Information,image_args=image_args)\n",
    "    \n",
    "    Center_p = moment_0_sc.wcs.world_to_pixel(center)\n",
    "    pylab.annotate(text=\"Center\",fontsize=12,xy=(Center_p[0]+20, Center_p[1]+10), rotation=-20,color=\"gray\")  \n",
    "    pylab.annotate(text=\"x\",fontsize=12,xy=(Center_p[0], Center_p[1]), rotation=-10,color=\"gray\")  \n",
    "    if show:\n",
    "        pylab.show()\n",
    "\n",
    "    \n",
    "    Make_Image_Plot((Line_Name+\" Moment 0 PC\"),\"Moment 0 Intensity (K km s^-1)\",moment_0_sc.hdu.data,wcs=moment_0_sc.wcs,vmin=0,vmax=np.nanmax(moment_0_sc.hdu.data),rows=1,columns=1,index=1,show=False,Axis=\"PC\",Cube_Info=Cube_Information,image_args=image_args)\n",
    "    \n",
    "    Center_p = moment_0_sc.wcs.world_to_pixel(center)\n",
    "    pylab.annotate(text=\"Center\",fontsize=12,xy=(Center_p[0]+20, Center_p[1]+10), rotation=-25,color=\"gray\")  \n",
    "    pylab.annotate(text=\"x\",fontsize=12,xy=(Center_p[0], Center_p[1]), rotation=-10,color=\"gray\")  \n",
    "    if show:\n",
    "        pylab.show()\n",
    "\n",
    "    \n",
    "    Make_Image_Plot((Line_Name+\"Column Density\"),\"Column Density (# cm^-2)\",Column_Densities,wcs=moment_0_sc.wcs,vmin=float(np.nanmin(Column_Densities.value)),vmax=float(np.nanmax(Column_Densities.value)),rows=1,columns=1,index=1,show=False,Axis=\"PC\",Cube_Info=Cube_Information) # Use the full line SC information here becasue I dont need specific things\n",
    "    \n",
    "    pylab.annotate(text=\"Center\",fontsize=12,xy=(Center_p[0]+20, Center_p[1]+10), rotation=-20,color=\"gray\")  \n",
    "    pylab.annotate(text=\"x\",fontsize=12,xy=(Center_p[0], Center_p[1]), rotation=-10,color=\"gray\")  \n",
    "    if show:\n",
    "        pylab.show()\n",
    "\n",
    "\n",
    "    Make_Image_Plot((Line_Name+\" Moment 0 over Column Density of the Continuum\"), \"Ratio (K km s^-1 over (# cm^-2))\",rmU.value,wcs=moment_0_sc.wcs,vmin=np.nanmean(rmU.value)*.5,vmax=abs(np.nanmean(rmU.value))*8,rows=1,columns=1,index=1,show=False,Axis=\"PC\",Cube_Info=Cube_Information)\n",
    "    \n",
    "    pylab.annotate(text=\"Center\",fontsize=12,xy=(Center_p[0]+20, Center_p[1]+10), rotation=-20,color=\"gray\")  \n",
    "    pylab.annotate(text=\"x\",fontsize=12,xy=(Center_p[0], Center_p[1]), rotation=-10,color=\"gray\")  \n",
    "    if show:\n",
    "        pylab.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ############\n",
    "    # Calculate the dendrogram 3d image of all the structures in the data set, based on the pixel, noise, and delta parameters\n",
    "    ############\n",
    "    \n",
    "    Delta_Factor = m.value*min_delta # A multiple of the noise, this is how much a pixel's brightness must differ before it is considered separate from teh current structure\n",
    "    min_value = m.value*noise_requirement # The minimum value of a structure. If this is positive, then only emission will be consdered.\n",
    "    min_npix = beam_area_ratio.value*Minimum_Pixel_Requirement # How many pixels before a structure is resolved. I say they must have at least enough pixels to fill the beam so tehy are not just artifacts.\n",
    "    \n",
    "    print(\"Computing Dendrogram...\")\n",
    "    \n",
    "    Dendrogram = astrodendro.Dendrogram.compute(datn,min_delta=Delta_Factor,min_value=min_value,min_npix=min_npix) \n",
    "    \n",
    "    \n",
    "    Save_Dend=Line_Name+\"_\"+File_Name[0:len(File_Name)-5]+\"_Dendrogram\"\n",
    "    \n",
    "    \n",
    "    if(Dendrogram_Save_Name==''):\n",
    "        Dendrogram.save_to((\"Dendrograms/\"+Save_Dend),\"fits\")\n",
    "    else:\n",
    "        Dendrogram.save_to((\"Dendrograms/\"+Dendrogram_Save_Name),\"fits\")\n",
    "\n",
    "    Cube_Information[\"Corresponding_Dendrogram\"] = Save_Dend\n",
    "    \n",
    "    # Show the dendrogram\n",
    "    p1=Dendrogram.plotter() \n",
    "    v1 = Dendrogram.viewer()\n",
    "    \n",
    "    v1.show()\n",
    "        \n",
    "    print(\"Dendrogram for\",Cube_Information[\"File_Descriptor\"],\"saved to\",\"Dendrograms/\"+Save_Dend+\".fits\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef6616-75de-4086-a2d2-87bdd874b6ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PPV statistics\n",
    "\n",
    " This function uses the dendrogram as an input and calculates all the quantities\n",
    " such as size, linewidth, Column Density, RMS velocity (velocity dispersion, a measure \n",
    " of turbulence), Luminosity (using the distance and the flux), Distance to structure (only\n",
    " matters for the CMZ), the Moment0_Flux, and the V_RMS_error\n",
    "\n",
    " \n",
    " These are all the things calculated in the Shetty 2012 paper\n",
    "\n",
    " To exclude incomplete, unresolvable, or otherwise nan strucutres, we use some exclusion properties:\n",
    " \n",
    " 1. Don't allow any structure under 3 pc in size (the smallest beam size) {now the min size is zero because i set the pixel requirement higher, but it used to be this}\n",
    " \n",
    " 2. Don't allow any structure over 18 pc\n",
    " \n",
    " 3. Don't allow structures that are only visible in 2d\n",
    " \n",
    " 4. In the dendrogram calculation, there are additional restrictions placed on the amount of minimum pixels in a stuctures and the noise, and     noise-delta. I use minimum 5 sigma noise, 5 sigma as delta, and require there to be enough pixels to fill a whole beam (the beam_area_ratio).\n",
    "\n",
    "\n",
    "Returns np.array(SizeA), np.array(SigmaA), np.array(CDA) ,np.array(LuminA) ,np.array(SIDS) , np.array(MOM0_FLUX) , np.array(Distances), np.array(V_rms_err) for each structure in the dendrogram and returns them in the form [[][]], where [Leaves][Branches] are the different kinds of structures in the dendrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6dd73-8feb-4243-a036-56533fba9d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Continuum is in Jansky/Beam, Line data should have the unit specified in the metadata as 'data_unit'\n",
    "\n",
    "#Dendrogram=the computed dendorgam, \n",
    "#Cube = the SC associated with the dendrogram\n",
    "#Cube_Information = defined above\n",
    "#Max_Size=the maximum allowed size for a strcutre in u.pc\n",
    "#Min_size = the minthra-carta jk it means the minimum size for a structure in u.pc hahahahahaha can you tell its 2 am right now?\n",
    "#Trunks= True/False if you want/dont want to include the dendrogram trunks as branches\n",
    "#edge_cases = accept structures that seem to go outside the cube? True/False\n",
    "#Distance calculation= for the CMZ, the distance from the center may be a significant portion of the 8170 distance. I created a function to calculate teh aproximate distance\n",
    "\n",
    "\n",
    "# Note: The column density information is not yet fully bug tested, and it relies on continuum data that came from Bolatto 2013a, and a conversion factor that must be estimated; \n",
    "# do not use it as gospel\n",
    "                        \n",
    "def Dendrogram_Stats(Dendrogram,Cube,Continuum_Cube,Continuum_Information,Cube_Information,Trunks=True,min_size=0*u.pc,max_size=18*u.pc,edge_cases=False,distance_calculation=False,r_err=False):\n",
    "    \n",
    "    #initial the statistacs to return, in [[leaves], [branches]]format\n",
    "    Size,RMS_Velocity,V_rms_err,Luminosity,CDs,SIDs,MOM0_FLUX,Distances = [[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]\n",
    "    R_errs=[[],[]]\n",
    "    #Size,RMS_Velocity,V_rms_err,Luminosity,CDs,SIDs,MOM0_FLUX,Distances = np.array([[],[],[],[]],dtype=type(1*u.pix)),np.array([[],[],[],[]],dtype=type(1*u.pix)),np.array([[],[]],dtype=type(1*u.pix)),np.array([[],[]],dtype=type(1*u.pix)),np.array([[],[]],dtype=type(1*u.pix)),np.array([[],[]],dtype=type(1*u.pix)),np.array([[],[]],dtype=type(1*u.pix)),np.array([[],[]],dtype=type(1*u.pix))\n",
    "\n",
    "    dist_val=Cube_Information['distance']  #convert to the value in pc\n",
    "    distance = dist_val\n",
    "    center = Cube_Information[\"center\"]\n",
    "    m = Cube_Information[\"RMS_Noise\"]\n",
    "    d_copy = Dendrogram #make on that i can edit \n",
    "        \n",
    "    center_ra_pix,center_dec_pix = int(Cube.wcs[:][:][0].world_to_pixel(center)[0]),int(Cube.wcs[:][:][0].world_to_pixel(center)[1])\n",
    "    \n",
    "    LineData = Cube.hdu.data\n",
    "    sliced= LineData[int(len(LineData)/2)]\n",
    "    CubeShape = np.shape(sliced) #the shape of a slice of the cube\n",
    "    \n",
    "    ContData = Continuum_Cube.hdu.data\n",
    "    \n",
    "    \n",
    "    # get properties for the moment 2 map\n",
    "    cube = Cube #copy\n",
    "    chans,ny,nx = cube.hdu.data.shape\n",
    "    bunit = u.Unit(cube.header['bunit'])\n",
    "    cunit = u.Unit(cube.header['cunit3'])\n",
    "    cdelt = cube.header['cdelt3']*cunit\n",
    "    crval = cube.header['crval3']*cunit\n",
    "    crpix = cube.header['crpix3']\n",
    "    pix_cen = np.array([((i+crpix-1)*cdelt+crval).value for i in np.arange(chans)])*cunit\n",
    "    pix_cen = np.array([np.full((ny,nx), cen) for cen in pix_cen])*pix_cen.unit\n",
    "\n",
    "    # mask cube\n",
    "    min_snr = Cube_Information[\"Dendrogram_Noise_Threshold\"]            # check dendrogram threshold \n",
    "    m=Cube_Information[\"RMS_Noise\"]\n",
    "    cube.hdu.data[cube.hdu.data*bunit<min_snr*m] = np.nan\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #Find the part of the cube that actually has data (useless if the nans are cropped but i have this anyway)\n",
    "    \n",
    "    DataShape=[[0,0],[0,0]]\n",
    "\n",
    "    for lmi in range(CubeShape[0]):\n",
    "        allData=np.nansum(sliced[lmi])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[0][0] = lmi+1 # teh left edge\n",
    "            break\n",
    "    for lmi in range(CubeShape[0]):\n",
    "        allData=np.nansum(sliced[CubeShape[0] - lmi -1])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[0][1] = CubeShape[0] - lmi -1 #the right edge\n",
    "            break\n",
    "    for lmi in range(CubeShape[1]):\n",
    "        allData=np.nansum(sliced[:,lmi])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[1][0] = lmi+1 #the bottom\n",
    "            break\n",
    "    for lmi in range(CubeShape[1]):\n",
    "        allData=np.nansum(sliced[:,CubeShape[1] - lmi -1])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[1][1] = CubeShape[1] - lmi -1 #the top\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # Go through every structure identivfied\n",
    "    \n",
    "    for t in Dendrogram.all_structures: \n",
    "\n",
    "        I = t.indices() #the pixel values in x, y of the structure\n",
    "        \n",
    "        \n",
    "                \n",
    "        Cont = True\n",
    "        if t.is_branch:\n",
    "            \n",
    "            # If the structure is a branch with no parents, it is a trunk\n",
    "                if t.parent==None:\n",
    "                    \n",
    "                    if(Trunks):\n",
    "                        Cont = True\n",
    "                    else:\n",
    "                        Cont = False\n",
    "                else:\n",
    "                    Cont=True\n",
    "        \n",
    "        # Exclude structures that go off the edge of the data and are therefore not fully resolved\n",
    "        \n",
    "        if edge_cases == False:\n",
    "            \n",
    "            #For non-rotated data cubes, it is really easy to see if a structure is on the edge of the cube\n",
    "            \n",
    "            if Cube_Information['target_image_rotation'] == 0*u.deg :\n",
    "            \n",
    "            \n",
    "                for lmi in range(len(I[0])):\n",
    "                        \n",
    "                    # cehck if there are any nan values nearby or any edges of the cube nearby\n",
    "                    \n",
    "                    if(I[1][lmi] <= DataShape[0][0] or I[1][lmi] >= DataShape[0][1] or I[2][lmi] <= DataShape[1][0] or I[2][lmi] >= DataShape[1][1]):\n",
    "                        #print(I[1][lmi], DataShape[0][0], I[1][lmi] , DataShape[0][1] , I[2][lmi],DataShape[1][0], I[2][lmi],DataShape[1][1])\n",
    "                        #print(I[1][lmi] <= DataShape[0][0], I[1][lmi] >= DataShape[0][1] , I[2][lmi] <= DataShape[1][0] , I[2][lmi] >= DataShape[1][1],I,\"A\")\n",
    "                        Cont=False\n",
    "                        break\n",
    "                        \n",
    "                    # Check if they are ON the edge also\n",
    "                    \n",
    "                    elif(I[1][lmi] <= 1 or I[1][lmi] >= CubeShape[0] or I[2][lmi] <= 1 or I[2][lmi] >= CubeShape[1]):\n",
    "                        #print(I[1][lmi] <= 1 or I[1][lmi] >= CubeShape[0] or I[2][lmi] <= 1 or I[2][lmi] >= CubeShape[1],\"B\")\n",
    "                        Cont=False\n",
    "                        break\n",
    "                        \n",
    "            else:          \n",
    "            \n",
    "            #For rotated cubes, I check every direction to make sure there are no nan values beside them\n",
    "            \n",
    "                for lmi in range(len(I[0])):\n",
    "                    NansNE=0\n",
    "                    NansSE=0\n",
    "                    NansNW=0\n",
    "                    NansSW=0\n",
    "                    Length = 3\n",
    "                    for lmj in range(Length):\n",
    "                        #Check four 45 degree prongs from each point and see if they have nans. If that happens its too close to the boundary or the data is bad\n",
    "                        try:\n",
    "\n",
    "                            if(sliced[I[1][lmi]+lmj,I[2][lmi]-lmj]>0 or sliced[I[1][lmi]+lmj,I[2][lmi]-lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansSE+=1\n",
    "                            if(sliced[I[1][lmi]-lmj,I[2][lmi]-lmj]>0 or sliced[I[1][lmi]-lmj,I[2][lmi]-lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansSW+=1\n",
    "                            if(sliced[I[1][lmi]-lmj,I[2][lmi]+lmj]>0 or sliced[I[1][lmi]-lmj,I[2][lmi]+lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansNW+=1\n",
    "                            if(sliced[I[1][lmi]+lmj,I[2][lmi]+lmj]>0 or sliced[I[1][lmi]+lmj,I[2][lmi]+lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansNE+=1\n",
    "                        except:\n",
    "                            #only fails if the I goes close to the boundary of the cube and tries to get a pixel outside the cube\n",
    "                            Cont = False\n",
    "                            break\n",
    "                    #count the number of nans nearby:\n",
    "                        \n",
    "                    if(NansNE>2 or NansNW>2 or NansSE>2 or NansSW>2):\n",
    "                        Cont=False\n",
    "                        break\n",
    "\n",
    "        # Assuming the structure is not an error, continue:\n",
    "        \n",
    "        if(Cont):\n",
    "            \n",
    "            #The metadata is used to calculate various properties. it passes the spatial scale, data unit, and such, for Jy->K calculation\n",
    "            metadata=Cube_Information\n",
    "            \n",
    "            # calcuate the properties of the using this function from the astrodendro library\n",
    "            s = PPVStatistic(t,metadata=metadata) \n",
    "            \n",
    "            s_radius = s.radius #Give the size in degrees\n",
    "            s_v_rms = s.v_rms #in u.km/u.s\n",
    "            pc_per_pixel = metadata['spatial_scale_x']\n",
    "            #Parsec_Size = (float(s_radius.to(u.rad).value*dist_val)) # Convert to parsecs using the distance in Pc\n",
    "            # Parsec_Size = (s_radius*u.pix**2*pc_per_pixel).to(u.pc) # If needed, Convert to parsecs using the spatial scale in Pc/pixel\n",
    "            Parsec_Size = ((s.radius).to(u.rad)*dist_val/u.rad).to(u.pc) # convert to pc instead of deg/pixel\n",
    "            #also check to make sure the size is greater than the minimum size (1/3 the beam) any less and it will be too much noise\n",
    "            #and make sure the rms velocity is non-zero. otherwise, its not a 3d structure\n",
    "            \n",
    "            if(Parsec_Size<max_size and Parsec_Size>min_size and s_v_rms>.01*u.km/u.s):\n",
    "            \n",
    "                '''unused\n",
    "                # A slightly different radius calculation, using the mean-projected area . from shetty 2012\n",
    "                nproj_pix=len(set(zip(*tuple(I[i] for i in [1,2]))))\n",
    "                #Intensity mean-weighted velocity:\n",
    "                v_IWM = np.nansum(LineData[I]*(DataVel[I[0]])/u.km*u.s)/np.nansum(LineData[I])\n",
    "                # calculating the Rms velocity manually\n",
    "                sig_Sh = np.sqrt(np.nansum(LineData[I]*((DataVel[I[0]])/u.km*u.s-v_IWM)**2)/np.nansum(LineData[I])) \n",
    "                '''\n",
    "                \n",
    "                #column density\n",
    "                #Need the flux from the continuum\n",
    "                #Convert to Jansky from Jansky per beam:\n",
    "\n",
    "                Cont_Flux=0\n",
    "\n",
    "                proj = tuple(set(zip(*tuple(I[i] for i in [1,2]))))\n",
    "                for lmi in range(len(proj)):\n",
    "\n",
    "                    Cont_Flux+=ContData[proj[lmi]]\n",
    "                Cont_Flux = Cont_Flux*Continuum_Cube[0][0].unit # add the unit back in\n",
    "                \n",
    "                # assuming the continuum is reprojected to the cube, i can use the beam_area_ratio of the cube here:\n",
    "                # Convert from Jansky / beam. The beam is changed from FWHM to Gaussian\n",
    "                #Cont_Flux=Cont_Flux/(((metadata['beam_area_ratio']*(pc_per_pixel**2)).to(u.cm**2/u.beam))*(2*np.sqrt(2*np.log(2))))\n",
    "                \n",
    "                Cont_Flux=Cont_Flux/(((metadata['beam_area_ratio']*(pc_per_pixel**2)).to(u.cm**2/u.beam)))\n",
    "                 \n",
    "                #Fro FWHM beam:\n",
    "                Dust_Column = Flux_to_Mass(Cont_Flux,distance)*Num_per_kg   # calculate column density, from flux to mass\n",
    "\n",
    "                #check for divide by zero errors:\n",
    "                if(str(Dust_Column) == str(np.nan) or str(Dust_Column)==str(np.inf)):\n",
    "                    Dust_Column=0\n",
    "                lum = Flux_to_Lum(s.flux,distance)\n",
    "                s_flux = s.flux\n",
    "\n",
    "                Index = tuple(I[i] for i in [0,1,2])\n",
    "                K_Km_s_Flux=np.nansum(LineData[Index]*metadata[\"velocity_scale\"]*Cube[0][0][0].unit)#Find the total flux from the structures in K km/s, assuming the input data is in K as it should be, \n",
    "                \n",
    "                if distance_calculation:\n",
    "                    Distance = np.sqrt((float(s.x_cen/u.pix)-center_ra_pix)**2+(float(s.y_cen/u.pix)- center_dec_pix)**2).to(u.rad)*metadata['spatial_scale']*dist_val#pc dist from barycenter\n",
    "                else:\n",
    "                    Distance = np.nan\n",
    "                \n",
    "                NF=5\n",
    "                iterations=5\n",
    "                #V_err= Get_V_rms_err(dend1=d_copy,idx=int(t.idx),struct=t,m=m.value,NF=NF,iterations=iterations,metadata=metadata)\n",
    "                #Get V rms error by taking the 84th and 16th percentile of the second moment of the structure.\n",
    "                '''\n",
    "                flat_pixels=[] #get the 2 projection fo the structure:\n",
    "                for lmi in range(len(I[0])):\n",
    "                    if [I[lmi][0] ,  I[lmi][1]] not in flat_pixels:\n",
    "                        flat_pixels.append([I[lmi][0] ,  I[lmi][1]])\n",
    "                '''\n",
    "                #C1 = copy.deepcopy(Cube[I]) #get the data from the structure\n",
    "                #mom2 = C1.moment2()\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                #velocity_scale = metadata['velocity_scale']\n",
    "\n",
    "                #dv = velocity_scale\n",
    "                '''\n",
    "                dv = s.velocity_scale if s.velocity_scale is not None else u.pixel\n",
    "                ax = [0, 0, 0]\n",
    "                ax[s.vaxis] = 1\n",
    "                \n",
    "                test =  dv * np.sqrt(s.stat.mom2_along(tuple(ax)))\n",
    "                mom2 = s.stat.mom2()\n",
    "                #print(mom2)\n",
    "                \n",
    "                w = np.atleast_2d(tuple(ax)).astype(float)\n",
    "                for row in w:\n",
    "                    row /= np.linalg.norm(row)\n",
    "                #print(np.dot(w, mom2))\n",
    "                result = np.dot(np.dot(w, mom2), w.T)\n",
    "                #print(result,dv * np.sqrt(result))\n",
    "                \n",
    "                #mean/median of per pixel moment 2\n",
    "                ###################################################################################################\n",
    "\n",
    "                mean = np.nanmean(mom2)\n",
    "                std  = np.nanstd(mom2)\n",
    "                p16,median,p84 = np.nanpercentile(mom2, (16,50,84))\n",
    "                \n",
    "                print(\"1\",mean,s_v_rms,p16,median,p84,test)\n",
    "                #8.320892732487192 9.807691033727796 km / s 8.320892732487192 8.320892732487192 8.320892732487192 9.807691033727796 km / s\n",
    "                #4.632530923209886 9.807691033727796 km / s -5.917875023908338 6.736166347151977 13.60092186257982 9.807691033727796 km / s\n",
    "                #9.049809177357027 9.807691033727796 km / s 3.9121341645280565 7.12497664499009 13.60092186257982 9.807691033727796 km / s\n",
    "\n",
    "                V_err = std\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                \n",
    "                \n",
    "\n",
    "                mask    = t.get_mask() #get the pixel values from the structure\n",
    "                mcube   = np.nan_to_num(cube.hdu.data*mask)*bunit #Make a cube containing only the structure\n",
    "\n",
    "                mom0 = moment0(mcube,             cdelt)\n",
    "                mom1 = moment1(mcube, mom0,       cdelt, pix_cen)\n",
    "                mom2 = moment2(mcube, mom0, mom1, cdelt, pix_cen)\n",
    "\n",
    "                mean = np.nanmean(mom2.value)\n",
    "                std  = np.nanstd(mom2.value)\n",
    "                p16,median,p84 = np.nanpercentile(mom2.value, (16,50,84))\n",
    "                \n",
    "                #print(\"2\",mean,s_v_rms,p16,median,p84,std)\n",
    "                \n",
    "                V_err = std*u.km/u.s\n",
    "\n",
    "                ###################################################################################################3\n",
    "                if r_err:\n",
    "                    R_err= (Get_R_err(dend1=d_copy,idx=int(t.idx),struct=t,m=m.value,NF=NF,iterations=iterations,metadata=metadata).to(u.rad)*dist_val/u.rad).to(u.pc) # convert to pc instead of deg/pixel\n",
    "                else:\n",
    "                    R_err = 0\n",
    "                \n",
    "                \n",
    "                if(t.is_leaf):\n",
    "                    # add teh value to the leaf axis: \n",
    "                    \n",
    "                    \n",
    "                    Size[0].append(Parsec_Size)\n",
    "                    RMS_Velocity[0].append(s_v_rms)\n",
    "                    V_rms_err[0].append(V_err) \n",
    "                    Luminosity[0].append(lum) \n",
    "                    CDs[0].append(Dust_Column) \n",
    "                    SIDs[0].append(t.idx) \n",
    "                    MOM0_FLUX[0].append(K_Km_s_Flux) \n",
    "                    Distances[0].append(Distance)\n",
    "                    R_errs[0].append(R_err)\n",
    "                \n",
    "\n",
    "                    \n",
    "                if(t.is_branch\t):\n",
    "                    # add teh value to the branch axis: \n",
    "                    \n",
    "                    \n",
    "                    Size[1].append(Parsec_Size)\n",
    "                    RMS_Velocity[1].append(s_v_rms)\n",
    "                    V_rms_err[1].append(V_err) \n",
    "                    Luminosity[1].append(lum) \n",
    "                    CDs[1].append(Dust_Column) \n",
    "                    SIDs[1].append(t.idx) \n",
    "                    MOM0_FLUX[1].append(K_Km_s_Flux) \n",
    "                    Distances[1].append(Distance)\n",
    "                    R_errs[1].append(R_err)\n",
    "                    \n",
    "                    \n",
    "                del s\n",
    "                    \n",
    "                    \n",
    "    if r_err:\n",
    "        return Size,RMS_Velocity,V_rms_err,Luminosity,CDs,SIDs,MOM0_FLUX,Distances,R_errs\n",
    "    else:\n",
    "        return Size,RMS_Velocity,V_rms_err,Luminosity,CDs,SIDs,MOM0_FLUX,Distances\n",
    "\n",
    "                        \n",
    "                        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f09f78-af4d-43be-82fc-296bf343ca64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7341e818-e621-45ae-9e87-691a207162bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2668f26f-6e69-4392-8d5a-eea5398b24e3",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92df41c-3d72-4a28-bbaa-bbbae55e77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This is a mass conversion factor for CO 3-2 to H2 calibrated using the 850 Ghz dust continuum\n",
    "# It will not be used widly, but if it was I would need a relation factor for each molecular based \n",
    "# on their relative abundance, and their line transition ratio.\n",
    "# This has also been divided by two to account for the higher metallicity of NGC253 as compared to the CMZ\n",
    "# as Krieger did in 2020\n",
    "\n",
    "a_850 = 6.7*10**19*u.erg/u.s/u.Hz/u.M_sun #6.7+-1.7, from Bolatto 2013a\n",
    "\n",
    "a_850 = 6.7*10**19*u.erg/u.s/u.Hz/(1*u.M_sun).to(u.kg) #6.7+-1.7, from Bolatto 2013a\n",
    "\n",
    "\n",
    "def Flux_to_Mass(flux, dist, Lum_per_mass_factor=a_850):\n",
    "    \n",
    "    #Here is the manual process to convert to ergs\n",
    "    J_to_e = 10**-23*u.erg/u.s/u.cm**2/u.Hz/u.Jy #Jansky to flux in erg/(s cm^2 Hz)\n",
    "    flux_erg = flux*J_to_e\n",
    "    #flux_erg = flux.to(u.erg/u.cm**2/u.s/u.Hz) # doesnt work\n",
    "    #here is the conversion factor for Mpc to cm\n",
    "    #Mpc_to_cm = 3.086*10**24 * u.cm/u.Mpc\n",
    "    dist_cm=dist.to(u.cm)\n",
    "    \n",
    "    # Now use the distance and flux to calculate the luminosity, then use that and the conversion factor to find the mass\n",
    "    \n",
    "    # L = 4pi*r2 * Flux\n",
    "    \n",
    "    L = 4*np.pi*(dist_cm)**2*flux_erg #Megaparsec is converted to cm\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    M_mol = L/Lum_per_mass_factor #Just in Solar mass*1.989*10**30*u.kg/u.M_sun #Determines mass of the cont for 850 in kg\n",
    "    \n",
    "    return M_mol\n",
    "\n",
    "        \n",
    "# This assumes the input flux will be in Jansky, which the astrodendro library defaults to.\n",
    "# And make sure the input distance is in Mpc\n",
    "\n",
    "def Flux_to_Lum(flux, dist):\n",
    "    \n",
    "    #Here is the manual process to convert to ergs\n",
    "    #J_to_e = 10**-23*u.erg/u.s/u.cm**2/u.Hz/u.Jy #Jansky to flux in erg/(s cm^2 Hz)\n",
    "    J_to_e = 10**-23*u.erg/u.s/u.cm**2/u.Hz/u.Jy #Jansky to flux in erg/(s cm^2 Hz)\n",
    "\n",
    "    flux_erg = flux*J_to_e\n",
    "    #flux_erg = flux.to(u.erg/u.cm**2/u.s/u.Hz) # doesnt work\n",
    "    #here is the conversion factor for Mpc to cm\n",
    "    #Mpc_to_cm = 3.086*10**24 * u.cm/u.Mpc\n",
    "    dist_cm=dist.to(u.cm)\n",
    "    \n",
    "    # Now use the distance and flux to calculate the luminosity, then use that and the conversion factor to find the mass\n",
    "    \n",
    "    # L = 4pi*r2 * Flux\n",
    "    \n",
    "    L = 4*np.pi*(dist_cm)**2*flux_erg #Return the luminosity in Erg\n",
    "\n",
    "    \n",
    "    return L\n",
    "'''\n",
    "def Flux_to_Lum(flux, dist):\n",
    "    \n",
    "    #Mpc_to_cm = 3.086*10**24 * u.cm/u.Mpc\n",
    "    #dist_cm=dist.to(u.cm)\n",
    "    \n",
    "    # Now use the distance and flux to calculate the luminosity, then use that and the conversion factor to find the mass\n",
    "    # L = 4pi*r2 * Flux\n",
    "    \n",
    "    L = 4*np.pi*(dist)**2*flux #Return the luminosity in K km/s *pc^2\n",
    "\n",
    "    \n",
    "    return L\n",
    "'''\n",
    "\n",
    "# gets the RMS noise of a structure (estimation)\n",
    "# TO do this, we apply a avariation by adding a noise-structure to the structure and finding the std of multiple of these noise-added structures (bootstrapping method)\n",
    "'''\n",
    "def Get_V_rms_err(mom2,dend1,struct,idx,m,NF,iterations,metadata):\n",
    "    \n",
    "    \n",
    "    vs=[] # an array of the calculated V_RMS\n",
    "    np.random.seed((99)**2*123)\n",
    "    \n",
    "    for llll in range(iterations):\n",
    "        \n",
    "        s = dend1.__getitem__(idx) # Get the structure from the dendrogram\n",
    "        \n",
    "        npixels = np.product(np.shape(s.values()))\n",
    "        \n",
    "        additional_noise = np.random.normal(0., m*NF, npixels)\n",
    "        additional_noise = np.reshape(additional_noise, np.shape(s.values()))\n",
    "        \n",
    "        # add or subract noise to the values and calculate the v rms, them find the std of that array and\n",
    "        # call that the uncertainty in v rms for a structure\n",
    "        dat1P = dend1.data[s.indices()] # a copy of the data\n",
    "        dend1.data[s.indices()]+= additional_noise #add teh noise\n",
    "        \n",
    "        s = dend1.__getitem__(idx)\n",
    "        \n",
    "        vs.append(float(PPVStatistic(s,metadata=metadata).v_rms.value))\n",
    "        dend1.data[s.indices()]= dat1P # reset the dend data\n",
    "        \n",
    "        dend1.data[s.indices()]-= additional_noise # subtract the noise also\n",
    "        s = dend1.__getitem__(idx)\n",
    "        \n",
    "        vs.append(float(PPVStatistic(s,metadata=metadata).v_rms.value))\n",
    "        \n",
    "        UNIT = PPVStatistic(s,metadata=metadata).v_rms.unit\n",
    "        \n",
    "        del s\n",
    "        \n",
    "    v_rms_std = np.nanstd(vs)*UNIT\n",
    "    #print(vs*UNIT,v_rms_std)\n",
    "    return v_rms_std\n",
    "\n",
    "\n",
    " mean/median of per pixel moment 2\n",
    "###################################################################################################\n",
    "\n",
    "for co,CO in lines.items():\n",
    "    for gal,GAL in galaxies.items():\n",
    "\n",
    "        from astropy.table.column import Column\n",
    "\n",
    "        catalog    = dendrograms[co][gal]['catalog']\n",
    "\n",
    "        lw = {'mean': [], 'std': [], 'median': [], '16th': [], '84th': []}\n",
    "        for idx in tqdm(moms[co][gal]['mom0'].keys()):\n",
    "            mom2 = moms[co][gal]['mom2'][idx]\n",
    "            mean = np.nanmean(mom2.value)\n",
    "            std  = np.nanstd(mom2.value)\n",
    "            p16,median,p84 = np.nanpercentile(mom2.value, (16,50,84))\n",
    "            lw['mean'].append(mean)\n",
    "            lw['std'].append(std)\n",
    "            lw['median'].append(median)\n",
    "            lw['16th'].append(p16)\n",
    "            lw['84th'].append(p84)\n",
    "        log_lw = {k:np.log10(v) for k,v in lw.items()}\n",
    "\n",
    "        for k in lw.keys():\n",
    "            catalog.add_column( Column(name='linewidth (mom2 '+k+')',   data=lw[k],   dtype=np.float64, unit='km/s') )\n",
    "\n",
    "        for k in log_lw.keys():\n",
    "            catalog.add_column( Column(name='log linewidth (mom2 '+k+')',   data=log_lw[k],   dtype=np.float64, unit='km/s') )\n",
    "\n",
    "'''     \n",
    "# Similar bootstrapping method as the v_rm_err calculation\n",
    "\n",
    "def Get_R_err(dend1,struct,idx,m,NF,iterations,metadata):\n",
    "    \n",
    "    \n",
    "    rs=[] # an array of the calculated V_RMS\n",
    "    np.random.seed((99)**2*123)\n",
    "    \n",
    "    for llll in range(iterations):\n",
    "        \n",
    "        s = dend1.__getitem__(idx) # Get the structure from the dendrogram\n",
    "        \n",
    "        npixels = np.product(np.shape(s.values()))\n",
    "        \n",
    "        additional_noise = np.random.normal(0., m*NF, npixels)\n",
    "        additional_noise = np.reshape(additional_noise, np.shape(s.values()))\n",
    "        \n",
    "        # add or subract noise to the values and calculate the v rms, them find the std of that array and\n",
    "        # call that the uncertainty in v rms for a structure\n",
    "        dat1P = dend1.data[s.indices()] # a copy of the data\n",
    "        dend1.data[s.indices()]+= additional_noise #add teh noise\n",
    "        \n",
    "        s = dend1.__getitem__(idx)\n",
    "        \n",
    "        rs.append(float(PPVStatistic(s,metadata=metadata).radius.value))\n",
    "        dend1.data[s.indices()]= dat1P # reset the dend data\n",
    "        \n",
    "        dend1.data[s.indices()]-= additional_noise # subtract the noise also\n",
    "        s = dend1.__getitem__(idx)\n",
    "        \n",
    "        rs.append(float(PPVStatistic(s,metadata=metadata).radius.value))\n",
    "        \n",
    "        UNIT = PPVStatistic(s,metadata=metadata).radius.unit\n",
    "        \n",
    "        del s\n",
    "        \n",
    "    r_std = np.nanstd(rs)*UNIT\n",
    "    #print(rs*UNIT,r_std)\n",
    "    return r_std\n",
    "\n",
    "# Return a cropped cube for some ra and dec, also crops the velocity axis if needed (0 for no crop)\n",
    "# cube= the spetral cube you wish to crop\n",
    "# WCS = that cube's world coordinate system (.WCS)\n",
    "# Np1, Np2 = the first and final pixel that don't have nan values (the left and right bounds of the cube)\n",
    "# BadVel = put a number based on the amound of velocity channels that are just noise, or leave at zero if you want to keep the noise\n",
    "# D2 = put True if the cube is 2D, otherwise, put False\n",
    "\n",
    "def Crop(cube,WCS,Np1,Np2,BadVel,D2):\n",
    "    NraDP1 = [int(WCS.world_to_pixel(Np1)[0]),int(WCS.world_to_pixel(Np1)[1])]\n",
    "    NraDP2 = [int(WCS.world_to_pixel(Np2)[0]),int(WCS.world_to_pixel(Np2)[1])]\n",
    "    if(D2==False):\n",
    "        return cube[BadVel:np.shape(cube)[0]-BadVel,NraDP1[1]:NraDP2[1],NraDP1[0]:NraDP2[0]]\n",
    "    if(D2==True):\n",
    "        return cube[NraDP1[1]:NraDP2[1],NraDP1[0]:NraDP2[0]]\n",
    "\n",
    "    \n",
    "    \n",
    "# crops rectangular nan data added during reprojection\n",
    "def Crop_Nans(data):\n",
    "\n",
    "    sx,sy,ex,ey=0,0,0,0\n",
    "    for lmi in range(np.shape(data[0,:,:])[0]):\n",
    "\n",
    "        if(ey!=0 and sx!=0 and ex!=0 and sy!=0):\n",
    "            print(\"F\",lmi)\n",
    "            break\n",
    "        for lmj in range(np.shape(data[0,:,:])[1]):\n",
    "\n",
    "            if(sx==0):            \n",
    "                if(np.nanmean(data[0,lmi,:])>0 or np.nanmean(data[0,lmi,:])<0):\n",
    "                    sx=lmi\n",
    "\n",
    "\n",
    "            if(sy==0):\n",
    "                if(np.nanmean(data[0,:,lmj])>0 or np.nanmean(data[0,:,lmj])<0):\n",
    "                    sy=lmj\n",
    "\n",
    "            if(ex==0):\n",
    "                if(np.nanmean(data[0,np.shape(datn[0,:,:])[0]-lmi-1,:])>0 or np.nanmean(data[0,np.shape(data[0,:,:])[0]-lmi-1,:])<0):\n",
    "                    ex=np.shape(data[0,:,:])[0]-lmi-1\n",
    "\n",
    "            if(ey==0):\n",
    "                if(np.nanmean(data[0,:,np.shape(data[0,:,:])[1]-lmj-1])>0 or np.nanmean(data[0,:,np.shape(data[0,:,:])[1]-lmj-1])<0):\n",
    "                    ey=np.shape(data[0,:,:])[1]-lmj-1\n",
    "\n",
    "            if(ey!=0 and ex!=0 and sx!=0 and sy!=0):\n",
    "                break\n",
    "                \n",
    "    print(sx,ex,sy,ey)\n",
    "    return sx,ex,sy,ey\n",
    "\n",
    "\n",
    "\n",
    "# calculate moment maps for all structures\n",
    "###################################################################################################\n",
    "\n",
    "def moment0(cube, chanwidth, mask_zeros=True):\n",
    "    mom0 = chanwidth*np.nansum(cube.value, axis=0)*cube.unit\n",
    "    if mask_zeros:\n",
    "        mom0[mom0==0.0] = np.nan\n",
    "    return mom0\n",
    "\n",
    "def moment1(cube, moment0, chanwidth, pix_centers):\n",
    "    mom1 = np.nansum(cube*pix_centers*chanwidth, axis=0)/moment0\n",
    "    return mom1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def moment2(cube, moment0, moment1, chanwidth, pix_centers, mask_zeros=True):\n",
    "    mom2 = np.full_like(moment1.value, 0.0) *cube.unit*pix_centers.unit**2*chanwidth.unit\n",
    "    for chan in np.arange(cube.shape[0]):\n",
    "        mom2 += cube[chan] *(pix_centers[chan]-moment1)**2 *chanwidth\n",
    "    mom2 = np.sqrt(mom2/moment0)\n",
    "    if mask_zeros:\n",
    "        mom2[mom2==0.0] = np.nan\n",
    "    return mom2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce15fd70-a481-46cf-bbff-ec5e902d0538",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81159ff0-823a-4650-9cb8-cbdd6947614d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Make an image from the SC data\n",
    "\n",
    "#Data=the image data to plot\n",
    "#vmin,vmax=the min/max scale for the graph's color bar\n",
    "#Name2, another descriptor\n",
    "\n",
    "def Make_Image_Plot(Title,Units,Cube_data,wcs,vmin,vmax,rows=1,columns=1,index=1,show=False,Axis=\"RADEC\",Cube_Info='nil',image_args={},figsize=(13.33,8)):\n",
    "    \n",
    "    fix_axis=False\n",
    "    scale=1\n",
    "    COLORBARSCALE = 1\n",
    "    XLABELS_NUM = 10\n",
    "    vmin=vmin\n",
    "    vmax=vmax\n",
    "    norm='linear'\n",
    "    dpi=800\n",
    "    for key in image_args.keys():\n",
    "        if key == \"FIX_AXIS\":\n",
    "            fix_axis=True\n",
    "        if key ==\"FIGSIZE\":\n",
    "            scale=image_args[key][0]/figsize[0]\n",
    "            figsize=image_args[key]  \n",
    "            \n",
    "        if key == \"SCALE\":\n",
    "            scale=image_args[key]\n",
    "        if key == \"COLORBARSCALE\":\n",
    "            COLORBARSCALE = image_args[key]\n",
    "        if key ==\"XLABELS_NUM\":\n",
    "            XLABELS_NUM = image_args[key]\n",
    "        if key ==\"VMIN\":\n",
    "            vmin=image_args[key]\n",
    "        if key ==\"VMAX\":\n",
    "            vmax=image_args[key]\n",
    "        if key ==\"NORM\":\n",
    "            norm = image_args[key]#unsured\n",
    "        if key ==\"DPI\":\n",
    "            dpi = image_args[key]#unsured\n",
    "\n",
    "            \n",
    "            \n",
    "    font = {'fontname':'Comic Sans MS'}\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans Mono' \n",
    "    plt.rcParams['font.sans-serif'] = ['Arial']  \n",
    "\n",
    "    WCS = wcs\n",
    "    \n",
    "    Glon = str(WCS).find(\"GLON\")!=-1 #If the wcs is in Glon or not\n",
    "    \n",
    "    # Build a plot\n",
    "    print(figsize)\n",
    "    fig,ax = plt.subplots(nrows=rows, ncols=columns, squeeze=True, sharex='none', sharey='none', figsize=figsize)\n",
    "    \n",
    "    if(Cube_Info!='nil'):\n",
    "        pc_per_pixel = Cube_Info[\"spatial_scale_y\"].to(u.pc/u.pix)\n",
    "        beam_position_angle = Cube_Info['Beam_Position_angle']\n",
    "        \n",
    "    # labels\n",
    "    if Glon and Axis!=\"PC\":\n",
    "\n",
    "        ax.set_xlabel('Galactic Longitude',fontsize=15*scale)\n",
    "        ax.set_ylabel('Galactic Latitue',fontsize=15*scale)\n",
    "        \n",
    "    elif(Axis==\"PC\"):\n",
    "        \n",
    "        ax.set_xlabel('Pc',fontsize=15*scale)\n",
    "        ax.set_ylabel('Pc',fontsize=15*scale)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        \n",
    "        ax.set_xlabel('Right Ascension',fontsize=15*scale)\n",
    "        ax.set_ylabel('Declination',fontsize=15*scale)\n",
    "    \n",
    "    # Make axis in RA and DEC if chosen:\n",
    "    \n",
    "    yaxis = [int(i) for i in np.arange(0,np.shape(Cube_data)[0], int(np.shape(Cube_data)[0]/5/scale))]\n",
    "    xaxis = [int(i) for i in np.arange(0,np.shape(Cube_data)[1], int(np.shape(Cube_data)[1]/XLABELS_NUM/scale))]\n",
    "    \n",
    "    if(Axis==\"RADEC\" and Glon==False):\n",
    "        \n",
    "\n",
    "        #Get the coordinates the axis pixels\n",
    "\n",
    "        xlabels = WCS.pixel_to_world(xaxis,np.zeros(len(xaxis))).ra\n",
    "        ylabels = WCS.pixel_to_world(np.zeros(len(yaxis)),yaxis).dec\n",
    "\n",
    "        #Convert to HMS\n",
    "        xlabels_hms=np.array(xlabels,dtype=type(\"aarf\"))\n",
    "        for i, e in enumerate(xlabels):\n",
    "            xlabels_hms[i] = f\"{int(e.hms.h)}h{int(e.hms.m)}m{np.round(e.hms.s,2)}s\"\n",
    "        del xlabels\n",
    "        xlabels = xlabels_hms\n",
    "\n",
    "        #Round\n",
    "        ylabels_r=np.array(ylabels,dtype=type(\"aarf\"))\n",
    "        for i, e in enumerate(ylabels):\n",
    "            ylabels_r[i] = (str(int(e.dms.d))+'d' +str(abs(int(e.dms.m)))+\"m\"+ str(abs(np.round(e.dms.s,3)))+\"s\")\n",
    "        del ylabels\n",
    "        ylabels = ylabels_r\n",
    "        \n",
    "    elif Glon and Axis!=\"PC\":\n",
    "        \n",
    "        #Get the coordinates the axis pixels\n",
    "\n",
    "        xlabels = WCS.pixel_to_world(xaxis,np.zeros(len(xaxis))).l.deg\n",
    "        \n",
    "        if(fix_axis):            \n",
    "            print(\"changing xlables:\",xlabels)\n",
    "        \n",
    "            #xlabels = np.concatenate([-xlabels[int(math.floor(len(xlabels)/2)):],np.flip(xlabels[int(math.ceil(len(xlabels)/2)):])]) #-180 to 180 cuz it makes more sense on a plot\n",
    "            \n",
    "            \n",
    "            xlabels = -np.round(xlabels,1)+np.full(np.shape(xlabels),180)\n",
    "            print(\"to:\",xlabels)\n",
    "        \n",
    "        ylabels = WCS.pixel_to_world(np.zeros(len(yaxis)),yaxis).b.deg\n",
    "\n",
    "        #Convert to rounded deg strings\n",
    "        xlabels_string=np.array(xlabels,dtype=type(\"aarf\"))\n",
    "        for i, e in enumerate(xlabels):\n",
    "            xlabels_string[i] = str(np.round(e,2))\n",
    "        del xlabels\n",
    "        xlabels = xlabels_string\n",
    "\n",
    "        #Strings        \n",
    "        ylabels_string=np.array(ylabels,dtype=type(\"aarf\"))\n",
    "        for i, e in enumerate(ylabels):\n",
    "            ylabels_string[i] = str(np.round(e,2))\n",
    "        del ylabels\n",
    "        ylabels = ylabels_string\n",
    "        \n",
    "    elif(Axis==\"PC\"):\n",
    "        \n",
    "        xlabels = np.array([e*pc_per_pixel.value for e in xaxis],dtype=type(1))\n",
    "        ylabels = np.array([e*pc_per_pixel.value for e in yaxis],dtype=type(1))\n",
    "        \n",
    "        if(fix_axis):  \n",
    "            print(\"changing xlables:\",xlabels)\n",
    "\n",
    "            #xlabels = np.concatenate([np.flip(-xlabels[0:int(math.floor(len(xlabels)/2))]),(xlabels[0:int(math.ceil(len(xlabels)/2))])])  #-pc to pc cuz it makes more sense on a plot\n",
    "            x_pc = [e*pc_per_pixel.value for e in xaxis]\n",
    "            xlabels= np.array(np.arange(max(x_pc)/2,-max(x_pc)/2,-1000),dtype = type(1))\n",
    "            xaxis = np.linspace(0,np.shape(Cube_data)[1],len(xlabels))\n",
    "            print(xlabels,xaxis)\n",
    "            #xlabels = np.concatenate([np.flip(-xlabels[0:int(math.floor(len(xlabels)/2))]),(xlabels[0:int(math.ceil(len(xlabels)/2))])])  #-pc to pc cuz it makes more sense on a plot\n",
    "            print(\"to:\",xlabels)\n",
    "    else:\n",
    "        print(\"error\")\n",
    "        del agagasgaa\n",
    "        \n",
    "\n",
    "    ax.set_xticks(xaxis)\n",
    "    ax.set_yticks(yaxis)\n",
    "    ax.set_xticklabels(xlabels,fontsize=12*scale);\n",
    "    ax.set_yticklabels(ylabels,fontsize=12);\n",
    "    ax.tick_params(direction='in',labelsize=12*scale)\n",
    "    ax.tick_params(axis='both', length=8*scale, width=1.5*scale,bottom=True,left=True,right=True,top=True,colors='gray')  \n",
    "\n",
    "\n",
    "    \n",
    "    #ax.set_title(Title,y=(-.45),fontsize=20)        \n",
    "    #ax.set_title(Title,fontsize=10,xy=(0.02,1.05),xycoords=\"axes fraction\")        \n",
    "    pylab.annotate(text=Title,fontsize=10*scale,xy=(0.02,1.05),xycoords=\"axes fraction\")  \n",
    "\n",
    "    \n",
    "    \n",
    "    color = plt.cm.inferno\n",
    "    #color = plt.cm.Blues\n",
    "    # show image\n",
    "    im = ax.imshow(Cube_data,\n",
    "                   origin        = 'lower',\n",
    "                   interpolation = 'nearest',\n",
    "                   cmap          = color,\n",
    "                   aspect        = 'equal',\n",
    "                   vmin          = vmin,\n",
    "                   vmax          = vmax\n",
    "                  )\n",
    "    \n",
    "\n",
    "    cbar = fig.colorbar(im,fraction=0.0096*COLORBARSCALE, pad=0.0)\n",
    "    cbar.ax.tick_params(direction='in',length=8, width=1.5,colors='gray')\n",
    "    cbar.set_label(Units)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # plot beam\n",
    "    from matplotlib.patches import Ellipse\n",
    "    \n",
    "    bmaj_pix = Cube_Info['desired_beam_oversampling'] # cube.header['bmaj'] # The beam oversampling is the amount of pixels in the beam\n",
    "    bmin_pix = Cube_Info['desired_beam_oversampling'] # cube.header['bmin']\n",
    "    bpa      = beam_position_angle.value\n",
    "    \n",
    "    figoff=figsize[0]/figsize[1]\n",
    "    beam = Ellipse(xy     = (int(np.shape(Cube_data)[1]/25/figoff), int(np.shape(Cube_data)[0]/10/figoff)),\n",
    "                   width  = bmaj_pix/2,\n",
    "                   height = bmin_pix/2,\n",
    "                   angle  = bpa,\n",
    "                   ec     = None,\n",
    "                   fc     = 'gray',\n",
    "                   alpha  = 1.\n",
    "                  )\n",
    "    ax.add_artist(beam)\n",
    "\n",
    "\n",
    "    if(show==True):\n",
    "        pylab.show()\n",
    "        \n",
    "    # save figure\n",
    "    \n",
    "    #fig.savefig((\"Plots/\"+Title+\".pdf\"), dpi=1250, bbox_inches='tight')\n",
    "    #fig.savefig((\"Plots/\"+Title+\".pdf\"), dpi=dpi, bbox_inches='tight')\n",
    "    fig.savefig((\"Plots/\"+Title+\".svg\"), dpi=dpi, bbox_inches='tight')\n",
    "    fig.savefig((\"Plots/\"+Title+\".pdf\"), dpi=dpi, bbox_inches='tight')\n",
    "\n",
    "        \n",
    "def Make_Image_Plot_Old(Pointing_Information,Annotation,Data,vmin,vmax,rows=1,columns=1,index=1,show=False):\n",
    "    \n",
    "    Name=Pointing_Information['target']\n",
    "    WCS=Pointing_Information['wcsu']\n",
    "    Glon = str(WCS).find(\"GLON\")!=-1 #If the wcs is in Glon or not\n",
    "    \n",
    "    # Build a plot\n",
    "\n",
    "    ax = pylab.subplot(rows,columns,index,projection=WCS) \n",
    "    RA = ax.coords[0]                                                                  # \n",
    "    Dec = ax.coords[1]\n",
    "    im = pylab.imshow(Data,vmin=vmin,vmax=vmax,cmap='rainbow')\n",
    "    RA.set_ticks(size=-3)                                                                                      \n",
    "    Dec.set_ticks(size=-3) \n",
    "    RA.set_ticklabel(exclude_overlapping=True) \n",
    "    Dec.set_ticklabel(exclude_overlapping=True)                                                                                     \n",
    "    \n",
    "    if(Glon==False):\n",
    "        pylab.xlabel('Right Ascension',fontsize=20,labelpad=1)                               \n",
    "        pylab.ylabel('Declination',fontsize=20,labelpad=1)\n",
    "    else:\n",
    "        pylab.xlabel('Galatic longitude',fontsize=20,labelpad=1)                               \n",
    "        pylab.ylabel('Galatic latitude',fontsize=20,labelpad=1)\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = 15)    \n",
    "    cb=pylab.colorbar(im,fraction=0.1,pad=0.0)                                     \n",
    "    cb.set_label(label=Name,fontsize=10,rotation=270,labelpad=20) \n",
    "    cb.ax.tick_params(which = 'major', labelsize = 10)   \n",
    "    pylab.annotate(text=Annotation,fontsize=10,xy=(0.02,1.05),xycoords=\"axes fraction\")  \n",
    "    \n",
    "    if(show==True):\n",
    "        pylab.show()\n",
    " \n",
    "def Setup_Comp_Plot(Title,axes=[\"\",\"\"],xlim=[0,1],ylim=[0,1],xticks= [],yticks=[],xlabels=[],ylabels=[],figsize=(8,8),args={\"lims\":True,\"ts\":10,\"ls\":12}):\n",
    "    \n",
    "    font = {'fontname':'Comic Sans MS'}\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans Mono' \n",
    "    plt.rcParams['font.sans-serif'] = ['Arial']  \n",
    "\n",
    "    \n",
    "    # Build a plot\n",
    "    \n",
    "    fig,ax = plt.subplots(nrows=1, ncols=1, squeeze=True, sharex='none', sharey='none', figsize=figsize)\n",
    "           \n",
    "    ax.set_xlabel(axes[0],fontsize=15)\n",
    "    ax.set_ylabel(axes[1],fontsize=15)\n",
    "    if(args[\"lims\"]):\n",
    "        ax.set_xlim(xlim[0],xlim[1])\n",
    "        ax.set_ylim(ylim[0],ylim[1])\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_yticks(yticks)\n",
    "    ax.set_xticklabels(xlabels,fontsize=ls);\n",
    "    ax.set_yticklabels(ylabels,fontsize=ls);\n",
    "    ax.tick_params(direction='in',labelsize=12)\n",
    "    ax.tick_params(axis='both', length=8, width=1.5,bottom=True,left=True,right=True,top=True,colors='gray')  \n",
    "    ts=args[\"ts\"]\n",
    "    pylab.annotate(text=Title,fontsize=ts,xy=(0.02,1.05),xycoords=\"axes fraction\")  \n",
    "\n",
    "\n",
    "    return fig,ax\n",
    "    \n",
    "\n",
    "'''\n",
    "# dataset to use:\n",
    "###################################################################################################\n",
    "\n",
    "co  = 'CO(3-2)'\n",
    "gal = 'NGC253'\n",
    "idx = {'idx': 4822, 'vmin': 5000, 'vmax': 12000}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot map and structures\n",
    "###################################################################################################\n",
    "\n",
    "# figure\n",
    "fig,ax = plt.subplots(nrows=1, ncols=1, squeeze=True, sharex='none', sharey='none', figsize=(5,3))\n",
    "\n",
    "\n",
    "\n",
    "xaxis = get_pixel_locations(mom0, axis=1).value\n",
    "yaxis = get_pixel_locations(mom0, axis=2).value\n",
    "xlabels = [int(i) for i in np.arange(np.max(np.round(xaxis,-1)), np.min(np.round(xaxis,-1)), -2)]\n",
    "ylabels = [int(i) for i in np.arange(np.min(np.round(yaxis,-1)), np.max(np.round(yaxis,-1)), 2)]\n",
    "xticks  = coordinate_to_pixel(mom0, 1, xlabels*u.pc).value\n",
    "yticks  = coordinate_to_pixel(mom0, 2, ylabels*u.pc).value\n",
    "\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_yticks(yticks)\n",
    "ax.set_xticklabels(xlabels);\n",
    "ax.set_yticklabels(ylabels);\n",
    "\n",
    "# show image\n",
    "im = ax.imshow(mom0.data,\n",
    "               origin        = 'lower',\n",
    "               interpolation = 'nearest',\n",
    "               cmap          = plt.cm.Blues,\n",
    "               aspect        = 'equal',\n",
    "               vmin          = idx['vmin'],\n",
    "               vmax          = idx['vmax']\n",
    "              )\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.set_label('intensity [K\\,km\\,s$^{-1}$]')\n",
    "\n",
    "# show contour\n",
    "d = dendrograms[co][gal]['dendrogram']\n",
    "p = d.plotter()\n",
    "p.plot_contour(ax, structure=idx['idx'], lw=1, colors='orange')\n",
    "\n",
    "# show ellipse\n",
    "idx_list = [i.idx for i in d.all_structures]\n",
    "all_structs_ordered = [x for _,x in sorted(zip(idx_list,list(d.all_structures)))]\n",
    "s = PPVStatistic(all_structs_ordered[idx['idx']])\n",
    "ellipse = s.to_mpl_ellipse(edgecolor='red', facecolor='none')\n",
    "ax.add_patch(ellipse)\n",
    "\n",
    "# zoom in on structure\n",
    "center   = ellipse.get_center()\n",
    "vertices = ellipse.get_patch_transform().transform( ellipse.get_path().vertices.copy() )\n",
    "bounds   = (np.min(vertices[:,0]), np.max(vertices[:,0]), np.min(vertices[:,1]), np.max(vertices[:,1]))\n",
    "extent   = (bounds[1]-bounds[0], bounds[3]-bounds[2])\n",
    "\n",
    "xmin = center[0]-2.5*extent[0]\n",
    "xmax = center[0]+2.5*extent[0]\n",
    "ymin = center[1]-2.5*extent[1]\n",
    "ymax = center[1]+2.5*extent[1]\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "# show half axes\n",
    "dist = np.sqrt( (vertices[:,0]-center[0])**2 + (vertices[:,1]-center[1])**2 )\n",
    "p_maj = vertices[np.argmax(dist)+1]\n",
    "p_min = vertices[np.argmin(dist)]\n",
    "\n",
    "a_maj = ax.plot((center[0],p_maj[0]), (center[1],p_maj[1]), c='red', ls='--', lw=0.5)\n",
    "a_min = ax.plot((center[0],p_min[0]), (center[1],p_min[1]), c='red', ls='--', lw=0.5)\n",
    "\n",
    "# plot beam\n",
    "from matplotlib.patches import Ellipse\n",
    "bmaj_pix = ( angle_to_parsec(cube.header['bmaj']*u.deg, source=gal) / cdelt ).value\n",
    "bmin_pix = ( angle_to_parsec(cube.header['bmin']*u.deg, source=gal) / cdelt ).value\n",
    "bpa      = cube.header['bpa']\n",
    "beam = Ellipse(xy     = (xmin+0.15*(xmax-xmin), ymin+0.15*(xmax-xmin)),\n",
    "               width  = bmaj_pix,\n",
    "               height = bmin_pix,\n",
    "               angle  = bpa,\n",
    "               ec     = None,\n",
    "               fc     = 'black',\n",
    "               alpha  = 1.\n",
    "              )\n",
    "ax.add_artist(beam)\n",
    "\n",
    "# save figure\n",
    "fig.savefig(join(plotdir, 'paper', 'fig1.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "###################################################################################################\n",
    "# fig A.2: structure definition comparison\n",
    "###################################################################################################\n",
    "\n",
    "catalog    = dendrograms['CO(3-2)']['NGC253']['catalog']\n",
    "dendrogram = dendrograms['CO(3-2)']['NGC253']['dendrogram']\n",
    "\n",
    "fig,axes = plt.subplots(nrows=1, ncols=2, squeeze=True, sharex='none', sharey='none', figsize=(8,4))\n",
    "fig.subplots_adjust(hspace=0., wspace=0.1)\n",
    "\n",
    "R_measures = ['size (astrodendro)',\n",
    "              'size (area_ellipse)',\n",
    "              'size (manual)']\n",
    "lw_measures = ['linewidth (astrodendro)',\n",
    "               'linewidth (mom2 mean)',\n",
    "               'linewidth (mom2 median)',\n",
    "               'linewidth (90% flux)',\n",
    "               'linewidth (FWHM)',\n",
    "               'linewidth (FW10%)']\n",
    "\n",
    "\n",
    "# A1: size definition comparison\n",
    "###################################################################################################\n",
    "\n",
    "ax = axes[0]\n",
    "A1colors = mpl.cm.YlOrRd(np.linspace(0.1,0.9,len(R_measures)))\n",
    "\n",
    "min = np.nanmin([x if x!=0. else np.nan for x in flatten([catalog[R_measure].data for R_measure in R_measures])])\n",
    "max = np.nanmax([catalog[R_measure].data for R_measure in R_measures])\n",
    "\n",
    "for R_measure,color in zip(R_measures,A1colors):\n",
    "    size_ad = catalog['size (astrodendro)'].data\n",
    "    size    = catalog[R_measure].data\n",
    "    ax.scatter(size_ad, size, marker='.', s=8, c=[color],\n",
    "               label=R_measure.replace('size (','').replace(')','').replace('_',' ').replace('astrodendro',r'R$_\\mathrm{astrodendro}$').replace('manual',r'R$_\\mathrm{circular}$').replace('area ellipse',r'R$_\\mathrm{ellipse}$'),\n",
    "               zorder=3,\n",
    "               rasterized=True)\n",
    "ax.plot([0.5*min,2.0*max],[0.5*min,2.0*max], ls='-', color='grey', lw=1, zorder=2)\n",
    "\n",
    "ax.set_xlabel(r'R$_\\mathrm{astrodendro}$ [pc]')\n",
    "ax.set_ylabel(r'R [pc]')\n",
    "ax.set_xlim(0.5*min, 2.0*max)\n",
    "ax.set_ylim(0.5*min, 2.0*max)\n",
    "ax.legend(bbox_to_anchor=(0.,1.02,1.,0.05), loc='lower left', mode='expand', borderaxespad=0., ncol=3, scatterpoints=1, handletextpad=0., fancybox=True, fontsize=10)\n",
    "\n",
    "\n",
    "# A2: linewidth definition comparison\n",
    "###################################################################################################\n",
    "\n",
    "ax = axes[1]\n",
    "A2colors = mpl.cm.YlGnBu(np.linspace(0.1,1,len(lw_measures)))\n",
    "\n",
    "min = np.nanmin([x if x>1e-2 else np.nan for x in flatten([catalog[lw_measure].data for lw_measure in lw_measures])])\n",
    "max = np.nanmax([catalog[lw_measure].data for lw_measure in lw_measures])\n",
    "\n",
    "for lw_measure,color in zip(lw_measures,A2colors):\n",
    "    linewidth_ad = catalog['linewidth (astrodendro)'].data\n",
    "    linewidth    = catalog[lw_measure].data\n",
    "    ax.scatter(linewidth_ad, linewidth, marker='.', s=8, c=[color],\n",
    "               label=lw_measure.replace('linewidth (','').replace(')','').replace('_',' ').replace('astrodendro',r'$\\sigma_\\mathrm{astrodendro}$').replace('mom2 mean',r'$\\sigma_\\mathrm{mom2\\ mean}$').replace('mom2 median',r'$\\sigma_\\mathrm{mom2\\ median}$').replace('90% flux',r'$\\sigma_\\mathrm{90\\%}$').replace('FWHM',r'$\\sigma_\\mathrm{FWHM}$').replace('FW10%',r'$\\sigma_\\mathrm{FW10\\%}$'),\n",
    "               zorder=3,\n",
    "               rasterized=True)\n",
    "ax.plot([0.5*min,2.0*max],[0.5*min,2.0*max], ls='-', color='grey', lw=1, zorder=2)\n",
    "\n",
    "ax.set_xlabel(r'$\\sigma_\\mathrm{astrodendro}$ [km\\,s$^{-1}$]')\n",
    "ax.set_ylabel(r'$\\sigma$ [km\\,s$^{-1}$]')\n",
    "ax.set_xlim(0.5*min, 2.0*max)\n",
    "ax.set_ylim(0.5*min, 2.0*max)\n",
    "ax.yaxis.set_label_position('right')\n",
    "ax.tick_params(axis='y', which='both', labelleft='off', labelright='on')\n",
    "ax.legend(bbox_to_anchor=(0.,1.02,1.,0.05), loc='lower left', mode='expand', borderaxespad=0., ncol=3, scatterpoints=1, handletextpad=0., fancybox=True, fontsize=10)\n",
    "\n",
    "\n",
    "# save figure\n",
    "for ax in axes:\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(ls=':', c='lightgrey')\n",
    "fig.savefig(join(plotdir, 'paper', 'figA.pdf'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99684d-0c92-4211-a824-cd052c236fb8",
   "metadata": {},
   "source": [
    "# More useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b729c6-b506-4752-abbd-83c9b6fcb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to flatten out the cubes so that they are similarly rotated at 0 degrees\n",
    "\n",
    "def Rotate_Cube(Cube, Image_rotation):\n",
    "\n",
    "    # R is from scipy.spatial.transform import Rotation as R\n",
    "    # Flatten the cube by rotating -Image_rotation\n",
    "    r = R.from_euler('z', -Image_rotation.to(u.deg), degrees=True) # Create a rotation quaternion for a rotation about the cube's z axis\n",
    "    M = r.as_dcm() # the rotation in matrix form\n",
    "\n",
    "    # Make a dictionary containing the paremeters that the header uses to rotate\n",
    "    rotation_matrix = {'PC1_1':  float(M[0][0]),\n",
    "                       'PC2_1':  float(M[1][0]),\n",
    "                       'PC3_1':  float(M[2][0]),\n",
    "                       'PC1_2':  float(M[0][1]),\n",
    "                       'PC2_2':  float(M[1][1]),\n",
    "                       'PC3_2':  float(M[2][1]),\n",
    "                       'PC1_3':  float(M[0][2]),\n",
    "                       'PC2_3':  float(M[1][2]),\n",
    "                       'PC3_3':  float(M[2][2])}\n",
    "    \n",
    "    # Make a reprojection header\n",
    "    reheader = copy.deepcopy(Cube.header)\n",
    "    for key,val in rotation_matrix.items():\n",
    "        reheader[key] = val\n",
    "\n",
    "    # rotate by regridding onto rotated header\n",
    "    Cube = SpectralCube.read(Cube.hdu)\n",
    "    Cube.allow_huge_operations = True\n",
    "    Cube3 = Cube.reproject(reheader, order='bilinear', use_memmap=True, filled=True)\n",
    "    Cube3 = SpectralCube.read(Cube3.hdu)\n",
    "    return Cube3\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c8d6d-991e-4425-90e0-62067f41a42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def gaussian_beam(f, beam_gauss_width):\n",
    "    '''\n",
    "    Fourier transform of a Gaussian beam. NOT the power spectrum (multiply exp\n",
    "    argument by 2 for power spectrum).\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : np.ndarray\n",
    "        Frequencies to evaluate beam at.\n",
    "    beam_gauss_width : float\n",
    "        Beam size. Should be the Gaussian rms, not FWHM.\n",
    "    '''\n",
    "    return np.exp(-f**2 * np.pi**2 * 2 * beam_gauss_width**2)\n",
    "\n",
    "def gauss_correlated_noise_2D(shape, sigma, beam_gauss_width,\n",
    "                              randomseed=327485749):\n",
    "    \n",
    "    '''\n",
    "    Generate correlated Gaussian noise with sigma, smoothed by a\n",
    "    Gaussian kernel.\n",
    "    '''\n",
    "\n",
    "    # Making a real signal. Only need real part of FFT\n",
    "    freqs_yy, freqs_xx = np.meshgrid(np.fft.fftfreq(shape[0]),\n",
    "                                     np.fft.rfftfreq(shape[1]), indexing=\"ij\")\n",
    "\n",
    "    freqs = np.sqrt(freqs_yy**2 + freqs_xx**2)\n",
    "    # freqs[freqs == 0.] = np.NaN\n",
    "    # freqs[freqs == 0.] = 1.\n",
    "\n",
    "    imsize = shape[0]\n",
    "\n",
    "    Np1 = (imsize - 1) // 2 if imsize % 2 != 0 else imsize // 2\n",
    "    \n",
    "    with NumpyRNGContext(randomseed):\n",
    "\n",
    "        angles = np.random.uniform(0, 2 * np.pi,\n",
    "                                   size=freqs.shape)\n",
    "\n",
    "    noise = np.cos(angles) + 1j * np.sin(angles)\n",
    "\n",
    "    if imsize % 2 == 0:\n",
    "        noise[1:Np1, 0] = np.conj(noise[imsize:Np1:-1, 0])\n",
    "        noise[1:Np1, -1] = np.conj(noise[imsize:Np1:-1, -1])\n",
    "        noise[Np1, 0] = noise[Np1, 0].real + 1j * 0.0\n",
    "        noise[Np1, -1] = noise[Np1, -1].real + 1j * 0.0\n",
    "\n",
    "    else:\n",
    "        noise[1:Np1 + 1, 0] = np.conj(noise[imsize:Np1:-1, 0])\n",
    "        noise[1:Np1 + 1, -1] = np.conj(noise[imsize:Np1:-1, -1])\n",
    "\n",
    "    # Zero freq components must have no imaginary part to be own conjugate\n",
    "    noise[0, -1] = noise[0, -1].real + 1j * 0.0\n",
    "    noise[0, 0] = noise[0, 0].real + 1j * 0.0\n",
    "\n",
    "    corr_field = np.fft.irfft2(noise *\n",
    "                               gaussian_beam(freqs, beam_gauss_width))\n",
    "\n",
    "    norm = (np.sqrt(np.sum(corr_field**2)) / np.sqrt(corr_field.size)) / sigma\n",
    "\n",
    "    corr_field /= norm\n",
    "    \n",
    "    return corr_field\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#From Krieger 2020 (github):\n",
    "\n",
    "####################################################################################################\n",
    "# get pixel locations for axis\n",
    "####################################################################################################\n",
    "\n",
    "def get_pixel_locations(fitsimage, axis):\n",
    "    \"\"\"\n",
    "    Get a list of pixel locations for a given image and axis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fitsimage : string or PrimaryHDU\n",
    "        File name of a fits image or astropy.fits PrimaryHDU object.\n",
    "    axis : int\n",
    "        Axis number. No default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    astropy.unit list\n",
    "        List of coordinates along the axis in the units of the header (e.g. degree for a RA or DEC axis).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import astropy.units as u\n",
    "    from astropy.io import fits\n",
    "    import numpy as np\n",
    "\n",
    "    if isinstance(fitsimage, str):\n",
    "        header = fits.getheader(fitsimage)\n",
    "    elif isinstance(fitsimage, fits.hdu.image.PrimaryHDU):\n",
    "        header = fitsimage.header\n",
    "    else:\n",
    "        raise TypeError(\"Unknown format for fitsimage. Must be filename or HDUList.\")\n",
    "\n",
    "    crpix = header['crpix'+str(axis)]*u.pix\n",
    "    crval = u.Quantity(str(header['crval'+str(axis)])+header['cunit'+str(axis)])\n",
    "    cdelt = u.Quantity(str(header['cdelt'+str(axis)])+header['cunit'+str(axis)])/u.pix\n",
    "    naxis = header['naxis'+str(axis)]\n",
    "\n",
    "    return (np.arange(naxis)*u.pix-crpix)*cdelt+crval\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "####################################################################################################\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# axis location to pixel\n",
    "####################################################################################################\n",
    "\n",
    "def coordinate_to_pixel(fitsimage, axis, coordinates, precision=2):\n",
    "    \"\"\"\n",
    "    Calculate the pixel positions corresponding to a given coordinate along an\n",
    "    axis of an fits image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fitsimage : string or PrimaryHDU\n",
    "        File name of a fits image or astropy.fits PrimaryHDU object.\n",
    "    axis : int\n",
    "        Axis number. No default.\n",
    "    coordinates : astropy.Quantity or Quantity list\n",
    "        The coordinates to be converted in the same unit as the header axis unit.\n",
    "    precision : int\n",
    "        Precision  of the return pixel position. Defaults to two decimal places.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    astropy.Quantity or Quantity list\n",
    "        Single value or list of pixel position corresponding to the given coordinates.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import astropy.units as u\n",
    "    from astropy.io import fits\n",
    "\n",
    "    if isinstance(fitsimage, str):\n",
    "        header = fits.getheader(fitsimage)\n",
    "    elif isinstance(fitsimage, fits.hdu.image.PrimaryHDU):\n",
    "        header = fitsimage.header\n",
    "    else:\n",
    "        raise TypeError(\"Unknown format for fitsimage. Must be filename or HDUList.\")\n",
    "\n",
    "    crpix = header['crpix'+str(axis)]*u.pix\n",
    "    crval = u.Quantity(str(header['crval'+str(axis)])+header['cunit'+str(axis)])\n",
    "    cdelt = u.Quantity(str(header['cdelt'+str(axis)])+header['cunit'+str(axis)])/u.pix\n",
    "    naxis = header['naxis'+str(axis)]\n",
    "\n",
    "    if not coordinates.unit==crval.unit:\n",
    "        raise TypeError(\"Header unit is \"+str(crval.unit)+\". Use matching unit.\")\n",
    "\n",
    "    return np.round((coordinates-crval)/cdelt+crpix, precision)\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "#\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de5802-8143-4fb1-a85d-3b8dc627412a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Noise matching\n",
    "\n",
    "this is an unused function that is from Krieger 2020, where he adds additional noise to the CMZ image to match the noise present in the NGC253\n",
    "image. I do not do this because it creates a ton of false structures that dont exist and I dont want those in the CMZ data.\n",
    "\n",
    "I require at least 5 times the noise from a noise-channel before I allow the pixels to be considered real data.\n",
    "I calculate the noise for each cube after doing all the data reduction, which is expected to amplify the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7914523-d3a7-46bd-8407-f10c06de66e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return an input cube matched to the given nosie\n",
    "def Noise_matching(Input_Cube,m,manual_noise=0*u.K):\n",
    "    \n",
    "    datn = Input_Cube.hdu.data\n",
    "\n",
    "    npixels = np.product(Input_Cube.hdu.data.shape)\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    #######\n",
    "    # Calculate the RMS noise of the cube (for the line)\n",
    "    #######\n",
    "    #######\n",
    "\n",
    "    Non_nan=((datn[0]>0)  | (datn[0]<0 )) # All the data that is not a nan value in the first (emissionless) channel of the cube\n",
    "\n",
    "    m_current = np.nanstd(datn[0],where= Non_nan)*Input_Cube[0][0][0].unit #Noise (K)\n",
    "    \n",
    "    if manual_noise != 0*u.K:\n",
    "        m_current = manual_noise\n",
    "        \n",
    "    actual_noise = m_current\n",
    "    \n",
    "    print(\"Current RMS noise found to be:\",actual_noise,\"Default to manual?: \",manual_noise,\"Match to \",m)\n",
    "    \n",
    "    actual_noise=actual_noise.value\n",
    "    target_noise = m.value\n",
    "    \n",
    "    additional_sigma = np.sqrt(np.abs(target_noise**2 - actual_noise**2))\n",
    "\n",
    "    additional_noise = np.random.normal(0., additional_sigma, npixels)\n",
    "    additional_noise = np.reshape(additional_noise, Input_Cube.hdu.data.shape)\n",
    "\n",
    "\n",
    "    fwhm_factor = np.sqrt(8*np.log(2))\n",
    "    add_noise = np.zeros(np.shape(datn))\n",
    "    for lmi in range(len(datn)):\n",
    "        new_seed = np.random.randint(1e9)\n",
    "        additional_noise = gauss_correlated_noise_2D(shape=(Input_Cube.hdu.data[6].shape[0],Input_Cube.hdu.data[6].shape[1]), sigma=additional_sigma, beam_gauss_width=5/fwhm_factor,randomseed=new_seed)\n",
    "        pp=np.where(additional_noise!=np.nan)\n",
    "        add_noise[lmi][pp]=additional_noise[pp]\n",
    "\n",
    "    new_data = datn+add_noise\n",
    "    QCopy = Input_Cube.hdu\n",
    "    QCopy.data = new_data\n",
    "    Q = SpectralCube.read(QCopy)\n",
    "    del QCopy\n",
    "\n",
    "    \n",
    "\n",
    "    return Q\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce69b88-81f3-4b65-8f9f-b62c0a890854",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5229df-e6b6-4e5a-90e8-32c722d4049e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cluster finding\n",
    "\n",
    "Currently unused, but this can find the highly star forming clusters as detected by Levy 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee4b1a-ce6c-4375-a4b6-d045062fa5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "    \n",
    "def Read_Clusters(FileName):\n",
    "    \n",
    "    sh= len(np.genfromtxt(FileName,usecols=0))\n",
    "    Data=[]\n",
    "    for lmi in range(50):\n",
    "        try:\n",
    "            Data.append(np.genfromtxt(FileName,usecols=lmi,dtype=type(\"2d4m\")))\n",
    "            #print(np.genfromtxt(FileName,usecols=lmi,dtype=type(\"2d4m\"),skip_header=1))\n",
    "        except:\n",
    "            pass\n",
    "    return Data\n",
    "def Find_Clusters_NGC(Data):\n",
    "    for lmi in range(len(Data)):\n",
    "        if \"ID\" in Data[lmi]:\n",
    "            IDs= Data[lmi][1:9999]\n",
    "        if \"RA\" in Data[lmi]: \n",
    "            RAs= Data[lmi][1:9999]\n",
    "        if \"Dec\" in Data[lmi]:\n",
    "            Decs= Data[lmi][1:9999]\n",
    "        if \"r_deconv\" in Data[lmi]: \n",
    "            R_deconv= Data[lmi][1:9999]#pc\n",
    "        if \"glon\" in Data[lmi]: \n",
    "            glons= Data[lmi][1:9999]#\n",
    "        if \"glat\" in Data[lmi]: \n",
    "            glats= Data[lmi][1:9999]#\n",
    "            \n",
    "    return IDs,RAs,Decs,R_deconv\n",
    "#Take the cont in Jy and find the HWHM from the structures in the catalog\n",
    "def Find_Clusters(Data,wcs,Cont_Data,header):\n",
    "    for lmi in range(len(Data)):\n",
    "        if \"ID\" in Data[lmi]:\n",
    "            IDs= Data[lmi][1:9999]\n",
    "        if \"RA\" in Data[lmi]: \n",
    "            RAs= Data[lmi][1:9999]\n",
    "        if \"Dec\" in Data[lmi]:\n",
    "            Decs= Data[lmi][1:9999]\n",
    "        if \"r_deconv\" in Data[lmi]: \n",
    "            R_deconv= Data[lmi][1:9999]#pc\n",
    "        if \"glon\" in Data[lmi]: \n",
    "            glons= Data[lmi][1:9999]#\n",
    "        if \"glat\" in Data[lmi]: \n",
    "            glats= Data[lmi][1:9999]#\n",
    "        if \"herschel_column\" in Data[lmi]: \n",
    "            CD= (Data[lmi][1:9999])#pc\n",
    "            \n",
    "        if \"flux_integrated\" in Data[lmi]: \n",
    "            Flux_1p3mm= Data[lmi][1:9999]#pc\n",
    "    #remove nan \n",
    "    for lmii in range(len(CD)):\n",
    "        try:\n",
    "            if CD[lmii]=='np.nan':\n",
    "                CD= np.delete(CD, lmii)\n",
    "                Flux_1p3mm= np.delete(Flux_1p3mm, lmii)\n",
    "                IDs= np.delete(IDs, lmii)\n",
    "                glats= np.delete(glats, lmii)\n",
    "                glons= np.delete(glons, lmii)\n",
    "                \n",
    "        except:\n",
    "            CD = np.array(CD,dtype=type(1.2**5))#float\n",
    "            break\n",
    "    glats_New=[]\n",
    "    glons_New=[]\n",
    "    CDs_New=[]\n",
    "    IDs_New=[]\n",
    "    Flux_1p3mm_New=[]\n",
    "\n",
    "    #print(CD,sorted(CD),type(CD),type(CD[0]))\n",
    "    nth = sorted(CD)[len(CD)-34]#34 most dense leaves\n",
    "    #print(nth,\"A\",CD,sorted(CD))\n",
    "    for lmj in range(len(CD)):\n",
    "        if CD[lmj]>nth:\n",
    "            glats_New.append(glats[lmj])\n",
    "            glons_New.append(glons[lmj])\n",
    "            CDs_New.append(CD[lmj])\n",
    "            IDs_New.append(int(IDs[lmj]))\n",
    "            Flux_1p3mm_New.append(Flux_1p3mm[lmj])\n",
    "    HWHM_rad = []      \n",
    "    #print(Flux_1p3mm_New,glats_New,glons_New,CDs_New,IDs_New)\n",
    "    for lmi in range(len(CDs_New)):\n",
    "        glat = glats_New[lmi]\n",
    "        glon = glons_New[lmi]\n",
    "        Flux = float(Flux_1p3mm_New[lmi])#INtegerated flux in jy\n",
    "        \n",
    "        Circle_R = 0\n",
    "        distance = 8.178*10**-3*u.Mpc\n",
    "        \n",
    "        pixel_res = abs(header['cdelt1'])*np.pi/180*distance*10**6/u.Mpc*u.pc # cdelt in deg, goes to res in pc\n",
    "        \n",
    "        #sky = SkyCoord('00h47m33.9s', '-25d17m26.8s', frame='icrs')\n",
    "        sky = SkyCoord(l=float(glon)*u.deg, b=float(glat)*u.deg, frame='galactic')\n",
    "        #center = SkyCoord(l=359.94487501*u.degree,b=-00.04391769*u.degree, frame='galactic')\n",
    "        p1,p2 = int(wcs.world_to_pixel(sky)[0]),int(wcs.world_to_pixel(sky)[1]) #Ra,dec\n",
    "        \n",
    "        while(True):\n",
    "            Circle_R += .01\n",
    "            #pixels=[(p1,p2)]\n",
    "            pixels=[(p2,p1)]#Goes lat then long for the cont data\n",
    "            #print(p1,p2)\n",
    "            #print(np.shape(Cont_Data[p2-50:p2+50]))\n",
    "            #print(np.shape(Cont_Data[50,p1-50:p1+50]))\n",
    "            for lmii in range(np.shape(Cont_Data[p2-50:p2+50])[0]):\n",
    "                for lmjj in range(np.shape(Cont_Data[p2-50+lmii,p1-50:p1+50])[0]):\n",
    "                    #Find pixels within the circle around the center (excude the center since its there already)\n",
    "                    #print(np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res,lmjj)\n",
    "                    if np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res.value < Circle_R and lmjj!=50:\n",
    "                        pixels.append((lmjj-50+p2,lmii-50+p1))#Goes lat then long\n",
    "                        \n",
    "            \n",
    "            \n",
    "            sum_flux=0\n",
    "            for lmkk in range(len(pixels)):\n",
    "                sum_flux += (Cont_Data[pixels[lmkk]])\n",
    "            #print(p1,p2,glat,glon,np.shape(Cont_Data),pixels,Cont_Data[pixels[0]],Flux,sum_flux,Circle_R)\n",
    "            if sum_flux>Flux/2:\n",
    "                HWHM_rad.append(Circle_R)#Pc\n",
    "                break\n",
    "                \n",
    "    return HWHM_rad,CDs_New,glons_New,glats_New,IDs_New\n",
    "\n",
    "#Return masked data around clusters or one pc around clusters\n",
    "def Mask_Clusters_NGC(HWHM,wcs,header,unmasked_data,ras,decs,One_Pc=False,One_Pc_Size=1,HWHM_Fac=1):\n",
    "    \n",
    "    Masked_Data=copy.deepcopy(unmasked_data)\n",
    "    for lmi in range(len(HWHM)):\n",
    "        ra = ras[lmi]\n",
    "        dec = decs[lmi]\n",
    "                \n",
    "        Circle_R = HWHM[lmi]*HWHM_Fac\n",
    "        if(One_Pc):\n",
    "            \n",
    "            Circle_R=One_Pc_Size\n",
    "        distance = 3.5*u.Mpc\n",
    "        \n",
    "        pixel_res = abs(header['cdelt1'])*np.pi/180*distance*10**6/u.Mpc*u.pc # cdelt in deg, goes to res in pc\n",
    "        \n",
    "        #sky = SkyCoord('00h47m33.9s', '-25d17m26.8s', frame='icrs')\n",
    "        sky = SkyCoord(str(ra),str(dec), frame='icrs')\n",
    "        #center = SkyCoord(l=359.94487501*u.degree,b=-00.04391769*u.degree, frame='galactic')\n",
    "        p1,p2 = int(wcs.world_to_pixel(sky)[0]),int(wcs.world_to_pixel(sky)[1]) #Ra,dec\n",
    "        \n",
    "\n",
    "\n",
    "        #pixels=[(p1,p2)]\n",
    "        pixels=[(p2,p1)]#Goes lat then long for the cont data\n",
    "        #print(p1,p2)\n",
    "        #print(np.shape(Cont_Data[p2-50:p2+50]))\n",
    "        #print(np.shape(Cont_Data[50,p1-50:p1+50]))\n",
    "        for lmii in range(np.shape(unmasked_data[0,p2-50:p2+50])[0]):\n",
    "            for lmjj in range(np.shape(unmasked_data[0,p2-50+lmii,p1-50:p1+50])[0]):\n",
    "                #Find pixels within the circle around the center (excude the center since its there already)\n",
    "                #print(np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res,lmjj)\n",
    "                \n",
    "                if np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res.value < Circle_R and lmjj!=50:\n",
    "                    pixels.append((lmjj-50+p2,lmii-50+p1))#Goes lat then long\n",
    "        \n",
    "        for lmi in range(len(unmasked_data)):\n",
    "            \n",
    "            for lmj in range(len(pixels)):\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "                Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]]=np.nan\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "     \n",
    "    return Masked_Data\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def Mask_Clusters_CMZ(HWHM,wcs,header,unmasked_data,glons,glats,One_Pc=False,One_Pc_Size=1,HWHM_Fac=1):\n",
    "    \n",
    "    Masked_Data=copy.deepcopy(unmasked_data)\n",
    "    for lmi in range(len(HWHM)):\n",
    "        glon = glons[lmi]\n",
    "        glat = glats[lmi]\n",
    "                \n",
    "        Circle_R = HWHM[lmi]*HWHM_Fac\n",
    "        if(One_Pc):\n",
    "            \n",
    "            Circle_R=One_Pc_Size\n",
    "        distance = dist_cmz\n",
    "        \n",
    "        pixel_res = abs(header['cdelt1'])*np.pi/180*distance*10**6/u.Mpc*u.pc # cdelt in deg, goes to res in pc\n",
    "        \n",
    "        #sky = SkyCoord('00h47m33.9s', '-25d17m26.8s', frame='icrs')\n",
    "        sky = SkyCoord(float(glon)*u.deg,float(glat)*u.deg, frame='galactic')\n",
    "        #center = SkyCoord(l=359.94487501*u.degree,b=-00.04391769*u.degree, frame='galactic')\n",
    "        p1,p2 = int(wcs.world_to_pixel(sky)[0]),int(wcs.world_to_pixel(sky)[1]) #Ra,dec\n",
    "        \n",
    "\n",
    "\n",
    "        #pixels=[(p1,p2)]\n",
    "        pixels=[(p2,p1)]#Goes lat then long for the cont data\n",
    "        #print(p1,p2)\n",
    "        #print(np.shape(Cont_Data[p2-50:p2+50]))\n",
    "        #print(np.shape(Cont_Data[50,p1-50:p1+50]))\n",
    "        for lmii in range(np.shape(unmasked_data[0,p2-50:p2+50])[0]):\n",
    "            for lmjj in range(np.shape(unmasked_data[0,p2-50+lmii,p1-50:p1+50])[0]):\n",
    "                #Find pixels within the circle around the center (excude the center since its there already)\n",
    "                #print(np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res,lmjj)\n",
    "                \n",
    "                if np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res.value < Circle_R and lmjj!=50:\n",
    "                    pixels.append((lmjj-50+p2,lmii-50+p1))#Goes lat then long\n",
    "        \n",
    "        for lmi in range(len(unmasked_data)):\n",
    "            \n",
    "            for lmj in range(len(pixels)):\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "                Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]]=np.nan\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "     \n",
    "    return Masked_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06d394-e918-4240-b92c-e3b266deab33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"this isnt made correctly\n",
    "\n",
    "    ######## find the original FOV\n",
    "    \n",
    "    fp_dec,lp_dec,fp_ra,lp_ra=0,0,0,0\n",
    "    \n",
    "    for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "        for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "                if (data[0][lmj][lmk]>0 or data[0][lmj][lmk]<0):\n",
    "                    fp_dec=lmj#the first upwards pixel with data\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "        if (fp_dec != 0):\n",
    "            break\n",
    "    for lmj in range(1,np.shape(sc_for_cropping)[1]):\n",
    "        for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "                if (data[0][np.shape(sc_for_cropping)[1]-lmj][lmk]>0 or data[0][np.shape(sc_for_cropping)[1]-lmj][lmk]<0):\n",
    "                    lp_dec=np.shape(sc_for_cropping)[1]-lmj#the last upwards pixel with data\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "        if (lp_dec != 0):\n",
    "            break\n",
    "    for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "        for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "                if (data[0][lmj][lmk]>0 or data[0][lmj][lmk]<0):\n",
    "                    fp_ra=lmk # the first upwards pixel with data\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "        if (fp_ra != 0):\n",
    "            break\n",
    "    for lmk in range(1,np.shape(sc_for_cropping)[2]):\n",
    "        for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "                if (data[0][lmj][np.shape(sc_for_cropping)[2]-lmk]>0 or data[0][lmj][np.shape(sc_for_cropping)[2]-lmk]<0):\n",
    "                    lp_ra=np.shape(sc_for_cropping)[2]-lmk # the last upwards pixel with data\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "        if (lp_ra != 0):\n",
    "            break\n",
    "    print(fp_dec,lp_dec,fp_ra,lp_ra)\n",
    "            \n",
    "            \n",
    "    if(r!=0*u.deg):\n",
    "        if(lp_ra-fp_ra>lp_dec-fp_dec):\n",
    "            disk_pixels=(lp_dec-fp_dec)/np.sin(r_rad)\n",
    "            jet_pixels=(lp_dec-fp_dec)/np.sin(np.pi/2 - r_rad)\n",
    "        else:\n",
    "            disk_pixels=(lp_ra-fp_ra)/np.sin(r_rad)\n",
    "            jet_pixels=(lp_ra-fp_ra)/np.sin(np.pi/2 - r_rad)\n",
    "    else:\n",
    "        disk_pixels=lp_ra-fp_ra\n",
    "        jet_pixels=lp_dec-fp_dec\n",
    "        \n",
    "    print(disk_pixels,jet_pixels)\n",
    "    orig_fov=[0,0]            \n",
    "    orig_fov[0] = np.round(((disk_pixels*(cdelt_x.to(u.rad)*d))/u.rad).to(u.pc),1) #the fov across the disk\n",
    "    orig_fov[1] = np.round(((jet_pixels*(cdelt_x.to(u.rad)*d))/u.rad).to(u.pc),1) #the fov coming up from the disk\n",
    "    \n",
    "    \n",
    "    print(orig_fov)\n",
    "    \n",
    "    print(\"cropped cube from\",orig_fov,\"to:\",desired_fov)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f9a05-2e3c-4c10-9fd9-9c3b70f6b541",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spider crawl, a function used for modifying the input parameters to the dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8af054b5-5b93-46d1-9d17-274afa7c9678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def spider_crawl(file_paths, search_text, replacement_text,modify=False):\n",
    "    \"\"\"\n",
    "    Goes into a series of files and replaces only the matching part of a line.\n",
    "    \n",
    "    Parameters:\n",
    "    file_paths (list): List of file paths to modify.\n",
    "    search_text (str): The text to search for.\n",
    "    replacement_text (str): The text to replace the found text with.\n",
    "    \"\"\"\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            modified = False\n",
    "            if modify:\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    for line in lines:\n",
    "                        if search_text in line:\n",
    "                            new_line = re.sub(re.escape(search_text), replacement_text, line)\n",
    "                            if(modify):\n",
    "                                file.write(new_line)\n",
    "                                print(\"Modified\",line, \"\\n to\",new_line)\n",
    "                                modified = True\n",
    "\n",
    "                            else:\n",
    "                                file.write(line)\n",
    "                                print(new_line)\n",
    "                        else:\n",
    "                            file.write(line)\n",
    "            else:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in lines:\n",
    "                        if search_text in line:\n",
    "                            print(line)\n",
    "\n",
    "                            \n",
    "\n",
    "            if modified:\n",
    "                print(f\"Modified: {file_path}\")\n",
    "            else:\n",
    "                print(f\"No changes made: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error modifying {file_path}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# spider_crawl([\"file1.py\", \"file2.txt\"], \"old_text\", \"new_text\")\n",
    "\n",
    "\n",
    "def spider_crawl_find(file_paths, search_text,modify=False):\n",
    "    \"\"\"\n",
    "    Goes into a series of files and replaces only the matching part of a line.\n",
    "    \n",
    "    Parameters:\n",
    "    file_paths (list): List of file paths to modify.\n",
    "    search_text (str): The text to search for.\n",
    "    replacement_text (str): The text to replace the found text with.\n",
    "    \"\"\"\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "  \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in lines:\n",
    "                        if search_text in line:\n",
    "                            print(line)\n",
    "\n",
    "                            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding {file_path}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# spider_crawl([\"file1.py\", \"file2.txt\"], \"old_text\", \"new_text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f858a3fd-ab99-4580-88e2-29ad5182aa48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified: test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in atexit._run_exitfuncs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ben/.local/lib/python3.8/site-packages/IPython/core/history.py\", line 780, in writeout_cache\n",
      "    self._writeout_input_cache(conn)\n",
      "  File \"/home/ben/.local/lib/python3.8/site-packages/IPython/core/history.py\", line 763, in _writeout_input_cache\n",
      "    conn.execute(\"INSERT INTO history VALUES (?, ?, ?, ?)\",\n",
      "sqlite3.OperationalError: attempt to write a readonly database\n"
     ]
    }
   ],
   "source": [
    "#Dendrogram_Calculation(Cube_Information,Minimum_Pixel_Requirement=1,min_delta=5,noise_requirement=5)\n",
    "#ol = \"Ooooooooooooooh\"\n",
    "#ol2 = \"Dendrogram_Calculation(Cube_Information,Minimum_Pixel_Requirement=.5,min_delta=5,noise_requirement=5)\"\n",
    "#nl = \"Dendrogram_Calculation(Cube_Information,Minimum_Pixel_Requirement=0,min_delta=5,noise_requirement=5)\"\n",
    "#ols = ol# [ol,ol2]\n",
    "#spider_crawl([\"test.txt\"], ols, nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "734b6e92-5454-4d47-953e-4b3c74aecf17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516141e-18ed-498a-a624-c220ba0ace48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''import os\n",
    "\n",
    "def spider_crawl(file_paths, search_text, replacement_text):\n",
    "    \"\"\"\n",
    "    #Searches for a specific line of text in multiple files and replaces it with a new line.\n",
    "    #Creates a backup of each file before modifying it.\n",
    "    \n",
    "    #:param file_paths: List of file paths to process\n",
    "    #:param search_text: The text to search for in each file\n",
    "    #:param replacement_text: The text to replace the found line with\n",
    "    \"\"\"\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"Skipping {file_path}: Not a valid file.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "            modified = False\n",
    "            new_lines = []\n",
    "            for line in lines:\n",
    "                for s in search_text:\n",
    "                    if s in line:\n",
    "                        new_lines.append(replacement_text + '\\n')  # Replace the line\n",
    "                        modified = True\n",
    "                        break\n",
    "                else:\n",
    "                    new_lines.append(line)\n",
    "            \n",
    "            if modified:\n",
    "                backup_path = file_path + \".bak\"\n",
    "                os.rename(file_path, backup_path)  # Create a backup\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.writelines(new_lines)\n",
    "                print(f\"Updated {file_path}. Backup saved as {backup_path}.\")\n",
    "            else:\n",
    "                print(f\"No changes made to {file_path}: Search text not found.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# spider_crawl([\"file1.txt\", \"file2.py\"], \"old_line_of_code\", \"new_line_of_code\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
