{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "922609c5-86a7-4cbf-8068-8dd90d034a73",
   "metadata": {},
   "source": [
    "# Updates/Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de9dc9-474e-4fb7-8404-90eb4065006f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "#Need to update? Here ya go!!\n",
    "\n",
    "\n",
    "#!py -m pip uninstall astropy\n",
    "#!py -m pip install git+https://github.com/astropy/astropy\n",
    "#!pip install emcee\n",
    "!pip install corner\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "!py -m pip install git+https://github.com/radio-astro-tools/spectral-cube.git\n",
    "!py -m pip install reproject\n",
    "!py -m pip install git+https://github.com/radio-astro-tools/spectral-cube.git \n",
    "!py -m pip install pyspeckit\n",
    "!py -m pip install regions\n",
    "!py -m pip install astrodendro\n",
    "!py -m pip  install wcsaxes \n",
    "!py -m pip  install ipympl\n",
    "!py -m pip install dask\n",
    "!py -m pip install radio_beam\n",
    "!py -m pip install casa_formats_io\n",
    "#try:\n",
    "#    !pip install casa_formats_io --no-binary :all:\n",
    "#except:\n",
    "#    !pip install casa_formats_io --no-cache --no-binary :all:\n",
    "\n",
    "!py -m pip  install spectral_cube \n",
    "!py -m pip  install typing \n",
    "!py -m pip install mypy\n",
    "!py -m pip  install typing_extensions \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9fb49-ce1d-425e-a426-1a0cfa419903",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdc0ed5b-634f-455a-80e0-6f7bfa128b70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/bin/python\n",
      "3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)\n",
      "astropy 5.1.dev153+gb740594dc\n",
      "reproject 0.8\n",
      "spectral_cube 0.6.1.dev22+g003ef16\n",
      "/home/ben/.local/lib/python3.8/site-packages/spectral_cube/__init__.py\n",
      "/home/ben/.local/lib/python3.8/site-packages/astrodendro/__init__.py\n",
      "1.23.1 Numpy\n",
      "Results will be saved to ./Result Files\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "\n",
    "#Every data reduction and analysis file will use these imports and functions.\n",
    "#So i run this file at the beggining of every other file to import stuff.\n",
    "\n",
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "######################################################################################################################################################################################################################################\n",
    "\n",
    "#These will show you what version of Python you are working with. Important because astropy works best with certain versions like 3.8.5\n",
    "\n",
    "import sys, traceback\n",
    "\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# The most important package for astronomy\n",
    "\n",
    "import astropy\n",
    "from astropy.coordinates import SkyCoord\n",
    "print('astropy',astropy.__version__ )\n",
    "import astropy.io.fits as fits              \n",
    "from astropy.wcs import WCS # World coordinate system\n",
    "from astropy import units as u  \n",
    "from astropy.table import Table\n",
    "from astropy.convolution import Gaussian1DKernel\n",
    "from astropy.utils import NumpyRNGContext\n",
    "\n",
    "\n",
    "# Spectral cubes are amazing at taking fits images and turning them into workable position-position-velocity cubes \n",
    "# (The cubes are 3D, but the moment maps/continuum images will be 2D and SC will also work for them)\n",
    "\n",
    "from spectral_cube import SpectralCube    \n",
    "from spectral_cube import LazyMask\n",
    "import spectral_cube\n",
    "print('spectral_cube',spectral_cube.__version__)\n",
    "print('spectral_cube file path',spectral_cube.__file__)\n",
    "\n",
    "# Need this for projection of the cubes\n",
    "\n",
    "from reproject import reproject_interp      \n",
    "from reproject.mosaicking import find_optimal_celestial_wcs \n",
    "import regions\n",
    "import reproject\n",
    "print('reproject',reproject.__version__)\n",
    "\n",
    "# Useful for doing analysis\n",
    "\n",
    "import pylab                                \n",
    "import matplotlib \n",
    "import matplotlib.gridspec as gridspec                                                                                             \n",
    "import matplotlib.colors as colors\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy\n",
    "import numpy as np                          \n",
    "from matplotlib.patches import Ellipse\n",
    "print(np.__version__,\"Numpy\")\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.optimize import leastsq\n",
    "\n",
    "# astrodendro is a key package, it allows a quick way to identify structures\n",
    "\n",
    "import astrodendro \n",
    "from astrodendro.analysis import PPVStatistic # Takes statistics of PPV structures\n",
    "\n",
    "print(\"astrodendro_file:\", astrodendro.__file__)\n",
    "\n",
    "# The radio_beam library allows you to check/change teh interferometric properties of the beam.\n",
    "\n",
    "import radio_beam\n",
    "\n",
    "# Garbage collection\n",
    "\n",
    "import gc\n",
    "\n",
    "# Suppress warnings we don't care about:\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    \n",
    "#     \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#\n",
    "\n",
    "gal=\"GC\"\n",
    "\n",
    "dist_cmz = 8.178*10**-3*u.Mpc\n",
    "   \n",
    "    \n",
    "\n",
    "if(os.path.exists(\"./Result Files\")):\n",
    "    print(\"Results will be saved to Directory ./Result Files\")\n",
    "else:\n",
    "    %mkdir \"./Result Files\"\n",
    "    print(\"Created new directory ./Result Files where the results will be saved\")\n",
    "    \n",
    "# If you need to interact with teh plots, use widget, otherwise this runs better\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib widget \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e724d-9141-4353-b6b9-da1591dcf8d9",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "Make general functions for each process so I can call them simply.\n",
    "The only things I need to change are the input files and their properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a18e1-3372-4c94-be24-f4c3acbe8e6d",
   "metadata": {},
   "source": [
    "# Pointing Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9e65a-7352-4c62-b1bd-d666e7fa8ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pointing_Information\n",
    "\n",
    "\n",
    "# Pointing_Information an important variable that will need to be defined for each source\n",
    "# It is a dictionary containing all the metadata and pointing information. This is important because the PPVstatistic needs it to be in a specific format\n",
    "#\n",
    "# eg Pointing_Information:  \n",
    "'''\n",
    "\n",
    "Pointing_Information = {}\n",
    "\n",
    "#This is the stuff that needs changing between cubes\n",
    "\n",
    "Pointing_Information[\"Original_File_Name\"] = \"HCN_J1-0.cube.fits\" #the name of the initial SC file.\n",
    "Pointing_Information[\"File_Descriptor\"] = \"NGC_253_HCN_J1-0_\"\n",
    "Pointing_Information[\"target\"] = \"NGC253\"#or the cmz\n",
    "Pointing_Information[\"center\"] = SkyCoord('00h47m33.14s' ,'-25d17m17.52s',frame='icrs') #the center of NGC253\n",
    "#I use center = SkyCoord('-00d03m20.76s  ', '-00d02m46.176s', frame='galactic') for the cmz center\n",
    "desired_beam_size = 4.3*u.pc #I add this to the PI later. choose this based on the largest common beam size between the compared observations\n",
    "distance = 3.5*u.Mpc\n",
    "Pointing_Information[\"distance\"] = distance.to(u.Mpc) #in Mpc\n",
    "Pointing_Information[\"target_image_rotation\"]=33*u.deg #this is the rotation of the specific image, not the target. (use clockwise rotation angle)\n",
    "Pointing_Information[\"target_inclination\"]=78*u.deg\n",
    "Pointing_Information[\"target_velocity\"]=250*u.km/u.s #the speed NGC253 is moving away from us\n",
    "Pointing_Information[\"vaxis\"]=0 #which axis is the velocity\n",
    "Pointing_Information[\"desired_velocity_resolution\"]= 3.3*u.km/u.s\n",
    "ovs = 3 #how much do you desire to oversample the beam by\n",
    "desired_fov = [360*u.pc,70*u.pc]\n",
    "\n",
    "\n",
    "#This stuff will input automagically if the rest is correct\n",
    "\n",
    "sc = SpectralCube.read(\"Spectral Cubes/\"+Pointing_Information[\"Original_File_Name\"])\n",
    "header = sc.header\n",
    "\n",
    "try:\n",
    "    freq = header[\"RESTFREQ\"]*u.Hz# assuming the header is in Hz\n",
    "    Pointing_Information['wavelength']=299792458*u.m/header[\"RESTFREQ\"]#\n",
    "    Pointing_Information['restfreq']=header[\"RESTFREQ\"]#            \n",
    "except:\n",
    "    freq = header[\"RESTFRQ\"]*u.Hz#\n",
    "    Pointing_Information['wavelength']=299792458*u.m/header[\"RESTFRQ\"]#            \n",
    "    Pointing_Information['restfreq']=header[\"RESTFRQ\"]#    \n",
    "\n",
    "######calculate teh beam size of the original image:\n",
    "\n",
    "if (header['CUNIT1'].find(\"deg\")!=-1):\n",
    "    CUNIT = 1*u.degree\n",
    "    Pointing_Information[\"CUNIT\"]=CUNIT\n",
    "else:\n",
    "    print(\"The header should show CUNIT in degrees. If not, just fix this or write CUNIT = the unit it says in the header\")\n",
    "    del jhgasdhgjkahsdkgdfjhsgjgjsdhkfjghjd #this causes an error and stops execution \n",
    "    \n",
    "beam_major =  (header[\"BMAJ\"]*CUNIT).to(u.arcsec) #degrees beam size -> arcsec\n",
    "beam_minor =  (header[\"BMIN\"]*CUNIT).to(u.arcsec)\n",
    "\n",
    "\n",
    "Pointing_Information[\"original_BMAJ\"]=beam_major\n",
    "Pointing_Information[\"original_BMIN\"]=beam_minor\n",
    "Pointing_Information[\"original_BMAJ_pc\"]=beam_major.to(u.rad)*Pointing_Information['distance']\n",
    "Pointing_Information[\"original_BMIN_pc\"]=beam_minor.to(u.rad)*Pointing_Information['distance']\n",
    "Pointing_Information[\"desired_beam_size\"] = desired_beam_size #ill put it in a circular beam at this size\n",
    "\n",
    "\n",
    "#this accounts for elliptical beams:            \n",
    "Pointing_Information['original_pixel_scale_x'] = (abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix\n",
    "Pointing_Information['original_pixel_scale_y'] = (abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix\n",
    "Pointing_Information['original_spatial_scale_x'] = (abs(header[\"CDELT1\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance']\n",
    "Pointing_Information['original_spatial_scale_y'] = (abs(header[\"CDELT2\"])*CUNIT.to(u.rad))/u.pix*Pointing_Information['distance']#convert to pc using the distance\n",
    "\n",
    "average_pixel=np.sqrt((abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix*(abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix)\n",
    "\n",
    "Pointing_Information['original_beam_oversampling_MAJ'] = beam_major/average_pixel\n",
    "Pointing_Information['original_beam_oversampling_MIN'] = beam_minor/average_pixel\n",
    "Pointing_Information['desired_beam_oversampling'] = ovs\n",
    "Pointing_Information['orig_FOV']= Crop_Around_Center(sc,Pointing_Information['target'],Pointing_Information['target_image_rotation'],Pointing_Information['center'],desired_fov,Pointing_Information['distance'])[1] #returns the current fov\n",
    "Pointing_Information['desired_FOV']=desired_FOV\n",
    "\n",
    "######\n",
    "\n",
    "\n",
    "#Cube_Information. This needs to be updated every time a reduction occurs on the cube. By the end of file 1 which does all the reprojection, this will not change.\n",
    "#The pointing information will not change, so I'll start with a copy of that:\n",
    "\n",
    "Cube_Information = copy.deepcopy(Pointing_Information)\n",
    "\n",
    "#For example, after doing the reprojection, I need to load: \n",
    "\n",
    "\n",
    "Current_Cube_Name = Pointing_Information[\"File_Descriptor\"] + str(Pointing_Information[\"desired_beam_size\"].value)+\"pc_beam_\"+str(FOVp[0])+\"x\"+str(FOVp[1])+'pc_'++str(i)+'.fits'\n",
    "Cube_Information[\"File_Name\"]=Current_Cube_Name\n",
    "\n",
    "sc = SpectralCube.read(Current_Cube_Name)\n",
    "header = sc.header\n",
    "\n",
    "pc_per_pixelx = abs(header[\"CDELT1\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "pc_per_pixely = abs(header[\"CDELT2\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "\n",
    "Cube_Information[\"pixel_scale_x\"]=pc_per_pixelx\n",
    "Cube_Information[\"pixel_scale_y\"]=pc_per_pixely\n",
    "\n",
    "Cube_Information['data_unit'] =sc[0][0][0].unit# or just use sc.header['BUNIT']\n",
    "\n",
    "Cube_Information['arc_per_pix_y'] =  abs(header[\"CDELT1\"]*CUNIT).to(u.arcsec)/u.pix \n",
    "Cube_Information['arc_per_pix_x'] =  abs(header[\"CDELT2\"]*CUNIT).to(u.arcsec)/u.pix\n",
    "\n",
    "beam_major =  (header[\"BMAJ\"]*CUNIT).to(u.arcsec) #degrees beam size -> arcsec\n",
    "beam_minor =  (header[\"BMIN\"]*CUNIT).to(u.arcsec) #if these two are different, something went wrong in the reprojection\n",
    "Cube_Information['beam_major'] =  beam_major\n",
    "Cube_Information['beam_minor'] =  beam_minor\n",
    "\n",
    "beam_area_ratio = beam_minor*beam_major/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']*1.13309#beam_area_ratio = Cube_Information['beam_minor']*Cube_Information['beam_major']/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']#This is for FWHM, use *(2*np.sqrt(2*np.log(2)))**2#For gaussian beam\n",
    "Cube_Information['beam_area_ratio']=beam_area_ratio #This is important for finding the minimum number of pixels a structure can have, or for calculating column densities\n",
    "\n",
    "#this accounts for elliptical beams:            \n",
    "Cube_Information['spatial_scale_x'] = abs(header[\"CDELT1\"])*CUNIT/u.pix\n",
    "Cube_Information['spatial_scale_y'] = abs(header[\"CDELT2\"])*CUNIT/u.pix\n",
    "Cube_Information[\"velocity_scale\"] = abs(header[\"CDELT3\"])*u.km/u.s#the cube should be in u.km/u.s instead of frequency on the z axis\n",
    "\n",
    "average_pixel=np.sqrt((abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix*(abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix)\n",
    "\n",
    "Cube_Information['beam_oversampling'] = beam_minor/average_pixel\n",
    "Cube_Information['desired_beam_oversampling'] = Pointing_Information['desired_beam_oversampling']\n",
    "\n",
    "Cube_Information[\"wcsu\"]=sc.wcs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#make a function to do this easily\n",
    "\n",
    "def Update_Cube_Information(Pointing_Information,Current_Cube_Name):\n",
    "\n",
    "    Cube_Information = copy.deepcopy(Pointing_Information)\n",
    "\n",
    "    Cube_Information[\"File_Name\"]=Current_Cube_Name\n",
    "    try:\n",
    "        del sc\n",
    "    except:\n",
    "        pass\n",
    "    sc = SpectralCube.read(\"Spectral Cubes/\"+Current_Cube_Name)\n",
    "    header = sc.header\n",
    "\n",
    "    pc_per_pixelx = abs(header[\"CDELT1\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "    pc_per_pixely = abs(header[\"CDELT2\"]*Cube_Information[\"CUNIT\"].to(u.rad))*Pointing_Information['distance']/u.pix #convert degrees to size using the distance\n",
    "\n",
    "    Cube_Information[\"pixel_scale_x\"]=pc_per_pixelx\n",
    "    Cube_Information[\"pixel_scale_y\"]=pc_per_pixely\n",
    "\n",
    "    Cube_Information['data_unit'] =sc[0][0][0].unit# or just use sc.header['BUNIT']\n",
    "\n",
    "    Cube_Information['arc_per_pix_y'] =  abs(header[\"CDELT1\"]*CUNIT).to(u.arcsec)/u.pix \n",
    "    Cube_Information['arc_per_pix_x'] =  abs(header[\"CDELT2\"]*CUNIT).to(u.arcsec)/u.pix\n",
    "\n",
    "    beam_major =  (header[\"BMAJ\"]*CUNIT).to(u.arcsec) #degrees beam size -> arcsec\n",
    "    beam_minor =  (header[\"BMIN\"]*CUNIT).to(u.arcsec) #if these two are different, something went wrong in the reprojection\n",
    "    Cube_Information['beam_major'] =  beam_major\n",
    "    Cube_Information['beam_minor'] =  beam_minor\n",
    "\n",
    "    beam_area_ratio = beam_minor*beam_major/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']*1.13309#beam_area_ratio = Cube_Information['beam_minor']*Cube_Information['beam_major']/Cube_Information['arc_per_pix_y']/Cube_Information['arc_per_pix_x']#This is for FWHM, use *(2*np.sqrt(2*np.log(2)))**2#For gaussian beam\n",
    "    Cube_Information['beam_area_ratio']=beam_area_ratio #This is important for finding the minimum number of pixels a structure can have, or for calculating column densities\n",
    "\n",
    "    #this accounts for elliptical beams:            \n",
    "    Cube_Information['spatial_scale_x'] = abs(header[\"CDELT1\"])*CUNIT/u.pix\n",
    "    Cube_Information['spatial_scale_y'] = abs(header[\"CDELT2\"])*CUNIT/u.pix\n",
    "    Cube_Information[\"velocity_scale\"] = abs(header[\"CDELT3\"])*u.km/u.s#the cube should be in u.km/u.s instead of frequency on the z axis\n",
    "\n",
    "    average_pixel=np.sqrt((abs(header[\"CDELT1\"])*CUNIT.to(u.arcsec))/u.pix*(abs(header[\"CDELT2\"])*CUNIT.to(u.arcsec))/u.pix)\n",
    "\n",
    "    Cube_Information['beam_oversampling'] = beam_minor/average_pixel\n",
    "    Cube_Information['desired_beam_oversampling'] = Pointing_Information['desired_beam_oversampling']\n",
    "\n",
    "    Cube_Information[\"wcsu\"]=sc.wcs\n",
    "    \n",
    "    Cube_Information['FOV']= Crop_Around_Center(sc,Cube_Information['target_image_rotation'],Cube_Information['center'],Cube_Information['desired_FOV'],Cube_Information['distance'])[1] #returns [the cropped sc, the current fov, the desired fov]\n",
    "    Cube_Information['desired_FOV']=Pointing_Information['desired_FOV']\n",
    "    \n",
    "    return Cube_Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6adba-7634-47ba-a3df-41fef9d61862",
   "metadata": {},
   "source": [
    "# Reprojection function\n",
    "\n",
    "To align the files across and get the same beam size and FOVs along different observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149233c-fc12-405c-b28b-c205e426cd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Spatial reprojection \n",
    "\n",
    "# File = the file name of the SC you want to reproject\n",
    "# Prime_Beam = the pc size of the beam. When comparing a line in two different sources, i match the beam sizes across the sources\n",
    "\n",
    "# Pointing_Information= see earlier\n",
    "# Cube_Information= see earlier\n",
    "# i_step = the amount of velocity channels for each step in the reprojection. lower this if your computer lags a lot.\n",
    "\n",
    "def Reproject_To_Region(Pointing_Information,Cube_Information,i_step=30,Cube_Name_Save='',Force_Origin=[False,[0,0]*u.deg,[0,0]*u.deg]):\n",
    "    \n",
    "    File = Cube_Information[\"File_Name\"]\n",
    "    Prime_Beam = Pointing_Information[\"desired_beam_size\"]\n",
    "    Gal = Pointing_Information[\"target\"]\n",
    "    ovs = Pointing_Information[\"desired_beam_oversampling\"]\n",
    "    FOV = Pointing_Information[\"desired_FOV\"]\n",
    "    distance=Pointing_Information[\"distance\"]\n",
    "    target_velocity= Pointing_Information[\"target_velocity\"]\n",
    "    center = Pointing_Information[\"center\"]\n",
    "    rotation_angle = Pointing_Information[\"target_image_rotation\"]\n",
    "    Line_Name = Pointing_Information[\"File_Descriptor\"]\n",
    "\n",
    "    \n",
    "    # These are related to error correction the original CMZ cubes because they have an error with their longitude values going from 360->0 through the center\n",
    "    \n",
    "    Force_Origins=Force_Origin[0]\n",
    "    Force_Value_x=Force_Origin[1]\n",
    "    Force_Value_y=Force_Origin[2]\n",
    "    \n",
    "    #Load observation cube\n",
    "    \n",
    "    scB = SpectralCube.read(File) \n",
    "    scB.allow_huge_operations=True\n",
    "    scB = scB.with_spectral_unit(u.km/u.s,velocity_convention=\"radio\") # Change units from Hz to km/s in case they are in Hz\n",
    "    \n",
    "    #Determine the resolution based on the oversample factor\n",
    "    \n",
    "    beam_deg =  ((Prime_Beam/(distance))*u.rad).to(u.deg)#deg corresponding to the desired beam size\n",
    "    \n",
    "    # Check the coordinate systems\n",
    "    \n",
    "    vel,RA,Dec = scB.world[:,0,0]\n",
    "    \n",
    "    # Need to break it up into 30-wide vel slices to do the reprojection (ram-draw too high otherwise)  \n",
    "    \n",
    "    for i in range(int(len(scB)/i_step) +1):\n",
    "        \n",
    "        try:\n",
    "            print('begin step:',i,\"of\",int(len(scB)/i_step) +1)\n",
    "            \n",
    "            Cube_Name_Save = str(Prime_Beam.value)+\"pc_beam_\"+Line_Name+str(FOV[0].value)+\"x\"+str(FOV[1].value)+'pc_'+'reprojected.fits'\n",
    "            \n",
    "            sc = scB[i*i_step:i*i_step+i_step]\n",
    "            sc = sc.spectral_slab(target_velocity- 251*u.km/u.s, target_velocity+ 251*u.km/u.s)  # Crop out velocities we don't care about  \n",
    "            sc.allow_huge_operations=True\n",
    "            \n",
    "            print('start beam convolution')\n",
    "\n",
    "            try:\n",
    "                #Make a circular beam to convolve the image to.\n",
    "    \n",
    "                beam = radio_beam.Beam(major=beam_deg, minor=beam_deg, pa=0*u.deg)\n",
    "                sc = sc.convolve_to(beam)\n",
    "            \n",
    "            except:\n",
    "                print(\"The initial image has no beam because it's not interferometric, so one will be created using the pixel size as the initial beam size\")\n",
    "                \n",
    "                cdelt_x = u.Quantity(str(np.abs(sc.header['cdelt1']))+sc.header['cunit1'])\n",
    "                cdelt_y = u.Quantity(str(np.abs(sc.header['cdelt2']))+sc.header['cunit2'])\n",
    "                if(cdelt_x>cdelt_y):\n",
    "                    majorBase=cdelt_x\n",
    "                    minorBase=cdelt_y\n",
    "                elif(cdelt_x<cdelt_y):\n",
    "                    majorBase=cdelt_y\n",
    "                    minorBase=cdelt_x\n",
    "                elif(cdelt_x==cdelt_y):\n",
    "                    majorBase=cdelt_x\n",
    "                    minorBase=cdelt_x\n",
    "                    \n",
    "                BaseBeam = radio_beam.Beam(major=majorBase, minor=minorBase, pa=0*u.deg)\n",
    "\n",
    "                sc = sc.with_beam(BaseBeam)\n",
    "                beam = radio_beam.Beam(major=beam_deg, minor=beam_deg, pa=0*u.deg)\n",
    "\n",
    "                sc.allow_huge_operations=True\n",
    "                #Requires me to edit convolve.py and set allow_huge =True\n",
    "                #If this fails, go edit that file in your repository.\n",
    "                sc = sc.convolve_to(beam)\n",
    "                \n",
    "            print('convolve end\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "            #Mask the pixels outside the fov\n",
    "            \n",
    "            #returns: cropped_sc,orig_fov,FOV\n",
    "            print('fov crop start \\n')\n",
    "            cropped_sc = Crop_Around_Center(sc,rotation_angle,center,FOV,distance)[0]\n",
    "            del sc\n",
    "            sc = cropped_sc\n",
    "            del cropped_sc\n",
    "            print('fov cropped\\n')\n",
    "            \n",
    "            \n",
    "            #\n",
    "            #prepare a header for the reprojection\n",
    "            #\n",
    "            \n",
    "            reheader = copy.deepcopy(sc.hdu.header)\n",
    "            \n",
    "\n",
    "            ## Find the number of expected pixels for the new resolution and the location of the left/right, up/down sides \n",
    "\n",
    "            #find out which direction the cube is read, left to right or right/to left. (in terms of RA/DEC). Then, do the same for up and down\n",
    "            \n",
    "            if sc.header['cdelt1']>0:\n",
    "                pix_x    = (beam_deg/ovs).to(u.degree).value\n",
    "                origin_x = sc.longitude_extrema[0].to(u.degree).value\n",
    "                \n",
    "                if(Force_Origins):\n",
    "                    origin_x = Force_Value_x[0]#358.6\n",
    "\n",
    "            else:\n",
    "                pix_x    = -1.*(beam_deg/ovs).to(u.degree).value\n",
    "                origin_x = (sc.longitude_extrema[1]).to(u.degree).value\n",
    "                \n",
    "                if(Force_Origins):\n",
    "                    origin_x = Force_Value_x[1]#.9\n",
    "\n",
    "            if sc.header['cdelt2']>0:\n",
    "                pix_y    = (beam_deg/ovs).to(u.degree).value\n",
    "                origin_y = sc.latitude_extrema[0].to(u.degree).value\n",
    "                \n",
    "                if(Force_Origins):\n",
    "                    origin_y = Force_Value_y[0]#-.6\n",
    "                    \n",
    "            else:\n",
    "                pix_y    = -1.*(beam_deg/ovs).to(u.degree).value\n",
    "                origin_y = sc.latitude_extrema[1].to(u.degree).value\n",
    "                \n",
    "                if(Force_Origins):\n",
    "                    origin_y = Force_Value_y[1]#.6\n",
    "                    \n",
    "\n",
    "            npix_x   = int(np.ceil(np.diff(sc.longitude_extrema, n=1)[0]/np.abs(pix_x)).value)\n",
    "            npix_y   = int(np.ceil(np.diff(sc.latitude_extrema, n=1)[0]/np.abs(pix_y)).value)\n",
    "            \n",
    "            if(Force_Origins):\n",
    "                npix_x   =int(np.diff(Force_Value_x)/np.abs(pix_x).value)\n",
    "                npix_y   =int(np.diff(Force_Value_y)/np.abs(pix_y).value)\n",
    "                \n",
    "            #Correct the header to the expected pixels for the new res\n",
    "\n",
    "            reheader['cdelt1'] = pix_x\n",
    "            reheader['cdelt2'] = pix_y\n",
    "\n",
    "            reheader['naxis1'] = npix_x\n",
    "            reheader['naxis2'] = npix_y\n",
    "\n",
    "            reheader['crval1'] = origin_x\n",
    "            reheader['crval2'] = origin_y\n",
    "\n",
    "            reheader['crpix1'] = 0\n",
    "            reheader['crpix2'] = 0\n",
    "            \n",
    "            if(str(sc.wcs).find(\"GLON\")!=-1):\n",
    "                reheader['CTYPE1'] = \"GLON-SIN\"\n",
    "                reheader['CTYPE2'] = \"GLAT-SIN\"\n",
    "                \n",
    "            # remove these things that confuse the reprojection since we won't be using them\n",
    "            try:\n",
    "                del reheader['lonpole']\n",
    "                del reheader['latpole']\n",
    "                del reheader['wcsaxes']# Dont need these anymore, maybe?\n",
    "                if(str(sc.wcs).find(\"GLON\")!=-1):\n",
    "\n",
    "                    del reheader['LBOUND1']\n",
    "                    del reheader['LBOUND2']\n",
    "                    del reheader['LBOUND3']\n",
    "                    del reheader.cards['LBOUND1']\n",
    "                    del reheader.cards['LBOUND2']\n",
    "                    del reheader.cards['LBOUND3']\n",
    "\n",
    "                    reheader['LBOUND1']=0\n",
    "                    reheader['LBOUND2']=0\n",
    "                    reheader['LBOUND3']=0\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"-\"*60)\n",
    "                traceback.print_exc(file=sys.stdout)\n",
    "\n",
    "            # regrid cube to target pixel size\n",
    "\n",
    "            print('start reprojection\\n')\n",
    "            print('check max SC value:',np.nanmax(sc),\"SC shape:\", np.shape(sc))#These should be a non zero float and the shape of the cube (30,~1000,~1000)\n",
    "            \n",
    "            sc2 = sc.reproject(reheader, order='bilinear', use_memmap=True, filled=True) # Had to change reproject.py so it deletes output.np before making a new one\n",
    "\n",
    "            del sc # save space\n",
    "\n",
    "            # make a new cube with the reprojcted data (remove all the logs from the old cube)\n",
    "            \n",
    "            new = SpectralCube(data=sc2.hdu.data,wcs =WCS(sc2.header),header=sc2.header,mask=sc2.mask)\n",
    "            new.allow_huge_operations=True\n",
    "            new = new*sc2[0][0][0].unit\n",
    "            \n",
    "            #do this because scs dont like being modified\n",
    "            del sc2\n",
    "            sc2 = new\n",
    "            del new\n",
    "            \n",
    "            print('\\nend reprojection\\n')\n",
    "            print('check max SC value:',np.nanmax(sc2),\"SC shape:\", np.shape(sc2))#These should ALSO be a non zero float and the shape of the cube (30,~1000,~1000)\n",
    "\n",
    "            sc = sc2\n",
    "            del sc2\n",
    "            sc.allow_huge_operations=True\n",
    "\n",
    "            #Mask the pixels outside the fov again after the reprojection to get rid of nan created pixels\n",
    "            \n",
    "            print('fov crop start 2 \\n')\n",
    "            cropped_sc = Crop_Around_Center(sc,rotation_angle,center,FOV,distance)[0]\n",
    "            del sc\n",
    "            sc = cropped_sc\n",
    "            del cropped_sc\n",
    "            print('fov cropped 2\\n')\n",
    "            \n",
    "            # Write the intermediary cubes that will be spliced together\n",
    "            \n",
    "            sc.write(\"Spectral Cubes/\"+str(str(i)+\"_\"+Cube_Name_Save),overwrite=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Failed (unless this says attempt to get argmin of empty sequence)\")\n",
    "            print(\"-\"*60)\n",
    "            traceback.print_exc(file=sys.stdout)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04e97c-3232-4c4a-9a3a-273aec12e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#############\n",
    "# Crop_Around_Center\n",
    "#############\n",
    "\n",
    "\n",
    "#This function crops out things outside the rectangle where the actual disk lies.\n",
    "#The disk may be rotated, so this will be a rotated rectangle.\n",
    "#Without a circular beam, this will be off, but I only use it after I circularize the beam.\n",
    "\n",
    "\n",
    "#returns: cropped_sc,orig_fov,FOV\n",
    "\n",
    "def Crop_Around_Center(sc_for_cropping,rotation_angle,center,desired_fov,distance):\n",
    "\n",
    "    \n",
    "    \n",
    "    r,c,FOV,d = rotation_angle,center,desired_fov,distance\n",
    "    \n",
    "    \n",
    "    cdelt_x = u.Quantity(str(np.abs(sc_for_cropping.header['cdelt1']))+sc_for_cropping.header['cunit1'])\n",
    "    cdelt_y = u.Quantity(str(np.abs(sc_for_cropping.header['cdelt2']))+sc_for_cropping.header['cunit2'])\n",
    "    center_ra_pix,center_dec_pix = [int(sc_for_cropping.wcs[:][:][0].world_to_pixel(c)[0]),int(sc_for_cropping.wcs[:][:][0].world_to_pixel(c)[1])]\n",
    "    PixFov = [int((FOV[0].to(u.pc)/(cdelt_x.to(u.rad)*d.to(u.pc))).value/2),int((FOV[1].to(u.pc)/(cdelt_x.to(u.rad)*d.to(u.pc))).value/2)] #they'll be in pixels, but I only need the int\n",
    "\n",
    "    print(\"Center:\",c,\"Pixel center:\",center_ra_pix,center_dec_pix,\"Pixel FOV:\",PixFov)\n",
    "    \n",
    "    pixels = np.zeros(np.shape(sc_for_cropping))           \n",
    "    \n",
    "    print(\"cropping cube. rotation:\",r,\"center:\",center,\"crop to:\",desired_fov)\n",
    "    \n",
    "    if(r!=0*u.deg):\n",
    "        #to save time\n",
    "        r_rad=r.to(u.rad).value\n",
    "        #Find the pixels that are outside the rectangular FOV\n",
    "        for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "            for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "\n",
    "                #The hypotenuse\n",
    "                hypo = np.sqrt(((lmj-center_dec_pix)**2) + (lmk-center_ra_pix)**2)\n",
    "\n",
    "                if (lmj-center_dec_pix!=0):\n",
    "                    ang = np.arctan(abs(lmk-center_ra_pix)/abs(lmj-center_dec_pix))#*u.rad#Find angle to the center\n",
    "                else:\n",
    "                    ang = np.pi/2#*u.rad\n",
    "                if(lmk>center_ra_pix and lmj>center_dec_pix):\n",
    "                    ang*=-1\n",
    "                elif(lmk<center_ra_pix and lmj<center_dec_pix):\n",
    "                    ang*=-1\n",
    "                elif(lmk>center_ra_pix and lmj<center_dec_pix):\n",
    "                    if ang > (np.pi/2-r_rad):\n",
    "\n",
    "                        ang= -r_rad+(np.pi-r_rad)-ang#coming from the opposite end of the ra axis now, but projecting still to 33 deg from north.\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                up_pixels = abs(hypo*np.cos(abs(r_rad+ang)))\n",
    "                side_pixels = abs(hypo*np.sin(abs(r_rad)+ang))\n",
    "\n",
    "                #Check if the pixels are inside the FOV\n",
    "                if(up_pixels<PixFov[0] and side_pixels<PixFov[1]):\n",
    "                    for lmi in range(np.shape(sc_for_cropping)[0]):\n",
    "                        pixels[lmi][lmj][lmk] = 1  # keep this pixel\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        # If the image is not rotated, the FOV is just a rectangle\n",
    "        \n",
    "        # Find all pixels in the fov\n",
    "        \n",
    "        for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "            for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "\n",
    "                up_pixels = abs(lmj-center_dec_pix)#Should not be over the fov in the upwards direction (relative to 0 degrees)\n",
    "                side_pixels = abs(lmk-center_ra_pix)#Should not be over the fov in the side-side direction (relative to 0 degrees)\n",
    "\n",
    "                if(up_pixels<PixFov[0] and side_pixels<PixFov[1]):\n",
    "                    for lmi in range(np.shape(sc_for_cropping)[0]):\n",
    "                        pixels[lmi][lmj][lmk] = 1  # keep this pixel\n",
    "                        \n",
    "    # Mask teh pixels outside the fov\n",
    "\n",
    "    bp = np.where(pixels!=1)\n",
    "    scCopy = sc_for_cropping.hdu\n",
    "    scCopy.data[bp]=np.nan\n",
    "    scP = SpectralCube.read(scCopy)\n",
    "    del scCopy\n",
    "    del bp\n",
    "    #Get right size by removing the nan pixels\n",
    " \n",
    "    scP.allow_huge_operations=True\n",
    "    datn = scP.hdu.data\n",
    "    sx,sy,ex,ey=0,0,0,0\n",
    "    for lmi in range(np.shape(datn[0,:,:])[0]):\n",
    "\n",
    "        # Go through a slice of the cube and find the first pixels with rael data\n",
    "        \n",
    "        # After this is done these will be non zero so break the loop\n",
    "        if(ey!=0 and sx!=0 and ex!=0 and sy!=0):\n",
    "            \n",
    "            break\n",
    "            \n",
    "        for lmj in range(np.shape(datn[0,:,:])[1]):\n",
    "\n",
    "            if(sx==0):            \n",
    "                if(np.nanmean(datn[0,lmi,:])>0 or np.nanmean(datn[0,lmi,:])<0):\n",
    "                    sx=lmi\n",
    "\n",
    "            if(sy==0):\n",
    "                if(np.nanmean(datn[0,:,lmj])>0 or np.nanmean(datn[0,:,lmj])<0):\n",
    "                    sy=lmj\n",
    "\n",
    "            if(ex==0):\n",
    "                if(np.nanmean(datn[0,np.shape(datn[0,:,:])[0]-lmi-1,:])>0 or np.nanmean(datn[0,np.shape(datn[0,:,:])[0]-lmi-1,:])<0):\n",
    "                    ex=np.shape(datn[0,:,:])[0]-lmi-1\n",
    "\n",
    "            if(ey==0):\n",
    "                if(np.nanmean(datn[0,:,np.shape(datn[0,:,:])[1]-lmj-1])>0 or np.nanmean(datn[0,:,np.shape(datn[0,:,:])[1]-lmj-1])<0):\n",
    "                    ey=np.shape(datn[0,:,:])[1]-lmj-1\n",
    "\n",
    "            if(ey!=0 and ex!=0 and sx!=0 and sy!=0):\n",
    "                break\n",
    "\n",
    "    sc1 = scP[:,sx:ex,sy:ey]\n",
    "    scP_Hdu=sc1.hdu\n",
    "    \n",
    "    \n",
    "    # Also check if there are zeroes in place of nan values, which is done on some cubes\n",
    "    \n",
    "    zeros=((scP_Hdu.data[:,:,:]==0))\n",
    "    bp = np.where(zeros)\n",
    "    scP_Hdu.data[bp]=np.nan\n",
    "    scCopy = SpectralCube.read(scP_Hdu)\n",
    "    del scP_Hdu\n",
    "    \n",
    "    # Make the final cropped cube\n",
    "\n",
    "    cropped_sc = SpectralCube.read(scCopy.hdu)\n",
    "    \n",
    "    del scCopy\n",
    "    \n",
    "    data=sc_for_cropping.hdu.data\n",
    "    \n",
    "    \n",
    "    ######## find the original FOV\n",
    "    \n",
    "    fp_dec,lp_dec,fp_ra,lp_ra=0,0,0,0\n",
    "    for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "        for lmk in range(np.shape(sc_for_cropping)[2]):\n",
    "                if (data[0][lmj][lmk]>0 or data[0][lmj][lmk]<0):\n",
    "                    pass\n",
    "                else:\n",
    "                    fp_dec=lmj#the first upwards pixel with data\n",
    "                    break\n",
    "        if (fp_dec != 0):\n",
    "            break\n",
    "    for lmj in range(np.shape(sc_for_cropping)[1]):\n",
    "        for lmk in range(1,np.shape(sc_for_cropping)[2]):\n",
    "                if (data[0][lmj][np.shape(sc_for_cropping)[2]-lmk]>0 or data[0][lmj][np.shape(sc_for_cropping)[2]-lmk]<0):\n",
    "                    pass\n",
    "                else:\n",
    "                    lp_dec=np.shape(sc_for_cropping)[2]-lmk-1#the last upwards pixel with data\n",
    "                    break\n",
    "        if (lp_dec != 0):\n",
    "            break\n",
    "    for lmk in range(np.shape(sc_for_cropping)[1]):\n",
    "        for lmj in range(np.shape(sc_for_cropping)[2]):\n",
    "                if (data[0][lmj][lmk]>0 or data[0][lmj][lmk]<0):\n",
    "                    pass\n",
    "                else:\n",
    "                    fp_ra=lmj#the first sideways pixel with data\n",
    "                    break\n",
    "        if (fp_ra != 0):\n",
    "            break\n",
    "    for lmk in range(np.shape(sc_for_cropping)[1]):\n",
    "        for lmj in range(1,np.shape(sc_for_cropping)[2]):\n",
    "                if (data[0][np.shape(sc_for_cropping)[1]-lmj][lmk]>0 or data[0][np.shape(sc_for_cropping)[1]-lmj][lmk]<0):\n",
    "                    pass\n",
    "                else:\n",
    "                    lp_ra=np.shape(sc_for_cropping)[1]-lmj-1#the last sideways pixel with data\n",
    "                    break\n",
    "        if (lp_ra != 0):\n",
    "            break\n",
    "            \n",
    "            \n",
    "    if(r!=0*u.deg):\n",
    "        if(lp_ra-fp_ra>lp_dec-fp_dec):\n",
    "            disk_pixels=(lp_dec-fp_dec)/np.sin(r_rad)\n",
    "            jet_pixels=(lp_dec-fp_dec)/np.sin(np.pi/2 - r_rad)\n",
    "        else:\n",
    "            disk_pixels=(lp_ra-fp_ra)/np.sin(r_rad)\n",
    "            jet_pixels=(lp_ra-fp_ra)/np.sin(np.pi/2 - r_rad)\n",
    "    else:\n",
    "        disk_pixels=lp_ra-fp_ra\n",
    "        jet_pixels=lp_dec-fp_dec\n",
    "        \n",
    "    orig_fov=[0,0]            \n",
    "    orig_fov[0] = np.round(((disk_pixels*(cdelt_x.to(u.rad)*d))*2/u.rad).to(u.pc),1) #the fov across the disk\n",
    "    orig_fov[1] = np.round(((jet_pixels*(cdelt_x.to(u.rad)*d))*2/u.rad).to(u.pc),1) #the fov coming up from the disk\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"cropped cube from\",orig_fov,\"to:\",desired_fov)\n",
    "    \n",
    "    return cropped_sc,orig_fov,FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f9a16-f527-4178-8281-e811e8b6ebae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Splice the partwise cubes back together:\n",
    "    \n",
    "def Splice_vels(Cube_Name_Load):\n",
    "    \n",
    "    for i in range(2000):\n",
    "        try:\n",
    "            Cube_Name_Load_p = str(i)+\"_\"+Cube_Name_Load\n",
    "            Cube_Name_Save = Cube_Name_Load\n",
    "\n",
    "            sc=SpectralCube.read((\"Spectral Cubes/\"+Cube_Name_Load_p)) \n",
    "            print(\"Loaded\",Cube_Name_Load_p)\n",
    "            \n",
    "            \n",
    "            # Define a header that we will form into the new header for the spliced cube\n",
    "            if i == 0:\n",
    "                reheader = sc.header\n",
    "                rewcs = sc.wcs\n",
    "\n",
    "            if i == 0:\n",
    "                scW=SpectralCube.read((\"Spectral Cubes/\"+Cube_Name_Load_p))      \n",
    "                mask = scW.mask.include() # Need to create a mask because it doesn't get splcied\n",
    "                \n",
    "            else:\n",
    "                if i == 1:\n",
    "                    scW = np.concatenate((scW[:].hdu.data,sc[:].hdu.data),dtype = type(sc))\n",
    "                    mask = np.concatenate((mask[:],sc[:].mask.include()),dtype = type(sc[:].mask.include()))\n",
    "                else:\n",
    "                    scW = np.concatenate((scW[:],sc.hdu.data[:]),dtype = type(sc))\n",
    "                    mask = np.concatenate((mask[:],sc[:].mask.include()),dtype = type(sc[:].mask.include()))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "            \n",
    "    # This only matters for formatting reasons:\n",
    "    def duh(lol):\n",
    "        gp = np.where(lol!=np.nan)\n",
    "        lol[gp]=True\n",
    "        return lol # Anywhere that has data will be unmasked\n",
    "    \n",
    "    reheader[\"NAXIS3\"] = len(scW)\n",
    "    Full_Mask = LazyMask(function = duh,data = mask, wcs = rewcs)\n",
    "    \n",
    "    \n",
    "    scWsc = SpectralCube(data = scW,wcs = rewcs, header = reheader, mask = Full_Mask)# The spliced cube\n",
    "\n",
    "    scWsc.allow_huge_operations=True\n",
    "    scWsc = scWsc*sc[0][0][0].unit#Add unit back in\n",
    "    del sc\n",
    "\n",
    "\n",
    "    scWsc.write(\"Spectral Cubes/\"+Cube_Name_Save,overwrite=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f716a-64b9-429a-a2d6-a52e76813bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Velocity reprojection\n",
    "###\n",
    "        \n",
    "# Smooth the data (gaussian) to match the velocity resolution of other cubes\n",
    "\n",
    "def Repo_Velocity(scp,Cube_Name_Save,Cube_Information):\n",
    "    \n",
    "    print(\"Start velocity reprojection of\" ,Cube_Name_Save)\n",
    "    \n",
    "    vel_prime = Cube_Information[\"desired_velocity_resolution\"] \n",
    "    target_velocity = Cube_Information[\"target_velocity\"]\n",
    "    Initial_vel = u.Quantity(str(np.abs(scp.header['cdelt3']))+scp.header['cunit3']) # The current velocity resolution\n",
    "    restfreq = Pointing_Information['restfreq']\n",
    "    \n",
    "    # Gaussian width = sqrt( new_vel^2 -  old_vel^2)\n",
    "    \n",
    "    G_width = np.sqrt((vel_prime.to(u.km/u.s)).value**2-(Initial_vel.to(u.km/u.s)).value**2) \n",
    "    \n",
    "    vel = np.arange((target_velocity - 251*u.km/u.s).value, (target_velocity + 251*u.km/u.s).value,vel_prime.to(u.km/u.s).value)*u.km/u.s #make the new velocity axis\n",
    "    scWsc_copy = scp\n",
    "    \n",
    "    # the factor converting the gaussian width to its FWHM\n",
    "    fwhm_factor = np.sqrt(8*np.log(2))\n",
    "\n",
    "    scWsc_copy = scWsc_copy.with_spectral_unit(u.km / u.s, velocity_convention='optical', rest_value=restfreq) # Make sure it has the right rest frequency\n",
    "    \n",
    "    # The reprojected velocity must be larger than the initial velocity or else this gives an error\n",
    "    scWsc_copy = scWsc_copy.spectral_smooth(Gaussian1DKernel(G_width/fwhm_factor))#Preserves information from the pixels lost in downsampling\n",
    "    \n",
    "    print(\"Smoothed to Gaussian Kernel of width\",G_width/fwhm_factor)\n",
    "\n",
    "    scWsc_copy = scWsc_copy.spectral_interpolate(spectral_grid=vel) # Match velocities to -250 251 range  \n",
    "\n",
    "    \n",
    "    Cube_Name_Save_new = Cube_Name_Save[0:len(Cube_Name_Save)-6]+\"_\"+str(vel_prime.value)+'_vel_res_'+'.fits'\n",
    "    \n",
    "    \n",
    "    scWsc_copy.write(\"Spectral Cubes/\"+Cube_Name_Save_new,overwrite=True)\n",
    "    \n",
    "    print(\"Wrote reprojected cube to\",\"Spectral Cubes/\"+Cube_Name_Save_new)\n",
    "    \n",
    "    gc.collect()\n",
    "    del scWsc_copy\n",
    "\n",
    "    gc.collect()######################################################################\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dcb87f-fb6e-4179-b210-827905050697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after the reprojection, there may be duplicated data from the fact that it just fills in stuff \n",
    "# expecting new data to be there. This fixes it by checking to see if it matches other data, then removes\n",
    "# the repeated slices. in other words, it may be set to reproject to 0-500 km/s through the velocity channels\n",
    "# but there may only be data between 100-400, so it will fill it in with copied data channels. This will check for those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e25a7b-87f6-44af-856d-b69ed52de240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix reprojected repeated pixels\n",
    "\n",
    "# All these parameters only matter here because they are in the cube name\n",
    "# Line_Name = Line Name, Beam_Size=beam size, FOV=field of view, min_vel =  velocity resolution\n",
    "\n",
    "def Remove_Repeated_Pixels(Cube_Name_Load):\n",
    "    \n",
    "    Cube_Name_Save = \"Cropped_\"+Cube_Name_Load\n",
    "\n",
    "    scRRP = SpectralCube.read(\"Spectral Cubes/\"+Cube_Name_Load).with_spectral_unit(u.km/u.s,velocity_convention=\"radio\")\n",
    "    \n",
    "    \n",
    "    sp=0 #starting pixel with real data (tbd)\n",
    "    \n",
    "    for lmi in range(len(scRRP)):\n",
    "        #Check to see if the slice has been repeated by the interpolation function\n",
    "        if(np.round(np.nanmean(scRRP[lmi].hdu.data),9)==np.round(np.nanmean(scRRP[lmi+1].hdu.data),9)):\n",
    "\n",
    "            sp = lmi+1\n",
    "        else:\n",
    "            print(\"Good data starting from channel\",sp,\"; start has been cropped to that channel\")\n",
    "            break\n",
    "            \n",
    "    l = len(scRRP)-1\n",
    "    ep=l\n",
    "    for lmi in range(l):\n",
    "        #Cehck again, starting from the end this time\n",
    "        if(np.round(np.nanmean(scRRP[l-lmi].hdu.data),9)==np.round(np.nanmean(scRRP[l-lmi-1].hdu.data),9)):\n",
    "            ep = l-lmi-1\n",
    "\n",
    "        else:\n",
    "            print(\"Good data ending at channel\",ep,\"; end has been cropped to that channel\")\n",
    "            break\n",
    "\n",
    "    #sp,ep, These are the start and stop slices where the actual unique data resides\n",
    "\n",
    "    scRRP.allow_huge_operations=True\n",
    "    scRRP = scRRP[sp:ep]\n",
    "\n",
    "\n",
    "\n",
    "    scP_Hdu=scRRP.hdu\n",
    "    zeros=((scP_Hdu.data[:,:,:]==0))\n",
    "    bp = np.where(zeros)\n",
    "    scP_Hdu.data[bp]=np.nan\n",
    "    scRRP = SpectralCube.read(scP_Hdu)\n",
    "\n",
    "\n",
    "\n",
    "    scRRP = scRRP.to(u.K)\n",
    "\n",
    "    scRRP.write((\"Spectral Cubes/\"+Cube_Name_Save),overwrite=True)\n",
    "    del scRRP\n",
    "    del scP_Hdu\n",
    "\n",
    "    print(\"Cropped cube saved as \",\"Spectral Cubes/\"+Cube_Name_Save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef6616-75de-4086-a2d2-87bdd874b6ce",
   "metadata": {},
   "source": [
    "# PPV statistics\n",
    "\n",
    " This function uses the dendrogram as an input and calculates all the quantities\n",
    " such as size, linewidth, Column Density, RMS velocity (velocity dispersion, a measure \n",
    " of turbulence), Luminosity (using the distance and the flux), Distance to structure (only\n",
    " matters for the CMZ), the Moment0_Flux, and the V_RMS_error\n",
    " \n",
    " \n",
    "\n",
    "\n",
    " \n",
    " These are all the things calculated in the Shetty 2012 paper\n",
    "\n",
    " To exclude incomplete, unresolvable, or otherwise nan strucutres, we use some exclusion properties:\n",
    " 1. Don't allow any structure under 3 pc in size (the beam size)\n",
    " 2. Don't allow any structure\n",
    "\n",
    "\n",
    "\n",
    "Returns np.array(SizeA), np.array(SigmaA), np.array(CDA) ,np.array(LuminA) ,np.array(SIDS) , np.array(MOM0_FLUX) , np.array(Distances), np.array(V_rms_err) for each structure in the dendrogram and returns them in the form [[][]], where [Leaves][Branches] are the different kinds of structures in the dendrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a6dd73-8feb-4243-a036-56533fba9d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#Continuum is in Jansky/Beam, Line data should have the unit specified in the metadata as 'data_unit'\n",
    "\n",
    "#Dendrogram=the computed dendorgam, \n",
    "#Line Data= the actual np.array of the cube's data\n",
    "#\n",
    "#Pointing_Information = defined above\n",
    "\n",
    "#Max_Size=the maximum allowed size for a strcutre in u.pc\n",
    "            \n",
    "#beam_size=the size of the beam in u.pc\n",
    "#beam_req= 1/(how much you are willing to oversample the beam)\n",
    "#Trunks= True/False if you want/dont want to include the dendrogram trunks as branches\n",
    "\n",
    "# Note: The column density information is not yet fully bug tested, do not use it as gospel\n",
    "                        \n",
    "def Dendro_Arrays(Dendrogram,LineData,DataVel,ContData,Pointing_Information,beam_size=999,beam_req = 1/3,Trunks=True,max_size=18,edge_cases=False):\n",
    "    \n",
    "    #init all the return arrays:                    \n",
    "    SizeA,SigmaA,LuminA,CDA,SIDS,MOM0_FLUX,Distances,V_rms_err = [[],[],[],[]],[[],[],[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]\n",
    "            \n",
    "    metadata=Pointing_information    \n",
    "            \n",
    "    dist_val=metadata['distance'].to(u.pc).value  #convert to the value in pc\n",
    "\n",
    "    center = Pointing_Information[\"center\"]\n",
    "                        \n",
    "    d_copy= Dendrogram #copy the dendrogram\n",
    "\n",
    "    \n",
    "    center_ra_pix,center_dec_pix = int(metadata['wcsu'][:][:][0].world_to_pixel(center)[0]),int(metadata['wcsu'][:][:][0].world_to_pixel(center)[1])\n",
    "    \n",
    "                        \n",
    "    sliced= LineData[6]\n",
    "    CubeShape = np.shape(sliced) #the shape of a slice of the cube\n",
    "    DataShape=[[0,0],[0,0]]#We will find the part of the cube that actually has data\n",
    "\n",
    "    for lmi in range(CubeShape[0]):\n",
    "        allData=np.nansum(sliced[lmi])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[0][0] = lmi+3\n",
    "            break\n",
    "    for lmi in range(CubeShape[0]):\n",
    "        allData=np.nansum(sliced[CubeShape[0] - lmi -1])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[0][1] = CubeShape[0] - lmi -3\n",
    "            break\n",
    "    for lmi in range(CubeShape[1]):\n",
    "        allData=(sliced[DataShape[0][0],lmi])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[1][0] = lmi+3\n",
    "            break\n",
    "    for lmi in range(CubeShape[1]):\n",
    "        allData=(sliced[DataShape[0][0],CubeShape[1] - lmi -1])\n",
    "        if(allData>0 or allData<0):\n",
    "            DataShape[1][1] = CubeShape[1] - lmi -3\n",
    "            break\n",
    "    for t in Dendrogram.all_structures: \n",
    "\n",
    "        I = t.indices() #the pixel values in x, y of the structure\n",
    "        Cont = True\n",
    "        if t.is_branch:\n",
    "                if t.parent==None:\n",
    "                    \n",
    "                    if(Trunks):\n",
    "                        Cont = True\n",
    "                    else:\n",
    "                        Cont = False\n",
    "                else:\n",
    "                    Cont=True\n",
    "        \n",
    "        if edge_cases ==False:\n",
    "            #the milky way data is flat soo the calucation is easier\n",
    "            if gal ==\"GC\":\n",
    "            \n",
    "            \n",
    "                for lmi in range(len(I[0])):\n",
    "                        \n",
    "                    #cehck if there are any nan values nearby or any edges of the cube nearby\n",
    "                        \n",
    "                    if(I[1][lmi]<=DataShape[0][0] or I[1][lmi]>=DataShape[0][1] or I[2][lmi]<=DataShape[1][0] or I[2][lmi]>=DataShape[1][1]):\n",
    "                        Cont=False\n",
    "                        break\n",
    "                        \n",
    "                    elif(I[1][lmi]<=1 or I[1][lmi]>=CubeShape[0] or I[2][lmi]<=1 or I[2][lmi]>=CubeShape[1]):\n",
    "                        Cont=False\n",
    "                        break\n",
    "                        \n",
    "            else:          \n",
    "\n",
    "                for lmi in range(len(I[0])):\n",
    "                    NansNE=0\n",
    "                    NansSE=0\n",
    "                    NansNW=0\n",
    "                    NansSW=0\n",
    "                    Length = 5\n",
    "                    for lmj in range(Length):\n",
    "                        #Check four 45 degree prongs from each point and see if they have nans. If that happens its too close to the boundary or the data is bad\n",
    "                        try:\n",
    "\n",
    "                            if(sliced[I[1][lmi]+lmj,I[2][lmi]-lmj]>0 or sliced[I[1][lmi]+lmj,I[2][lmi]-lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansSE+=1\n",
    "                            if(sliced[I[1][lmi]-lmj,I[2][lmi]-lmj]>0 or sliced[I[1][lmi]-lmj,I[2][lmi]-lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansSW+=1\n",
    "                            if(sliced[I[1][lmi]-lmj,I[2][lmi]+lmj]>0 or sliced[I[1][lmi]-lmj,I[2][lmi]+lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansNW+=1\n",
    "                            if(sliced[I[1][lmi]+lmj,I[2][lmi]+lmj]>0 or sliced[I[1][lmi]+lmj,I[2][lmi]+lmj]<0 ):\n",
    "                                pass\n",
    "                            else:\n",
    "                                NansNE+=1\n",
    "                        except:\n",
    "                            #only fails if the I goes close to the boundary of the cube and tries to get a pixel outside the cube\n",
    "                            Cont = False\n",
    "                            break\n",
    "                    #count the number of nans nearby:\n",
    "                        \n",
    "                    if(NansNE>2 or NansNW>2 or NansSE>2 or NansSW>2):\n",
    "                        Cont=False\n",
    "                        break\n",
    "\n",
    "\n",
    "        if(Cont):\n",
    "            s = PPVStatistic(t,metadata=metadata) #calcuate the properties of the structure.this function is from the astrodendro library\n",
    "            s_radius = s.radius #Give the size in degrees\n",
    "            s_v_rms = s.v_rms #in u.km/u.s\n",
    "                        \n",
    "            Parsec_Size = (float(s_radius*np.pi/180*dist_val/u.deg)) #Convert to parsecs using the distance in Pc\n",
    "            #also check to make sure the size is greater than 1/3 the beam. any less and it will be too much noise\n",
    "            #and make sure the rms velocity is non-zero. otherwise, its not a 3d structure\n",
    "            if(Parsec_Size<max_size and Parsec_Size>beam_size*1/3 and s_v_rms>.01*u.km/u.s):\n",
    "            \n",
    "            \n",
    "\n",
    "                nproj_pix=len(set(zip(*tuple(I[i] for i in [1,2]))))\n",
    "                v_IWM = np.nansum(LineData[I]*(DataVel[I[0]])/u.km*u.s)/np.nansum(LineData[I])\n",
    "                sig_Sh = np.sqrt(np.nansum(LineData[I]*((DataVel[I[0]])/u.km*u.s-v_IWM)**2)/np.nansum(LineData[I])) \n",
    "                \n",
    "                #The flux from the continuum\n",
    "                #Convert to Jansky from Jansky per beam:\n",
    "                if(ColD ==True):\n",
    "                    Cont_Flux=0\n",
    "\n",
    "                    proj = tuple(set(zip(*tuple(I[i] for i in [1,2]))))\n",
    "                    for lmi in range(len(proj)):\n",
    "\n",
    "                        Cont_Flux+=ContData[proj[lmi]]\n",
    "                    Cont_Flux=Cont_Flux/(metadata['beam_area_ratioc']*(2*np.sqrt(2*np.log(2))))*u.pix**2*u.beam/u.beam*u.Jy#SHould be input as Jansky /beam and will be converted to Jansky, then to unitless. The beam is changed from FWHM to Gaussian\n",
    "                    Dust_Column = Flux_to_Mass(Cont_Flux)*Num_per_kg/((s_radius*np.pi/180*dist_cmz.value/u.deg)**2*(3.086*10**24)**2)/np.pi*(1.989*10**30*u.kg/u.M_sun)/u.kg\n",
    "                    \n",
    "                else:\n",
    "                    Dust_Column=0\n",
    "                if(str(Dust_Column) == str(np.nan) or str(Dust_Column)==str(np.inf)):\n",
    "                    Dust_Column=0\n",
    "                lum = Flux_to_Lum(s.flux)\n",
    "                s_flux = s.flux\n",
    "\n",
    "                Index = tuple(I[i] for i in [0,1,2])\n",
    "                K_Km_s_Flux=np.nansum(LineData[Index]*metadata[\"velocity_scale\"])#Find the total flux from the structures in K km/s, assuming the input data is in K as it should be, \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                Distance = np.sqrt((float(s.x_cen/u.pix)-center_ra_pix)**2+(float(s.y_cen/u.pix)- center_dec_pix)**2)*metadata['spatial_scale']*np.pi/180*dist_cmz.value*10**6/u.deg#pc dist from barycenter\n",
    "                \n",
    "                \n",
    "                V_err= Get_V_rms_err(dend1=d_copy,idx=int(t.idx),struct=t,m=m,NF=1,iterations=5,metadata=metadata)\n",
    "                \n",
    "                \n",
    "                if(t.is_leaf):\n",
    "\n",
    "                    SizeA[0].append((float((s_radius*np.pi/180*dist_val/10**6/u.deg)))) #define size as astrodendro\n",
    "                    SigmaA[0].append((float(s_v_rms/u.km*u.s)))#\n",
    "                    CDA[0].append(float(Dust_Column))\n",
    "                    LuminA[0].append(float(lum*u.Hz*u.s/u.erg))\n",
    "                    SIDS[0].append(float(t.idx))\n",
    "                    MOM0_FLUX[0].append(float(K_Km_s_Flux*u.s/u.km))\n",
    "                    Distances[0].append(float(Distance))\n",
    "                    V_rms_err[0].append(float(V_err))\n",
    "                if(t.is_branch\t):\n",
    "\n",
    "                    SizeA[1].append((float((s_radius*np.pi/180*dist_val/10**6/u.deg)))) #define size as astrodendro\n",
    "                    SigmaA[1].append((float(s_v_rms/u.km*u.s)))#\n",
    "                    CDA[1].append(float(Dust_Column))\n",
    "                    LuminA[1].append(float(lum*u.Hz*u.s/u.erg))\n",
    "                    SIDS[1].append(float(t.idx))\n",
    "                    MOM0_FLUX[1].append(float(K_Km_s_Flux*u.s/u.km))\n",
    "                    Distances[1].append(float(Distance))\n",
    "                    V_rms_err[1].append(float(V_err))\n",
    "                del s\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    SizeA[0] = np.array(SizeA[0],dtype=type(1.))\n",
    "    SizeA[1] = np.array(SizeA[1],dtype=type(1.))\n",
    "    SizeA[2] = np.array(SizeA[2],dtype=type(1.))\n",
    "    SizeA[3] = np.array(SizeA[3],dtype=type(1.))\n",
    "    SigmaA[0] = np.array(SigmaA[0],dtype=type(1.))\n",
    "    SigmaA[1] = np.array(SigmaA[1],dtype=type(1.))\n",
    "    SigmaA[2] = np.array(SigmaA[2],dtype=type(1.))\n",
    "    SigmaA[3] = np.array(SigmaA[3],dtype=type(1.))\n",
    "    CDA[0] = np.array(CDA[0],dtype=type(1.))\n",
    "    CDA[1] = np.array(CDA[1],dtype=type(1.))\n",
    "    LuminA[0] = np.array(LuminA[0],dtype=type(1.))\n",
    "    LuminA[1] = np.array(LuminA[1],dtype=type(1.))\n",
    "    SIDS[0] = np.array(SIDS[0],dtype=type(1.))\n",
    "    SIDS[1] = np.array(SIDS[1],dtype=type(1.))\n",
    "    MOM0_FLUX[0] = np.array(MOM0_FLUX[0],dtype=type(1.))\n",
    "    MOM0_FLUX[1] = np.array(MOM0_FLUX[1],dtype=type(1.))\n",
    "    Distances[0] = np.array(Distances[0],dtype=type(1.))\n",
    "    Distances[1] = np.array(Distances[1],dtype=type(1.))\n",
    "    V_rms_err[0] = np.array(V_rms_err[0],dtype=type(1.))\n",
    "    V_rms_err[1] = np.array(V_rms_err[1],dtype=type(1.))\n",
    "    \n",
    "    return np.array(SizeA),np.array(SigmaA),np.array(CDA),np.array(LuminA),np.array(SIDS),np.array(MOM0_FLUX),np.array(Distances),np.array(V_rms_err)\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "#For the ngc253 data we need another function\n",
    "\n",
    "def Dendro_Arrays_NGC(Dendrogram,LineData,DataVel,ContData,metadata,ColD = True,beam_size=999,beam_req = 999999,Edge_Cases=True,Trunks=True,max_size=1):\n",
    "    SizeA,SigmaA,LuminA,CDA,SIDS,MOM0_FLUX,Distances,V_rms_err = [[],[],[],[]],[[],[],[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]],[[],[]]\n",
    "    print(metadata)\n",
    "    \n",
    "    d_copy= Dendrogram\n",
    "    #catalog = astrodendro.ppv_catalog(d, metadata)\n",
    "    center = SkyCoord('00h47m33.14s' ,'-25d17m17.52s',frame='icrs')\n",
    "    center_ra_pix,center_dec_pix = int(metadata['wcsu'][:][:][0].world_to_pixel(center)[0]),int(metadata['wcsu'][:][:][0].world_to_pixel(center)[1])\n",
    "    sliced= LineData[6]\n",
    "    CubeShape = np.shape(sliced)\n",
    "    for t in Dendrogram.all_structures: \n",
    "\n",
    "        I = t.indices()\n",
    "        Cont = True\n",
    "        if t.is_branch:\n",
    "                if t.parent==None:\n",
    "                    if Trunks:\n",
    "                        Cont=True\n",
    "                    else:\n",
    "                        Cont=False\n",
    "                else:\n",
    "                    Cont = True\n",
    "        \n",
    "        for lmi in range(len(I[0])):\n",
    "            NansNE=0\n",
    "            NansSE=0\n",
    "            NansNW=0\n",
    "            NansSW=0\n",
    "            Length = 10\n",
    "            #I[1][lmi+10]>\n",
    "            if Edge_Cases==False:\n",
    "                for lmj in range(Length):\n",
    "                    #Check four 45 degree prongs from each point and see if they have at least 7 nans in 10 pixels. If that happens its too close to the boundary\n",
    "                    try:\n",
    "\n",
    "                        if(sliced[I[1][lmi]+lmj,I[2][lmi]-lmj]>0 or sliced[I[1][lmi]+lmj,I[2][lmi]-lmj]<0 ):\n",
    "                            pass\n",
    "                        else:\n",
    "                            NansSE+=1\n",
    "                        if(sliced[I[1][lmi]-lmj,I[2][lmi]-lmj]>0 or sliced[I[1][lmi]-lmj,I[2][lmi]-lmj]<0 ):\n",
    "                            pass\n",
    "                        else:\n",
    "                            NansSW+=1\n",
    "                        if(sliced[I[1][lmi]-lmj,I[2][lmi]+lmj]>0 or sliced[I[1][lmi]-lmj,I[2][lmi]+lmj]<0 ):\n",
    "                            pass\n",
    "                        else:\n",
    "                            NansNW+=1\n",
    "                        if(sliced[I[1][lmi]+lmj,I[2][lmi]+lmj]>0 or sliced[I[1][lmi]+lmj,I[2][lmi]+lmj]<0 ):\n",
    "                            pass\n",
    "                        else:\n",
    "                            NansNE+=1\n",
    "                    except:\n",
    "                        #only fails if the I goes close to the boundary of the cube and tries to get a pixel outside the cube\n",
    "                        Cont = False\n",
    "                if(NansNE>Length-3 or NansNW>Length-3 or NansSE>Length-3 or NansSW>Length-3):\n",
    "                    Cont = False\n",
    "\n",
    "                    break\n",
    "\n",
    "        if(Cont):\n",
    "            s = PPVStatistic(t,metadata=metadata)\n",
    "            s_radius = s.radius\n",
    "            s_v_rms = s.v_rms\n",
    "            #print(np.nanmax(I[0]) - np.nanmin(I[0]) )\n",
    "            #if((float((s_radius*np.pi/180*3.5/u.deg)))*10**6<max_size and (float((s_radius*np.pi/180*3.5/u.deg)))*10**6>beam_size*beam_req and (float(s_v_rms/u.km*u.s))>.01 and (np.nanmax(I[0]) - np.nanmin(I[0]))*(float((s_radius*np.pi/180*3.5/u.deg)))*10**6 >3*3):\n",
    "            if((float((s_radius*np.pi/180*3.5/u.deg)))*10**6<max_size and (float((s_radius*np.pi/180*3.5/u.deg)))*10**6>beam_size*beam_req and (float(s_v_rms/u.km*u.s))>.01 and (np.nanmax(I[0]) - np.nanmin(I[0]))>3):\n",
    "            #if((float((s_radius*np.pi/180*3.5/u.deg)))*10**6<max_size and (float((s_radius*np.pi/180*3.5/u.deg)))*10**6>beam_size*beam_req and (float(s_v_rms/u.km*u.s))>.01):\n",
    "            \n",
    "            \n",
    "\n",
    "                nproj_pix=len(set(zip(*tuple(I[i] for i in [1,2]))))\n",
    "                v_IWM = np.nansum(LineData[I]*(DataVel[I[0]])/u.km*u.s)/np.nansum(LineData[I])\n",
    "                sig_Sh = np.sqrt(np.nansum(LineData[I]*((DataVel[I[0]])/u.km*u.s-v_IWM)**2)/np.nansum(LineData[I])) \n",
    "                \n",
    "                #The flux from the continuum\n",
    "                #Convert to Jansky from Jansky per beam:\n",
    "                if(ColD ==True):\n",
    "                    Cont_Flux=0\n",
    "\n",
    "                    proj = tuple(set(zip(*tuple(I[i] for i in [1,2]))))\n",
    "                    for lmi in range(len(proj)):\n",
    "\n",
    "                        Cont_Flux+=ContData[proj[lmi]]\n",
    "                    Cont_Flux=Cont_Flux/(metadata['beam_area_ratioc']*(2*np.sqrt(2*np.log(2))))*u.pix**2*u.beam/u.beam*u.Jy#SHould be input as Jansky /beam and will be converted to Jansky, then to unitless. The beam is changed from FWHM to Gaussian\n",
    "                    Dust_Column = Flux_to_Mass(Cont_Flux)*Num_per_kg/((s_radius*np.pi/180*3.5/u.deg)**2*(3.086*10**24)**2)/np.pi*(1.989*10**30*u.kg/u.M_sun)/u.kg\n",
    "                else:\n",
    "                    Dust_Column=0\n",
    "                if(str(Dust_Column) == str(np.nan) or str(Dust_Column)==str(np.inf)):\n",
    "                    Dust_Column=0\n",
    "                lum = Flux_to_Lum(s.flux)\n",
    "                s_flux = s.flux\n",
    "\n",
    "                Index = tuple(I[i] for i in [0,1,2])\n",
    "                K_Km_s_Flux=np.nansum(LineData[Index]*metadata[\"velocity_scale\"])#Find the total flux from the structures in K km/s, assuming the input data is in K as it should be, \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                Distance = np.sqrt((float(s.x_cen/u.pix)-center_ra_pix)**2+(float(s.y_cen/u.pix)- center_dec_pix)**2)*metadata['spatial_scale']*np.pi/180*3.5*10**6/u.deg#pc dist from barycenter\n",
    "                \n",
    "                \n",
    "                V_err= 0#Get_V_rms_err(dend1=d_copy,idx=int(t.idx),struct=t,m=m,NF=1,iterations=5,metadata=metadata)\n",
    "                \n",
    "                \n",
    "                if(t.is_leaf):\n",
    "\n",
    "                    SizeA[0].append((float((s_radius*np.pi/180*3.5/u.deg)))) #define size as astrodendro\n",
    "                    SigmaA[0].append((float(s_v_rms/u.km*u.s)))#\n",
    "                    CDA[0].append(float(Dust_Column))\n",
    "                    LuminA[0].append(float(lum*u.Hz*u.s/u.erg))\n",
    "                    SIDS[0].append(float(t.idx))\n",
    "                    MOM0_FLUX[0].append(float(K_Km_s_Flux*u.s/u.km))\n",
    "                    Distances[0].append(float(Distance))\n",
    "                    V_rms_err[0].append(float(V_err))\n",
    "                if(t.is_branch\t):\n",
    "\n",
    "                    SizeA[1].append((float((s_radius*np.pi/180*3.5/u.deg)))) #define size as astrodendro\n",
    "                    SigmaA[1].append((float(s_v_rms/u.km*u.s)))#\n",
    "                    CDA[1].append(float(Dust_Column))\n",
    "                    LuminA[1].append(float(lum*u.Hz*u.s/u.erg))\n",
    "                    SIDS[1].append(float(t.idx))\n",
    "                    MOM0_FLUX[1].append(float(K_Km_s_Flux*u.s/u.km))\n",
    "                    Distances[1].append(float(Distance))\n",
    "                    V_rms_err[1].append(float(V_err))\n",
    "                del s\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    SizeA[0] = np.array(SizeA[0],dtype=type(1.))\n",
    "    SizeA[1] = np.array(SizeA[1],dtype=type(1.))\n",
    "    SizeA[2] = np.array(SizeA[2],dtype=type(1.))\n",
    "    SizeA[3] = np.array(SizeA[3],dtype=type(1.))\n",
    "    SigmaA[0] = np.array(SigmaA[0],dtype=type(1.))\n",
    "    SigmaA[1] = np.array(SigmaA[1],dtype=type(1.))\n",
    "    SigmaA[2] = np.array(SigmaA[2],dtype=type(1.))\n",
    "    SigmaA[3] = np.array(SigmaA[3],dtype=type(1.))\n",
    "    CDA[0] = np.array(CDA[0],dtype=type(1.))\n",
    "    CDA[1] = np.array(CDA[1],dtype=type(1.))\n",
    "    LuminA[0] = np.array(LuminA[0],dtype=type(1.))\n",
    "    LuminA[1] = np.array(LuminA[1],dtype=type(1.))\n",
    "    SIDS[0] = np.array(SIDS[0],dtype=type(1.))\n",
    "    SIDS[1] = np.array(SIDS[1],dtype=type(1.))\n",
    "    MOM0_FLUX[0] = np.array(MOM0_FLUX[0],dtype=type(1.))\n",
    "    MOM0_FLUX[1] = np.array(MOM0_FLUX[1],dtype=type(1.))\n",
    "    Distances[0] = np.array(Distances[0],dtype=type(1.))\n",
    "    Distances[1] = np.array(Distances[1],dtype=type(1.))\n",
    "    V_rms_err[0] = np.array(V_rms_err[0],dtype=type(1.))\n",
    "    V_rms_err[1] = np.array(V_rms_err[1],dtype=type(1.))\n",
    "    \n",
    "    return np.array(SizeA),np.array(SigmaA),np.array(CDA),np.array(LuminA),np.array(SIDS),np.array(MOM0_FLUX),np.array(Distances),np.array(V_rms_err)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5229df-e6b6-4e5a-90e8-32c722d4049e",
   "metadata": {},
   "source": [
    "# Cluster finding\n",
    "\n",
    "Currently unused, but this can find the star forming clusters as detected by Levy 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ee4b1a-ce6c-4375-a4b6-d045062fa5e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "def Read_Clusters(FileName):\n",
    "    \n",
    "    sh= len(np.genfromtxt(FileName,usecols=0))\n",
    "    Data=[]\n",
    "    for lmi in range(50):\n",
    "        try:\n",
    "            Data.append(np.genfromtxt(FileName,usecols=lmi,dtype=type(\"2d4m\")))\n",
    "            #print(np.genfromtxt(FileName,usecols=lmi,dtype=type(\"2d4m\"),skip_header=1))\n",
    "        except:\n",
    "            pass\n",
    "    return Data\n",
    "def Find_Clusters_NGC(Data):\n",
    "    for lmi in range(len(Data)):\n",
    "        if \"ID\" in Data[lmi]:\n",
    "            IDs= Data[lmi][1:9999]\n",
    "        if \"RA\" in Data[lmi]: \n",
    "            RAs= Data[lmi][1:9999]\n",
    "        if \"Dec\" in Data[lmi]:\n",
    "            Decs= Data[lmi][1:9999]\n",
    "        if \"r_deconv\" in Data[lmi]: \n",
    "            R_deconv= Data[lmi][1:9999]#pc\n",
    "        if \"glon\" in Data[lmi]: \n",
    "            glons= Data[lmi][1:9999]#\n",
    "        if \"glat\" in Data[lmi]: \n",
    "            glats= Data[lmi][1:9999]#\n",
    "            \n",
    "    return IDs,RAs,Decs,R_deconv\n",
    "#Take the cont in Jy and find the HWHM from the structures in the catalog\n",
    "def Find_Clusters(Data,wcs,Cont_Data,header):\n",
    "    for lmi in range(len(Data)):\n",
    "        if \"ID\" in Data[lmi]:\n",
    "            IDs= Data[lmi][1:9999]\n",
    "        if \"RA\" in Data[lmi]: \n",
    "            RAs= Data[lmi][1:9999]\n",
    "        if \"Dec\" in Data[lmi]:\n",
    "            Decs= Data[lmi][1:9999]\n",
    "        if \"r_deconv\" in Data[lmi]: \n",
    "            R_deconv= Data[lmi][1:9999]#pc\n",
    "        if \"glon\" in Data[lmi]: \n",
    "            glons= Data[lmi][1:9999]#\n",
    "        if \"glat\" in Data[lmi]: \n",
    "            glats= Data[lmi][1:9999]#\n",
    "        if \"herschel_column\" in Data[lmi]: \n",
    "            CD= (Data[lmi][1:9999])#pc\n",
    "            \n",
    "        if \"flux_integrated\" in Data[lmi]: \n",
    "            Flux_1p3mm= Data[lmi][1:9999]#pc\n",
    "    #remove nan \n",
    "    for lmii in range(len(CD)):\n",
    "        try:\n",
    "            if CD[lmii]=='np.nan':\n",
    "                CD= np.delete(CD, lmii)\n",
    "                Flux_1p3mm= np.delete(Flux_1p3mm, lmii)\n",
    "                IDs= np.delete(IDs, lmii)\n",
    "                glats= np.delete(glats, lmii)\n",
    "                glons= np.delete(glons, lmii)\n",
    "                \n",
    "        except:\n",
    "            CD = np.array(CD,dtype=type(1.2**5))#float\n",
    "            break\n",
    "    glats_New=[]\n",
    "    glons_New=[]\n",
    "    CDs_New=[]\n",
    "    IDs_New=[]\n",
    "    Flux_1p3mm_New=[]\n",
    "\n",
    "    #print(CD,sorted(CD),type(CD),type(CD[0]))\n",
    "    nth = sorted(CD)[len(CD)-34]#34 most dense leaves\n",
    "    #print(nth,\"A\",CD,sorted(CD))\n",
    "    for lmj in range(len(CD)):\n",
    "        if CD[lmj]>nth:\n",
    "            glats_New.append(glats[lmj])\n",
    "            glons_New.append(glons[lmj])\n",
    "            CDs_New.append(CD[lmj])\n",
    "            IDs_New.append(int(IDs[lmj]))\n",
    "            Flux_1p3mm_New.append(Flux_1p3mm[lmj])\n",
    "    HWHM_rad = []      \n",
    "    #print(Flux_1p3mm_New,glats_New,glons_New,CDs_New,IDs_New)\n",
    "    for lmi in range(len(CDs_New)):\n",
    "        glat = glats_New[lmi]\n",
    "        glon = glons_New[lmi]\n",
    "        Flux = float(Flux_1p3mm_New[lmi])#INtegerated flux in jy\n",
    "        \n",
    "        Circle_R = 0\n",
    "        distance = 8.178*10**-3*u.Mpc\n",
    "        \n",
    "        pixel_res = abs(header['cdelt1'])*np.pi/180*distance*10**6/u.Mpc*u.pc # cdelt in deg, goes to res in pc\n",
    "        \n",
    "        #sky = SkyCoord('00h47m33.9s', '-25d17m26.8s', frame='icrs')\n",
    "        sky = SkyCoord(l=float(glon)*u.deg, b=float(glat)*u.deg, frame='galactic')\n",
    "        #center = SkyCoord(l=359.94487501*u.degree,b=-00.04391769*u.degree, frame='galactic')\n",
    "        p1,p2 = int(wcs.world_to_pixel(sky)[0]),int(wcs.world_to_pixel(sky)[1]) #Ra,dec\n",
    "        \n",
    "        while(True):\n",
    "            Circle_R += .01\n",
    "            #pixels=[(p1,p2)]\n",
    "            pixels=[(p2,p1)]#Goes lat then long for the cont data\n",
    "            #print(p1,p2)\n",
    "            #print(np.shape(Cont_Data[p2-50:p2+50]))\n",
    "            #print(np.shape(Cont_Data[50,p1-50:p1+50]))\n",
    "            for lmii in range(np.shape(Cont_Data[p2-50:p2+50])[0]):\n",
    "                for lmjj in range(np.shape(Cont_Data[p2-50+lmii,p1-50:p1+50])[0]):\n",
    "                    #Find pixels within the circle around the center (excude the center since its there already)\n",
    "                    #print(np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res,lmjj)\n",
    "                    if np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res.value < Circle_R and lmjj!=50:\n",
    "                        pixels.append((lmjj-50+p2,lmii-50+p1))#Goes lat then long\n",
    "                        \n",
    "            \n",
    "            \n",
    "            sum_flux=0\n",
    "            for lmkk in range(len(pixels)):\n",
    "                sum_flux += (Cont_Data[pixels[lmkk]])\n",
    "            #print(p1,p2,glat,glon,np.shape(Cont_Data),pixels,Cont_Data[pixels[0]],Flux,sum_flux,Circle_R)\n",
    "            if sum_flux>Flux/2:\n",
    "                HWHM_rad.append(Circle_R)#Pc\n",
    "                break\n",
    "                \n",
    "    return HWHM_rad,CDs_New,glons_New,glats_New,IDs_New\n",
    "\n",
    "#Return masked data around clusters or one pc around clusters\n",
    "def Mask_Clusters_NGC(HWHM,wcs,header,unmasked_data,ras,decs,One_Pc=False,One_Pc_Size=1,HWHM_Fac=1):\n",
    "    \n",
    "    Masked_Data=copy.deepcopy(unmasked_data)\n",
    "    for lmi in range(len(HWHM)):\n",
    "        ra = ras[lmi]\n",
    "        dec = decs[lmi]\n",
    "                \n",
    "        Circle_R = HWHM[lmi]*HWHM_Fac\n",
    "        if(One_Pc):\n",
    "            \n",
    "            Circle_R=One_Pc_Size\n",
    "        distance = 3.5*u.Mpc\n",
    "        \n",
    "        pixel_res = abs(header['cdelt1'])*np.pi/180*distance*10**6/u.Mpc*u.pc # cdelt in deg, goes to res in pc\n",
    "        \n",
    "        #sky = SkyCoord('00h47m33.9s', '-25d17m26.8s', frame='icrs')\n",
    "        sky = SkyCoord(str(ra),str(dec), frame='icrs')\n",
    "        #center = SkyCoord(l=359.94487501*u.degree,b=-00.04391769*u.degree, frame='galactic')\n",
    "        p1,p2 = int(wcs.world_to_pixel(sky)[0]),int(wcs.world_to_pixel(sky)[1]) #Ra,dec\n",
    "        \n",
    "\n",
    "\n",
    "        #pixels=[(p1,p2)]\n",
    "        pixels=[(p2,p1)]#Goes lat then long for the cont data\n",
    "        #print(p1,p2)\n",
    "        #print(np.shape(Cont_Data[p2-50:p2+50]))\n",
    "        #print(np.shape(Cont_Data[50,p1-50:p1+50]))\n",
    "        for lmii in range(np.shape(unmasked_data[0,p2-50:p2+50])[0]):\n",
    "            for lmjj in range(np.shape(unmasked_data[0,p2-50+lmii,p1-50:p1+50])[0]):\n",
    "                #Find pixels within the circle around the center (excude the center since its there already)\n",
    "                #print(np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res,lmjj)\n",
    "                \n",
    "                if np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res.value < Circle_R and lmjj!=50:\n",
    "                    pixels.append((lmjj-50+p2,lmii-50+p1))#Goes lat then long\n",
    "        \n",
    "        for lmi in range(len(unmasked_data)):\n",
    "            \n",
    "            for lmj in range(len(pixels)):\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "                Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]]=np.nan\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "     \n",
    "    return Masked_Data\n",
    "            \n",
    "#Make_Plot(\"Tes\",\"Test2\",Q.moment0().hdu.data,0,0,Q.wcs[:][:][0],2,2,1,True)\n",
    "#Make_Plot(\"Tes\",\"Test2\",Q.moment0().hdu.data,0,0,Q.wcs[:][:][0],2,2,2,True)\n",
    "\n",
    "\n",
    "def Mask_Clusters_CMZ(HWHM,wcs,header,unmasked_data,glons,glats,One_Pc=False,One_Pc_Size=1,HWHM_Fac=1):\n",
    "    \n",
    "    Masked_Data=copy.deepcopy(unmasked_data)\n",
    "    for lmi in range(len(HWHM)):\n",
    "        glon = glons[lmi]\n",
    "        glat = glats[lmi]\n",
    "                \n",
    "        Circle_R = HWHM[lmi]*HWHM_Fac\n",
    "        if(One_Pc):\n",
    "            \n",
    "            Circle_R=One_Pc_Size\n",
    "        distance = dist_cmz\n",
    "        \n",
    "        pixel_res = abs(header['cdelt1'])*np.pi/180*distance*10**6/u.Mpc*u.pc # cdelt in deg, goes to res in pc\n",
    "        \n",
    "        #sky = SkyCoord('00h47m33.9s', '-25d17m26.8s', frame='icrs')\n",
    "        sky = SkyCoord(float(glon)*u.deg,float(glat)*u.deg, frame='galactic')\n",
    "        #center = SkyCoord(l=359.94487501*u.degree,b=-00.04391769*u.degree, frame='galactic')\n",
    "        p1,p2 = int(wcs.world_to_pixel(sky)[0]),int(wcs.world_to_pixel(sky)[1]) #Ra,dec\n",
    "        \n",
    "\n",
    "\n",
    "        #pixels=[(p1,p2)]\n",
    "        pixels=[(p2,p1)]#Goes lat then long for the cont data\n",
    "        #print(p1,p2)\n",
    "        #print(np.shape(Cont_Data[p2-50:p2+50]))\n",
    "        #print(np.shape(Cont_Data[50,p1-50:p1+50]))\n",
    "        for lmii in range(np.shape(unmasked_data[0,p2-50:p2+50])[0]):\n",
    "            for lmjj in range(np.shape(unmasked_data[0,p2-50+lmii,p1-50:p1+50])[0]):\n",
    "                #Find pixels within the circle around the center (excude the center since its there already)\n",
    "                #print(np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res,lmjj)\n",
    "                \n",
    "                if np.sqrt((lmii-50)**2+(lmjj-50)**2)*pixel_res.value < Circle_R and lmjj!=50:\n",
    "                    pixels.append((lmjj-50+p2,lmii-50+p1))#Goes lat then long\n",
    "        \n",
    "        for lmi in range(len(unmasked_data)):\n",
    "            \n",
    "            for lmj in range(len(pixels)):\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "                Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]]=np.nan\n",
    "                #print(Masked_Data[lmi,pixels[lmj][0],pixels[lmj][1]],lmi,pixels,np.shape(Masked_Data))\n",
    "     \n",
    "    return Masked_Data\n",
    "            \n",
    "#Make_Plot(\"Tes\",\"Test2\",Q.moment0().hdu.data,0,0,Q.wcs[:][:][0],2,2,1,True)\n",
    "#Make_Plot(\"Tes\",\"Test2\",Q.moment0().hdu.data,0,0,Q.wcs[:][:][0],2,2,2,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668f26f-6e69-4392-8d5a-eea5398b24e3",
   "metadata": {},
   "source": [
    "# Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81159ff0-823a-4650-9cb8-cbdd6947614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Make an image from the SC data\n",
    "\n",
    "#Data=the image data to plot\n",
    "#vmin,vmax=the min/max scale for the graph's color bar\n",
    "#Name2, another descriptor\n",
    "\n",
    "def Make_Plot(Pointing_Information,Name2,Data,vmin,vmax,rows=1,columns=1,index=1,show=False):\n",
    "    \n",
    "    Name=Pointing_Information['target']\n",
    "    WCS=Pointing_Information['wcsu']\n",
    "    Glon = str(WCS).find(\"GLON\")!=-1 #If the wcs is in Glon or not\n",
    "    \n",
    "    # Build a plot\n",
    "    ax = pylab.subplot(rows,columns,index,projection=WCS) \n",
    "    RA = ax.coords[0]                                                                  # \n",
    "    Dec = ax.coords[1]\n",
    "    im = pylab.imshow(Data,vmin=vmin,vmax=vmax,cmap='rainbow')\n",
    "    RA.set_ticks(size=-3)                                                                                      \n",
    "    Dec.set_ticks(size=-3) \n",
    "    RA.set_ticklabel(exclude_overlapping=True) \n",
    "    Dec.set_ticklabel(exclude_overlapping=True)                                                                                     \n",
    "    \n",
    "    if(Glon==False):\n",
    "        pylab.xlabel('Right Ascension',fontsize=20,labelpad=1)                               \n",
    "        pylab.ylabel('Declination',fontsize=20,labelpad=1)\n",
    "    else:\n",
    "        pylab.xlabel('Galatic longitude',fontsize=20,labelpad=1)                               \n",
    "        pylab.ylabel('Galatic latitude',fontsize=20,labelpad=1)\n",
    "    ax.tick_params(axis = 'both', which = 'major', labelsize = 15)    \n",
    "    cb=pylab.colorbar(im,fraction=0.1,pad=0.0)                                     \n",
    "    cb.set_label(label=Name,fontsize=10,rotation=270,labelpad=20) \n",
    "    cb.ax.tick_params(which = 'major', labelsize = 10)   \n",
    "    pylab.annotate(text=Name2,fontsize=10,xy=(0.02,1.05),xycoords=\"axes fraction\")  \n",
    "    \n",
    "    if(show==True):\n",
    "        pylab.show()\n",
    "        \n",
    "        \n",
    "#Put this up here for the column density map\n",
    "# Make sure the input flux is in Jy\n",
    "# And make sure the input distance is in Mpc\n",
    "\n",
    "# This is a mass conversion factor for CO 3-2 to H2 calibrated using the 850 Ghz dust continuum\n",
    "# It will not be used widly, but if it was I would need a relation factor for each molecular based \n",
    "# on their relative abundance, and their line transition ratio.\n",
    "# This has also been divided by two to account for the higher metallicity of NGC253 as compared to the CMZ\n",
    "# as Krieger did in 2020\n",
    "\n",
    "a_850 = 6.7*10**19*u.erg/u.s/u.Hz/u.M_sun #6.7+-1.7, from Bolatto 2013a\n",
    "\n",
    "\n",
    "def Flux_to_Mass(flux, dist, Lum_per_mass_factor=a_850):\n",
    "    \n",
    "    #Here is the manual process to convert to ergs\n",
    "    #J_to_e = 10**-23*u.erg/u.s/u.cm**2/u.Hz/u.Jy #Jansky to flux in erg/(s cm^2 Hz)\n",
    "    #flux_erg = flux*J_to_e\n",
    "    flux_erg = flux.to(u.erg)\n",
    "    #here is the conversion factor for Mpc to cm\n",
    "    #Mpc_to_cm = 3.086*10**24 * u.cm/u.Mpc\n",
    "    dist_cm=dist.to(u.cm)\n",
    "    \n",
    "    # Now use the distance and flux to calculate the luminosity, then use that and the conversion factor to find the mass\n",
    "    \n",
    "    # L = 4pi*r2 * Flux\n",
    "    \n",
    "    L = 4*np.pi*(dist_cm)**2*flux_erg #Megaparsec is converted to cm\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    M_mol = L/Lum_per_mass_factor #Just in Solar mass*1.989*10**30*u.kg/u.M_sun #Determines mass of the cont for 850 in kg\n",
    "    \n",
    "    return M_mol\n",
    "\n",
    "        \n",
    "# This assumes the input flux will be in Jansky, which the astrodendro library defaults to.\n",
    "# And make sure the input distance is in Mpc\n",
    "\n",
    "def Flux_to_Lum(flux, dist):\n",
    "    \n",
    "    #Here is the manual process to convert to ergs\n",
    "    #J_to_e = 10**-23*u.erg/u.s/u.cm**2/u.Hz/u.Jy #Jansky to flux in erg/(s cm^2 Hz)\n",
    "    #flux_erg = flux*J_to_e\n",
    "    flux_erg = flux.to(u.erg)\n",
    "    #here is the conversion factor for Mpc to cm\n",
    "    #Mpc_to_cm = 3.086*10**24 * u.cm/u.Mpc\n",
    "    dist_cm=dist.to(u.cm)\n",
    "    \n",
    "    # Now use the distance and flux to calculate the luminosity, then use that and the conversion factor to find the mass\n",
    "    \n",
    "    # L = 4pi*r2 * Flux\n",
    "    \n",
    "    L = 4*np.pi*(dist_cm)**2*flux_erg #Return the luminosity in Erg\n",
    "\n",
    "    \n",
    "    return L\n",
    "\n",
    "\n",
    "def Get_V_rms_err(dend1,struct,idx,m,NF,iterations,metadata):\n",
    "    \n",
    "    \n",
    "    vs=[]\n",
    "    np.random.seed((99)**2*123)\n",
    "    for llll in range(iterations):\n",
    "        \n",
    "        #print(llll)\n",
    "        s = dend1.__getitem__(idx)\n",
    "        #s = struct#copy.deepcopy(struct)\n",
    "        #s2 = struct#copy.deepcopy(struct)\n",
    "        npixels = np.product(np.shape(s.values()))\n",
    "        #print(np.shape(s.values()),s.values())\n",
    "        \n",
    "        additional_noise = np.random.normal(0., m*NF, npixels)\n",
    "        additional_noise = np.reshape(additional_noise, np.shape(s.values()))\n",
    "        #add or subract noise to the values and calculate the v rms, them find the std of that array and\n",
    "        # call that the uncertainty in v rms for a structure\n",
    "        dat1P = dend1.data[s.indices()]\n",
    "        dend1.data[s.indices()]+= additional_noise\n",
    "        s = dend1.__getitem__(idx)\n",
    "        vs.append(float(PPVStatistic(s,metadata=metadata).v_rms/u.km*u.s))\n",
    "        dend1.data[s.indices()]= dat1P#reset the dend data\n",
    "        \n",
    "        dend1.data[s.indices()]-= additional_noise\n",
    "        #s._values+=additional_noise\n",
    "        #print(s.values(),s._values)\n",
    "        \n",
    "        #s2._values-=additional_noise\n",
    "        s = dend1.__getitem__(idx)\n",
    "        #print(dat1P[0],s._values[0],\"kaasl\")\n",
    "        vs.append(float(PPVStatistic(s,metadata=metadata).v_rms/u.km*u.s))\n",
    "        \n",
    "        del s\n",
    "        #del s2\n",
    "        \n",
    "    v_rms_std = np.nanstd(vs)\n",
    "    #print(v_rms_std)\n",
    "    return v_rms_std\n",
    "\n",
    "# Return a cropped cube for some ra and dec, also crops the velocity axis if needed (0 for no crop)\n",
    "# cube= the spetral cube you wish to crop\n",
    "# WCS = that cube's world coordinate system (.WCS)\n",
    "# Np1, Np2 = the first and final pixel that don't have nan values (the left and right bounds of the cube)\n",
    "# BadVel = put a number based on the amound of velocity channels that are just noise, or leave at zero if you want to keep the noise\n",
    "# D2 = put True if the cube is 2D, otherwise, put False\n",
    "\n",
    "def Crop(cube,WCS,Np1,Np2,BadVel,D2):\n",
    "    NraDP1 = [int(WCS.world_to_pixel(Np1)[0]),int(WCS.world_to_pixel(Np1)[1])]\n",
    "    NraDP2 = [int(WCS.world_to_pixel(Np2)[0]),int(WCS.world_to_pixel(Np2)[1])]\n",
    "    if(D2==False):\n",
    "        return cube[BadVel:np.shape(cube)[0]-BadVel,NraDP1[1]:NraDP2[1],NraDP1[0]:NraDP2[0]]\n",
    "    if(D2==True):\n",
    "        return cube[NraDP1[1]:NraDP2[1],NraDP1[0]:NraDP2[0]]\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "def Crop_Nans(data):\n",
    "\n",
    "    sx,sy,ex,ey=0,0,0,0\n",
    "    for lmi in range(np.shape(data[0,:,:])[0]):\n",
    "\n",
    "        if(ey!=0 and sx!=0 and ex!=0 and sy!=0):\n",
    "            print(\"F\",lmi)\n",
    "            break\n",
    "        for lmj in range(np.shape(data[0,:,:])[1]):\n",
    "\n",
    "            if(sx==0):            \n",
    "                if(np.nanmean(data[0,lmi,:])>0 or np.nanmean(data[0,lmi,:])<0):\n",
    "                    sx=lmi\n",
    "\n",
    "\n",
    "            if(sy==0):\n",
    "                if(np.nanmean(data[0,:,lmj])>0 or np.nanmean(data[0,:,lmj])<0):\n",
    "                    sy=lmj\n",
    "\n",
    "            if(ex==0):\n",
    "                if(np.nanmean(data[0,np.shape(datn[0,:,:])[0]-lmi-1,:])>0 or np.nanmean(data[0,np.shape(data[0,:,:])[0]-lmi-1,:])<0):\n",
    "                    ex=np.shape(data[0,:,:])[0]-lmi-1\n",
    "\n",
    "            if(ey==0):\n",
    "                if(np.nanmean(data[0,:,np.shape(data[0,:,:])[1]-lmj-1])>0 or np.nanmean(data[0,:,np.shape(data[0,:,:])[1]-lmj-1])<0):\n",
    "                    ey=np.shape(data[0,:,:])[1]-lmj-1\n",
    "\n",
    "            if(ey!=0 and ex!=0 and sx!=0 and sy!=0):\n",
    "                break\n",
    "                \n",
    "    print(sx,ex,sy,ey)\n",
    "    return sx,ex,sy,ey\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b99684d-0c92-4211-a824-cd052c236fb8",
   "metadata": {},
   "source": [
    "# More useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952c8d6d-991e-4425-90e0-62067f41a42e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def gaussian_beam(f, beam_gauss_width):\n",
    "    '''\n",
    "    Fourier transform of a Gaussian beam. NOT the power spectrum (multiply exp\n",
    "    argument by 2 for power spectrum).\n",
    "    Parameters\n",
    "    ----------\n",
    "    f : np.ndarray\n",
    "        Frequencies to evaluate beam at.\n",
    "    beam_gauss_width : float\n",
    "        Beam size. Should be the Gaussian rms, not FWHM.\n",
    "    '''\n",
    "    return np.exp(-f**2 * np.pi**2 * 2 * beam_gauss_width**2)\n",
    "\n",
    "def gauss_correlated_noise_2D(shape, sigma, beam_gauss_width,\n",
    "                              randomseed=327485749):\n",
    "    \n",
    "    '''\n",
    "    Generate correlated Gaussian noise with sigma, smoothed by a\n",
    "    Gaussian kernel.\n",
    "    '''\n",
    "\n",
    "    # Making a real signal. Only need real part of FFT\n",
    "    freqs_yy, freqs_xx = np.meshgrid(np.fft.fftfreq(shape[0]),\n",
    "                                     np.fft.rfftfreq(shape[1]), indexing=\"ij\")\n",
    "\n",
    "    freqs = np.sqrt(freqs_yy**2 + freqs_xx**2)\n",
    "    # freqs[freqs == 0.] = np.NaN\n",
    "    # freqs[freqs == 0.] = 1.\n",
    "\n",
    "    imsize = shape[0]\n",
    "\n",
    "    Np1 = (imsize - 1) // 2 if imsize % 2 != 0 else imsize // 2\n",
    "    \n",
    "    with NumpyRNGContext(randomseed):\n",
    "\n",
    "        angles = np.random.uniform(0, 2 * np.pi,\n",
    "                                   size=freqs.shape)\n",
    "\n",
    "    noise = np.cos(angles) + 1j * np.sin(angles)\n",
    "\n",
    "    if imsize % 2 == 0:\n",
    "        noise[1:Np1, 0] = np.conj(noise[imsize:Np1:-1, 0])\n",
    "        noise[1:Np1, -1] = np.conj(noise[imsize:Np1:-1, -1])\n",
    "        noise[Np1, 0] = noise[Np1, 0].real + 1j * 0.0\n",
    "        noise[Np1, -1] = noise[Np1, -1].real + 1j * 0.0\n",
    "\n",
    "    else:\n",
    "        noise[1:Np1 + 1, 0] = np.conj(noise[imsize:Np1:-1, 0])\n",
    "        noise[1:Np1 + 1, -1] = np.conj(noise[imsize:Np1:-1, -1])\n",
    "\n",
    "    # Zero freq components must have no imaginary part to be own conjugate\n",
    "    noise[0, -1] = noise[0, -1].real + 1j * 0.0\n",
    "    noise[0, 0] = noise[0, 0].real + 1j * 0.0\n",
    "\n",
    "    corr_field = np.fft.irfft2(noise *\n",
    "                               gaussian_beam(freqs, beam_gauss_width))\n",
    "\n",
    "    norm = (np.sqrt(np.sum(corr_field**2)) / np.sqrt(corr_field.size)) / sigma\n",
    "\n",
    "    corr_field /= norm\n",
    "    \n",
    "    return corr_field\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de5802-8143-4fb1-a85d-3b8dc627412a",
   "metadata": {},
   "source": [
    "# Noise matching\n",
    "\n",
    "this is an unused function that is from Krieger 2020, where he adds additional noise to the CMZ image to match the noise present in the NGC253\n",
    "image. I do not do this because it creates a ton of false structures that dont exist and I dont want those in the CMZ data.\n",
    "\n",
    "I require at least 5 times the noise from a noise-channel before I allow the pixels to be considered real data.\n",
    "I calculate the noise for each cube after doing all the data reduction, which is expected to amplify the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7914523-d3a7-46bd-8407-f10c06de66e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "        \n",
    "def Noise_matching(Input_Cube_Match,Input_Cube_Noisy,Cube_Name_Save,Force_region=False,FVX=0,FVY=0,Force_Noise=False,FNV1=0,FNV2=0):\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    #Find noise for ngc253\n",
    "    \n",
    "\n",
    "    Qp = Input_Cube_Noisy.with_spectral_unit(u.km/u.s,velocity_convention=\"radio\") \n",
    "    Qp.allow_huge_operations=True\n",
    "\n",
    "    Qp = Qp.to(u.K)#Jy to Kelvin\n",
    "\n",
    "    datn = Qp.hdu.data\n",
    "    del Qp\n",
    "\n",
    "\n",
    "    Non_nan=((datn[0,:,int(np.shape(datn)[2]/1.5):int(np.shape(datn)[2]-1)]>0)  | (datn[0,:,int(np.shape(datn)[2]/1.5):int(np.shape(datn)[2]-1)]<0 ))\n",
    "\n",
    "    NGCCO32_Noise = (np.nanstd(datn[0,:,int(np.shape(datn)[2]/1.5):int(np.shape(datn)[2]-1)],where= Non_nan)) #Noise K\n",
    "    print(np.shape(datn))\n",
    "    del datn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Qp = Input_Cube_Match\n",
    "    #match noise\n",
    "    Qp.allow_huge_operations=True\n",
    "    Q = Qp.to(u.K)#Jy to Kelvin\n",
    "    del Qp\n",
    "    datn=Q.hdu.data\n",
    "    \n",
    "    if gal ==\"GC\":\n",
    "        Non_nan=((datn[0,:,0:int(np.shape(datn)[2]/2)]>0)  | (datn[0,:,0:int(np.shape(datn)[2]/2)]<0 ))\n",
    "\n",
    "        m = (np.nanstd(datn[0,:,0:int(np.shape(datn)[2]/2)],where= Non_nan)) #Noise K\n",
    "    else:\n",
    "        Non_nan=((datn[0,:,int(np.shape(datn)[2]/1.5):int(np.shape(datn)[2]-1)]>0)  | (datn[0,:,int(np.shape(datn)[2]/1.5):int(np.shape(datn)[2]-1)]<0 ))\n",
    "\n",
    "        m = (np.nanstd(datn[0,:,int(np.shape(datn)[2]/1.5):int(np.shape(datn)[2]-1)],where= Non_nan)) #Noise K\n",
    "\n",
    "    if (Force_Noise):\n",
    "        m = FNV1 #Force a noise Values .037? .115?\n",
    "        NGCCO32_Noise=FNV2\n",
    "    print(m,\"Noise (K) matched to \",NGCCO32_Noise)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    npixels = np.product(Q.hdu.data.shape)\n",
    "\n",
    "    target_noise = float(NGCCO32_Noise)\n",
    "    actual_noise = m\n",
    "    additional_sigma = np.sqrt(np.abs(target_noise**2 - actual_noise**2))\n",
    "\n",
    "    additional_noise = np.random.normal(0., additional_sigma, npixels)\n",
    "    additional_noise = np.reshape(additional_noise, Q.hdu.data.shape)\n",
    "\n",
    "\n",
    "    fwhm_factor = np.sqrt(8*np.log(2))\n",
    "    add_noise = np.zeros(np.shape(datn))\n",
    "    for lmi in range(len(datn)):\n",
    "        new_seed = np.random.randint(1e9)\n",
    "        additional_noise = gauss_correlated_noise_2D(shape=(Q.hdu.data[6].shape[0],Q.hdu.data[6].shape[1]), sigma=additional_sigma, beam_gauss_width=5/fwhm_factor,randomseed=new_seed)\n",
    "        pp=np.where(additional_noise!=np.nan)\n",
    "        add_noise[lmi][pp]=additional_noise[pp]\n",
    "\n",
    "    new_data = datn+add_noise\n",
    "    QCopy = Q.hdu\n",
    "    QCopy.data = new_data\n",
    "    Q = SpectralCube.read(QCopy)\n",
    "    del QCopy\n",
    "\n",
    "    \n",
    "\n",
    "    Q.write(Cube_Name_Save,overwrite=True)\n",
    "    del Q\n",
    "    print(\"Wrote to\",Cube_Name_Save)\n",
    "    \n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
